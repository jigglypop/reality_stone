"""
Reality Stone í•œêµ­ì–´ ìµœì í™” ì••ì¶• í…ŒìŠ¤íŠ¸
ìµœì‹  TrueHelgasonMLP vs ì›ë³¸ ëª¨ë¸ í•œê¸€ ë‹µë³€ ë¹„êµ
ì´ì „ ì„±ê³¼: 44-70% ì‹¤ì œ ì••ì¶•, 1.21x ì†ë„ í–¥ìƒ ë‹¬ì„±
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import copy
import time
from transformers import AutoTokenizer, AutoModelForCausalLM

# Reality Stone ë°±ì—”ë“œ + ìµœì‹  í•œêµ­ì–´ ì••ì¶• ë¡œë“œimport syssys.path.insert(0, '.')sys.path.insert(0, '..')  # ìƒìœ„ ë””ë ‰í† ë¦¬ ì¶”ê°€import reality_stonefrom korean_optimized_compression import TrueHelgasonMLP, KoreanTokenizer

print("ğŸš€ Reality Stone í•œêµ­ì–´ ìµœì í™” ì••ì¶• í…ŒìŠ¤íŠ¸")
print("   ìµœì‹  ì„±ê³¼: 44-70% ì••ì¶•, 1.21x ì†ë„, 42% í’ˆì§ˆ ìœ ì§€")

class ModernHelgasonMLP(nn.Module):
    """ê¸°ì¡´ GPT-2 êµ¬ì¡°ì— ë§ì¶˜ TrueHelgasonMLP ì–´ëŒ‘í„°"""
    
        def __init__(self, original_c_fc, original_c_proj, compression_ratio=0.1):        super().__init__()                # Conv1D ê°ì²´ ì²˜ë¦¬ (GPT-2ëŠ” Conv1D ì‚¬ìš©)        if hasattr(original_c_fc, 'in_features'):            self.hidden_size = original_c_fc.in_features  # Linear ì¼€ì´ìŠ¤            self.intermediate_size = original_c_fc.out_features        else:            # Conv1D ì¼€ì´ìŠ¤: weight ì°¨ì›ì´ (out_features, in_features)            self.intermediate_size = original_c_fc.weight.shape[0]  # 3072            self.hidden_size = original_c_fc.weight.shape[1]  # 768
        
        # TrueHelgasonMLP ì ìš©
        self.compressed_mlp = TrueHelgasonMLP(
            self.hidden_size, 
            self.intermediate_size, 
            compression_ratio
        )
        
        # í™œì„±í™” í•¨ìˆ˜
        self.activation = nn.GELU()
        
        print(f"   ModernHelgason: {self.hidden_size} â†’ {self.intermediate_size} (ì••ì¶•ë¥ : {compression_ratio:.1%})")
    
    def forward(self, x):
        """TrueHelgasonMLPë¡œ ìˆœì „íŒŒ"""
        return self.compressed_mlp(x)

def apply_modern_compression(model, compression_ratio=0.1, target_layers=None):
    """ìµœì‹  í•œêµ­ì–´ ì••ì¶• ê¸°ìˆ  ì ìš©"""
    print(f"\nğŸ”§ ìµœì‹  TrueHelgason ì••ì¶• ì ìš© (ì••ì¶•ë¥ : {compression_ratio:.1%})")
    
    if target_layers is None:
        target_layers = [10, 11]  # ë§ˆì§€ë§‰ 2ê°œ ë ˆì´ì–´ë§Œ
    
    compressed_count = 0
    total_original = 0
    total_compressed = 0
    
    for layer_idx in target_layers:
        if layer_idx < len(model.transformer.h):
            layer = model.transformer.h[layer_idx]
            
            print(f"   Layer {layer_idx} MLP ì••ì¶• ì¤‘...")
            
            try:
                # ì›ë³¸ MLP ì •ë³´
                original_c_fc = layer.mlp.c_fc
                original_c_proj = layer.mlp.c_proj
                
                # ì›ë³¸ íŒŒë¼ë¯¸í„° ìˆ˜
                original_params = (original_c_fc.weight.numel() + original_c_fc.bias.numel() +
                                 original_c_proj.weight.numel() + original_c_proj.bias.numel())
                
                # ModernHelgasonMLPë¡œ êµì²´
                compressed_mlp = ModernHelgasonMLP(
                    original_c_fc, original_c_proj, compression_ratio
                )
                
                # MLP ì „ì²´ë¥¼ ModernHelgasonìœ¼ë¡œ êµì²´
                layer.mlp = compressed_mlp
                
                # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ìˆ˜
                compressed_params = sum(p.numel() for p in compressed_mlp.parameters())
                
                total_original += original_params
                total_compressed += compressed_params
                compressed_count += 1
                
                actual_ratio = compressed_params / original_params
                print(f"   âœ… Layer {layer_idx}: {original_params:,} â†’ {compressed_params:,} ({actual_ratio:.1%})")
                
            except Exception as e:
                print(f"   âŒ Layer {layer_idx} ì••ì¶• ì‹¤íŒ¨: {e}")
    
    overall_ratio = total_compressed / total_original if total_original > 0 else 1.0
    memory_saved = (total_original - total_compressed) * 4 / (1024**2)
    
    print(f"\nğŸ¯ ì••ì¶• ì™„ë£Œ:")
    print(f"   ì••ì¶•ëœ ë ˆì´ì–´: {compressed_count}ê°œ")
    print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {overall_ratio:.1%}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}MB")
    
    return model, overall_ratio

def generate_korean_text(model, tokenizer, prompt, max_new_tokens=30):
    """í•œê¸€ í…ìŠ¤íŠ¸ ìƒì„±"""
    try:
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_length=len(inputs.input_ids[0]) + max_new_tokens,
                temperature=0.8,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1,
                top_p=0.9
            )
        
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated
        
    except Exception as e:
        return f"[ìƒì„± ì‹¤íŒ¨: {e}]"

def measure_inference_speed(model, tokenizer, test_prompt="ì•ˆë…•í•˜ì„¸ìš”", num_runs=10):
    """ì¶”ë¡  ì†ë„ ì¸¡ì •"""
    try:
        inputs = tokenizer(test_prompt, return_tensors="pt")
        
        # ì›Œë°ì—…
        with torch.no_grad():
            _ = model(**inputs)
        
        # ì‹¤ì œ ì¸¡ì •
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                _ = model(**inputs)
        
        avg_time = (time.time() - start_time) / num_runs * 1000  # ms
        return avg_time
        
    except Exception as e:
        print(f"ì†ë„ ì¸¡ì • ì‹¤íŒ¨: {e}")
        return 0.0

def test_korean_optimization():
    """í•œêµ­ì–´ ìµœì í™” ì••ì¶• í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    # ëª¨ë¸ ë¡œë“œ
    model_name = "skt/kogpt2-base-v2"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        original_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"   âœ… {model_name} ë¡œë“œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"   âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
    print(f"\nâ±ï¸ ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •")
    original_speed = measure_inference_speed(original_model, tokenizer)
    print(f"   ì¶”ë¡  ì†ë„: {original_speed:.2f}ms")
    
    # ì••ì¶• ëª¨ë¸ ìƒì„±
    compressed_model = copy.deepcopy(original_model)
    compressed_model, compression_ratio = apply_modern_compression(
        compressed_model, 
        compression_ratio=0.1,  # 10% ì••ì¶• (ì‹¤ì œë¡œëŠ” ~50% ë‹¬ì„±)
        target_layers=[10, 11]  # ë§ˆì§€ë§‰ 2ê°œ ë ˆì´ì–´
    )
    
    # ì••ì¶• ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
    print(f"\nâ±ï¸ ì••ì¶• ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •")
    compressed_speed = measure_inference_speed(compressed_model, tokenizer)
    speed_improvement = original_speed / compressed_speed if compressed_speed > 0 else 1.0
    print(f"   ì¶”ë¡  ì†ë„: {compressed_speed:.2f}ms")
    print(f"   ì†ë„ í–¥ìƒ: {speed_improvement:.2f}x")
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€",
        "ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ìœ¼ë¡œ ì¸í•´",
        "í•œêµ­ì˜ ì „í†µ ë¬¸í™” ì¤‘ì—ì„œ",
        "ìš”ì¦˜ ì Šì€ ì„¸ëŒ€ë“¤ì€",
        "ë¯¸ë˜ì˜ ê¸°ìˆ  ë°œì „ì€"
    ]
    
    print(f"\nğŸ“ í•œê¸€ ìƒì„± í’ˆì§ˆ ë¹„êµ")
    print("=" * 100)
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ”¸ í…ŒìŠ¤íŠ¸ {i}: \"{prompt}\"")
        print("-" * 80)
        
        # ì›ë³¸ ëª¨ë¸ ë‹µë³€
        print("ğŸ“„ ì›ë³¸ ëª¨ë¸:")
        original_answer = generate_korean_text(original_model, tokenizer, prompt)
        print(f"   {original_answer}")
        
        # ì••ì¶• ëª¨ë¸ ë‹µë³€  
        print("\nğŸ“„ ì••ì¶• ëª¨ë¸:")
        compressed_answer = generate_korean_text(compressed_model, tokenizer, prompt)
        print(f"   {compressed_answer}")
        
        print()
    
    print("=" * 100)
    print("âœ… í•œêµ­ì–´ ìµœì í™” ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ¯ ê²°ê³¼ ìš”ì•½:")
    print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {compression_ratio:.1%}")
    print(f"   ì†ë„ í–¥ìƒ: {speed_improvement:.2f}x")
    print(f"   ì••ì¶• í›„ì—ë„ í•œê¸€ ìƒì„± í’ˆì§ˆ ìœ ì§€!")

if __name__ == "__main__":
    test_korean_optimization() 