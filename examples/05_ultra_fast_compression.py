"""
Reality Stone ì´ˆê³ ì† ì••ì¶• ì—”ì§„
ì‚¬ì „ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ë¡œ ëŸ°íƒ€ì„ ì˜¤ë²„í—¤ë“œ ì™„ì „ ì œê±°

ì´ì „ ë¬¸ì œ: SVD ì¬êµ¬ì„±ìœ¼ë¡œ ì¸í•œ ì†ë„ ì €í•˜ (0.5x)
í•´ê²°ì±…: ì´ˆê¸°í™” ì‹œì ì— ì••ì¶•ëœ ê°€ì¤‘ì¹˜ ì™„ì „ ê³„ì‚° í›„ ê³ ì •
ëª©í‘œ: 1.5-2x ì†ë„ í–¥ìƒ + 25-35% ì••ì¶•ë¥  + 90%+ í’ˆì§ˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import copy
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

# Reality Stone ë°±ì—”ë“œ ë¡œë“œ
import sys
sys.path.insert(0, '.')

try:
    import reality_stone
    print("âœ… Reality Stone ë°±ì—”ë“œ ë¡œë“œ ì„±ê³µ!")
    REALITY_STONE_AVAILABLE = True
except ImportError as e:
    print(f"âŒ Reality Stone ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    REALITY_STONE_AVAILABLE = False


class UltraFastCompressedLinear(nn.Module):
    """ì´ˆê³ ì† ì••ì¶• ì„ í˜• ë ˆì´ì–´ - ëŸ°íƒ€ì„ ì¬êµ¬ì„± ì—†ìŒ"""
    
            def __init__(self, original_linear, compression_ratio=0.4, layer_name="unknown"):        super().__init__()                self.layer_name = layer_name                # ì›ë³¸ ê°€ì¤‘ì¹˜ ë° ë°”ì´ì–´ìŠ¤ (Conv1D ì²˜ë¦¬!)        original_weight = original_linear.weight.data.clone()        original_bias = original_linear.bias.data.clone() if original_linear.bias is not None else None                device = original_weight.device        dtype = original_weight.dtype                print(f"   UltraFast {layer_name}: {original_weight.shape} (ì••ì¶•ë¥ : {compression_ratio:.1%})")                # Conv1DëŠ” (out_features, in_features), LinearëŠ” (out_features, in_features)          # í•˜ì§€ë§Œ SVDë¥¼ ìœ„í•´ (in_features, out_features)ë¡œ ì „ì¹˜        if len(original_weight.shape) == 2:            weight_for_svd = original_weight.T  # (in, out) for SVD        else:            weight_for_svd = original_weight                # 1. ë¹ ë¥¸ SVD ì••ì¶•        U, S, V = torch.svd(weight_for_svd.float())
        
        # 2. ì ì‘ì  ë­í¬ ì„ íƒ (ì—ë„ˆì§€ + ì••ì¶•ë¥  ê³ ë ¤)
        total_rank = min(U.shape[1], V.shape[0])
        
        # ì—ë„ˆì§€ ê¸°ë°˜ ì¤‘ìš” ë­í¬
        energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
        energy_rank = torch.sum(energy < 0.99).item() + 1  # 99% ì—ë„ˆì§€ ë³´ì¡´
        
        # ì••ì¶•ë¥  ê¸°ë°˜ ë­í¬
        target_rank = max(8, int(total_rank * compression_ratio))
        
        # ë‘˜ ì¤‘ ì‘ì€ ê°’ ì„ íƒ (í’ˆì§ˆê³¼ ì••ì¶•ì˜ ê· í˜•)
        final_rank = min(energy_rank, target_rank)
        final_rank = max(final_rank, 8)  # ìµœì†Œ 8ê°œ ë³´ì¥
        
        print(f"   ë­í¬ ì„ íƒ: ì—ë„ˆì§€({energy_rank}) vs íƒ€ê²Ÿ({target_rank}) â†’ ìµœì¢…({final_rank})")
        
        # 3. ì••ì¶•ëœ ê°€ì¤‘ì¹˜ ì‚¬ì „ ê³„ì‚° (í•µì‹¬!)
        compressed_weight = U[:, :final_rank] @ torch.diag(S[:final_rank]) @ V[:, :final_rank].T
        
        # 4. ì‚¬ì „ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ì €ì¥
        self.weight = nn.Parameter(compressed_weight.to(dtype).to(device))
        
        # 5. ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
        if original_bias is not None:
            self.bias = nn.Parameter(original_bias.to(dtype).to(device))
        else:
            self.register_parameter('bias', None)
        
        # 6. ì••ì¶• í†µê³„
        original_params = original_weight.numel() + (original_bias.numel() if original_bias is not None else 0)
        compressed_params = self.weight.numel() + (self.bias.numel() if self.bias is not None else 0)
        self.actual_compression_ratio = compressed_params / original_params
        
        print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {compressed_params:,} ({self.actual_compression_ratio:.3f})")
        print(f"   ì—ë„ˆì§€ ë³´ì¡´: {energy[final_rank-1]:.3f}")
        
    def forward(self, x):
        """ì´ˆê³ ì† ìˆœì „íŒŒ - ì¬êµ¬ì„± ì—†ìŒ, ì§ì ‘ linear ì—°ì‚°"""
        return F.linear(x, self.weight, self.bias)


class SmartCompressedMLP(nn.Module):
    """ìŠ¤ë§ˆíŠ¸ ì••ì¶• MLP - ì „ëµì  ë ˆì´ì–´ë³„ ì••ì¶•"""
    
    def __init__(self, original_mlp, layer_idx=0, aggressive=False):
        super().__init__()
        
        self.layer_idx = layer_idx
        
        # ë ˆì´ì–´ ìœ„ì¹˜ì— ë”°ë¥¸ ì••ì¶• ì „ëµ
        if layer_idx < 4:  # ì´ˆê¸° ë ˆì´ì–´ (í’ˆì§ˆ ì¤‘ìš”)
            compression_ratio = 0.6 if not aggressive else 0.4
        elif layer_idx < 8:  # ì¤‘ê°„ ë ˆì´ì–´ (ê· í˜•)
            compression_ratio = 0.5 if not aggressive else 0.3  
        else:  # í›„ë°˜ ë ˆì´ì–´ (ì••ì¶• ì¤‘ìš”)
            compression_ratio = 0.4 if not aggressive else 0.2
        
        print(f"\nğŸ“ Layer {layer_idx} MLP ìŠ¤ë§ˆíŠ¸ ì••ì¶• (ì••ì¶•ë¥ : {compression_ratio:.1%})")
        
        # c_fc ì••ì¶• (ì…ë ¥ â†’ ì¤‘ê°„ì¸µ)
        if hasattr(original_mlp, 'c_fc'):
            self.c_fc = UltraFastCompressedLinear(
                original_mlp.c_fc, compression_ratio, f"L{layer_idx}_c_fc"
            )
        
        # c_proj ì••ì¶• (ì¤‘ê°„ì¸µ â†’ ì¶œë ¥) - ë” ë³´ìˆ˜ì 
        if hasattr(original_mlp, 'c_proj'):
            conservative_ratio = compression_ratio * 1.2  # 20% ë” ë³´ìˆ˜ì 
            self.c_proj = UltraFastCompressedLinear(
                original_mlp.c_proj, conservative_ratio, f"L{layer_idx}_c_proj"
            )
        
        # í™œì„±í™” í•¨ìˆ˜
        self.activation = nn.GELU()
        
    def forward(self, x):
        """ìŠ¤ë§ˆíŠ¸ MLP ìˆœì „íŒŒ"""
        # í‘œì¤€ MLP í”Œë¡œìš°: c_fc â†’ activation â†’ c_proj
        h = self.c_fc(x)
        h = self.activation(h)
        output = self.c_proj(h)
        return output


def apply_ultra_fast_compression(model, aggressive=False, target_layers=None):
    """ì´ˆê³ ì† ì••ì¶• ì ìš©"""
    
    mode = "ê³µê²©ì " if aggressive else "ê· í˜•ì "
    print(f"\nğŸš€ ì´ˆê³ ì† ì••ì¶• ì ìš© ({mode} ëª¨ë“œ)")
    print("   ì „ëµ: ì‚¬ì „ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜, ëŸ°íƒ€ì„ ì˜¤ë²„í—¤ë“œ ì œê±°")
    
    if target_layers is None:
        # ì „ì²´ ë ˆì´ì–´ ì••ì¶• (ë” ë§ì€ ë©”ëª¨ë¦¬ ì ˆì•½)
        target_layers = list(range(len(model.transformer.h)))
    
    print(f"   ëŒ€ìƒ ë ˆì´ì–´: {len(target_layers)}ê°œ (ì „ì²´)")
    
    compressed_count = 0
    total_original = 0
    total_compressed = 0
    
    for layer_idx in target_layers:
        if layer_idx < len(model.transformer.h):
            layer = model.transformer.h[layer_idx]
            
            try:
                # ì›ë³¸ MLP íŒŒë¼ë¯¸í„° ìˆ˜
                original_mlp = layer.mlp
                original_params = sum(p.numel() for p in original_mlp.parameters())
                
                # SmartCompressedMLPë¡œ êµì²´
                compressed_mlp = SmartCompressedMLP(
                    original_mlp, layer_idx, aggressive
                )
                
                # MLP êµì²´
                layer.mlp = compressed_mlp
                
                # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ìˆ˜
                compressed_params = sum(p.numel() for p in compressed_mlp.parameters())
                
                total_original += original_params
                total_compressed += compressed_params
                compressed_count += 1
                
                actual_ratio = compressed_params / original_params
                print(f"   âœ… Layer {layer_idx}: {original_params:,} â†’ {compressed_params:,} ({actual_ratio:.1%})")
                
            except Exception as e:
                print(f"   âŒ Layer {layer_idx} ì••ì¶• ì‹¤íŒ¨: {e}")
    
    # ì „ì²´ ëª¨ë¸ ì••ì¶•ë¥  ê³„ì‚°
    total_model_params = sum(p.numel() for p in model.parameters())
    mlp_compression_ratio = total_compressed / total_original if total_original > 0 else 1.0
    overall_compression_ratio = (total_model_params - total_original + total_compressed) / total_model_params
    memory_saved = (total_original - total_compressed) * 4 / (1024**2)
    
    print(f"\nğŸ¯ ì´ˆê³ ì† ì••ì¶• ì™„ë£Œ:")
    print(f"   ì••ì¶•ëœ ë ˆì´ì–´: {compressed_count}ê°œ")
    print(f"   MLP ì••ì¶•ë¥ : {mlp_compression_ratio:.1%}")
    print(f"   ì „ì²´ ëª¨ë¸ ì••ì¶•ë¥ : {overall_compression_ratio:.1%}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}MB")
    
    return model, overall_compression_ratio


def load_korean_model():
    """í•œê¸€ ëª¨ë¸ ë¡œë“œ"""
    
    print("ğŸ“¥ í•œê¸€ ëª¨ë¸ ë¡œë”©...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"   ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"   âœ… ë¡œë“œ ì„±ê³µ!")
        return model, tokenizer, model_name
        
    except Exception as e:
        print(f"   âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None


def benchmark_speed(model, tokenizer, num_runs=50):
    """ì •ë°€ ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
    
    try:
        # ë‹¤ì–‘í•œ ì…ë ¥ ê¸¸ì´ë¡œ í…ŒìŠ¤íŠ¸
        test_inputs = [
            "ì•ˆë…•",
            "ì•ˆë…•í•˜ì„¸ìš” ì˜¤ëŠ˜ì€",
            "ì•ˆë…•í•˜ì„¸ìš” ì˜¤ëŠ˜ì€ ì •ë§ ì¢‹ì€ ë‚ ì”¨ë„¤ìš”"
        ]
        
        all_times = []
        
        for test_input in test_inputs:
            inputs = tokenizer(test_input, return_tensors="pt")
            
            # ì›Œë°ì—…
            with torch.no_grad():
                for _ in range(10):
                    _ = model(**inputs)
            
            # ì¸¡ì •
            times = []
            for _ in range(num_runs):
                start_time = time.time()
                with torch.no_grad():
                    _ = model(**inputs)
                times.append((time.time() - start_time) * 1000)
            
            avg_time = np.mean(times)
            all_times.append(avg_time)
            print(f"   '{test_input}': {avg_time:.2f}ms")
        
        overall_avg = np.mean(all_times)
        return overall_avg
        
    except Exception as e:
        print(f"   âŒ ì†ë„ ì¸¡ì • ì‹¤íŒ¨: {e}")
        return 0.0


def test_generation_quality(model, tokenizer, prompts):
    """ìƒì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ“ ìƒì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
    results = []
    times = []
    
    for i, prompt in enumerate(prompts):
        try:
            print(f"\n{i+1}. '{prompt}'")
            
            inputs = tokenizer(prompt, return_tensors="pt")
            start_time = time.time()
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=len(inputs.input_ids[0]) + 25,  # ë” ì§§ê²Œ
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.2,
                    top_p=0.85
                )
            
            gen_time = (time.time() - start_time) * 1000
            times.append(gen_time)
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            results.append(generated)
            
            print(f"   â†’ {generated}")
            print(f"   ì‹œê°„: {gen_time:.1f}ms")
            
        except Exception as e:
            print(f"   âŒ ìƒì„± ì‹¤íŒ¨: {e}")
            results.append(f"[ì‹¤íŒ¨: {e}]")
            times.append(0)
    
    avg_time = np.mean(times) if times else 0
    print(f"\nâ±ï¸ í‰ê·  ìƒì„± ì‹œê°„: {avg_time:.1f}ms")
    
    return results, avg_time


def ultra_fast_compression_test():
    """ì´ˆê³ ì† ì••ì¶• í…ŒìŠ¤íŠ¸"""
    
    print("ğŸš€ Reality Stone ì´ˆê³ ì† ì••ì¶• í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print("   ëª©í‘œ: 1.5-2x ì†ë„ í–¥ìƒ + 25-35% ì••ì¶•ë¥  + 90%+ í’ˆì§ˆ")
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer, model_name = load_korean_model()
    if model is None:
        return
    
    original_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   í¬ê¸°: {original_params * 4 / (1024**2):.1f}MB")
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ë” ì§§ê²Œ)
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì¸ê³µì§€ëŠ¥",
        "í•œêµ­ ë¬¸í™”"
    ]
    
    # ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“‹ ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬")
    print("-" * 60)
    
    print("â±ï¸ ì›ë³¸ ëª¨ë¸ ì†ë„ ì¸¡ì •")
    original_speed = benchmark_speed(model, tokenizer)
    print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {original_speed:.2f}ms")
    
    original_results, original_gen_time = test_generation_quality(model, tokenizer, test_prompts)
    
    # ë‘ ê°€ì§€ ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸
    modes = [
        {"name": "ê· í˜•", "aggressive": False},
        {"name": "ê³µê²©ì ", "aggressive": True}
    ]
    
    best_result = None
    best_score = 0
    
    for mode in modes:
        print(f"\nğŸ”§ {mode['name']} ëª¨ë“œ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        try:
            # ëª¨ë¸ ë³µì‚¬ ë° ì••ì¶•
            compressed_model = copy.deepcopy(model)
            compressed_model, actual_compression = apply_ultra_fast_compression(
                compressed_model, mode['aggressive']
            )
            
            # ì••ì¶• ëª¨ë¸ ì†ë„ ì¸¡ì •
            print("\nâ±ï¸ ì••ì¶• ëª¨ë¸ ì†ë„ ì¸¡ì •")
            compressed_speed = benchmark_speed(compressed_model, tokenizer)
            speed_improvement = original_speed / compressed_speed if compressed_speed > 0 else 1.0
            print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {compressed_speed:.2f}ms")
            print(f"   ì†ë„ í–¥ìƒ: {speed_improvement:.2f}x")
            
            # ì••ì¶• ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸
            compressed_results, compressed_gen_time = test_generation_quality(compressed_model, tokenizer, test_prompts)
            gen_speed_improvement = original_gen_time / compressed_gen_time if compressed_gen_time > 0 else 1.0
            
            # í’ˆì§ˆ í‰ê°€ (í…ìŠ¤íŠ¸ ê¸¸ì´ + í•œê¸€ ë¹„ìœ¨)
            quality_score = 0
            korean_ratio = 0
            
            for orig, comp in zip(original_results, compressed_results):
                if isinstance(comp, str) and len(comp) > 5:
                    # ê¸¸ì´ ë¹„ìœ¨
                    length_ratio = min(len(comp) / len(orig), 1.0) if len(orig) > 0 else 0
                    
                    # í•œê¸€ ë¹„ìœ¨
                    korean_chars = sum(1 for c in comp if 'ê°€' <= c <= 'í£')
                    total_chars = len(comp.replace(' ', ''))
                    kr_ratio = korean_chars / total_chars if total_chars > 0 else 0
                    
                    quality_score += length_ratio
                    korean_ratio += kr_ratio
            
            quality_score = quality_score / len(test_prompts) if test_prompts else 0
            korean_ratio = korean_ratio / len(test_prompts) if test_prompts else 0
            
            # ì¢…í•© ì ìˆ˜ (ì†ë„ * ì••ì¶•ë¥  * í’ˆì§ˆ)
            memory_saved_ratio = 1 - actual_compression
            overall_score = speed_improvement * memory_saved_ratio * quality_score
            
            print(f"\nğŸ“Š {mode['name']} ëª¨ë“œ ê²°ê³¼:")
            print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {actual_compression:.1%}")
            print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved_ratio:.1%}")
            print(f"   ì¶”ë¡  ì†ë„ í–¥ìƒ: {speed_improvement:.2f}x")
            print(f"   ìƒì„± ì†ë„ í–¥ìƒ: {gen_speed_improvement:.2f}x")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f}")
            print(f"   í•œê¸€ ë¹„ìœ¨: {korean_ratio:.3f}")
            print(f"   ì¢…í•© ì ìˆ˜: {overall_score:.3f}")
            
            # ìµœê³  ì„±ëŠ¥ ê¸°ë¡
            if overall_score > best_score:
                best_score = overall_score
                best_result = {
                    'mode': mode['name'],
                    'actual_compression': actual_compression,
                    'memory_saved': memory_saved_ratio,
                    'speed_improvement': speed_improvement,
                    'gen_speed_improvement': gen_speed_improvement,
                    'quality_score': quality_score,
                    'korean_ratio': korean_ratio,
                    'overall_score': overall_score
                }
            
        except Exception as e:
            print(f"   âŒ ì••ì¶• ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ† ì´ˆê³ ì† ì••ì¶• ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    
    if best_result:
        print(f"ğŸ¥‡ ìµœê³  ì„±ëŠ¥ ({best_result['mode']} ëª¨ë“œ):")
        print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {best_result['actual_compression']:.1%}")
        print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {best_result['memory_saved']:.1%}")
        print(f"   ì¶”ë¡  ì†ë„ í–¥ìƒ: {best_result['speed_improvement']:.2f}x")
        print(f"   ìƒì„± ì†ë„ í–¥ìƒ: {best_result['gen_speed_improvement']:.2f}x")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {best_result['quality_score']:.3f}")
        print(f"   í•œê¸€ ë¹„ìœ¨: {best_result['korean_ratio']:.3f}")
        print(f"   ì¢…í•© ì ìˆ˜: {best_result['overall_score']:.3f}")
        
        print(f"\nğŸ¯ ëª©í‘œ ë‹¬ì„±ë„:")
        speed_ok = best_result['speed_improvement'] >= 1.5
        compress_ok = best_result['memory_saved'] >= 0.25
        quality_ok = best_result['quality_score'] >= 0.9
        
        print(f"   ì†ë„ ê°œì„ : {'âœ…' if speed_ok else 'âš ï¸'} (ëª©í‘œ: 1.5-2x, ë‹¬ì„±: {best_result['speed_improvement']:.1f}x)")
        print(f"   ì••ì¶•ë¥ : {'âœ…' if compress_ok else 'âš ï¸'} (ëª©í‘œ: 25-35%, ë‹¬ì„±: {best_result['memory_saved']:.1%})")
        print(f"   í’ˆì§ˆ ìœ ì§€: {'âœ…' if quality_ok else 'âš ï¸'} (ëª©í‘œ: 90%+, ë‹¬ì„±: {best_result['quality_score']:.1%})")
        
        if speed_ok and compress_ok and quality_ok:
            print(f"\nğŸ‰ ëª¨ë“  ëª©í‘œ ë‹¬ì„±! ì´ˆê³ ì† ì••ì¶• ì„±ê³µ!")
        else:
            print(f"\nğŸ”„ ì¼ë¶€ ëª©í‘œ ë¯¸ë‹¬ì„±, ì¶”ê°€ ìµœì í™” í•„ìš”")
    else:
        print("âŒ ì„±ê³µì ì¸ ì••ì¶• ê²°ê³¼ ì—†ìŒ")
    
    print(f"\nâœ… ì´ˆê³ ì† ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    ultra_fast_compression_test() 