"""
Reality Stone ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì—°êµ¬
í’ˆì§ˆê³¼ ì •í™•ë„, ì••ì¶•ë¥ ì— ì§‘ì¤‘í•œ ìƒˆë¡œìš´ ì ‘ê·¼ë²•

ì—°êµ¬ ëª©í‘œ:
1. ì••ì¶•ë¥ : 30-50% ë‹¬ì„± (í˜„ì¬: 15-20%)
2. í•œêµ­ì–´ ì •í™•ë„: 90%+ ìœ ì§€ 
3. í’ˆì§ˆ: ì˜ë¯¸ë¡ ì  ì¼ê´€ì„± ë³´ì¡´

ìƒˆë¡œìš´ ê¸°ë²•:
- í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• (SVD + ì¤‘ìš”ë„ í”„ë£¨ë‹)
- í•œêµ­ì–´ íƒœìŠ¤í¬ ê¸°ë°˜ í‰ê°€
- ì§€ì‹ ì¦ë¥˜ ê¸°ë°˜ ì„±ëŠ¥ ë³´ì¡´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import copy
import warnings
warnings.filterwarnings("ignore")

# Reality Stone ë°±ì—”ë“œ ë¡œë“œ
import sys
sys.path.insert(0, '.')

try:
    import reality_stone
    print("âœ… Reality Stone ë°±ì—”ë“œ ë¡œë“œ ì„±ê³µ!")
except ImportError as e:
    print(f"âŒ Reality Stone ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")


class HybridCompressedLinear(nn.Module):
    """í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶•: SVD + ì¤‘ìš”ë„ ê¸°ë°˜ í”„ë£¨ë‹"""
    
    def __init__(self, original_layer, compression_ratio=0.3, layer_name="unknown"):
        super().__init__()
        
        self.layer_name = layer_name
        
        # ì›ë³¸ ê°€ì¤‘ì¹˜
        original_weight = original_layer.weight.data.clone()
        original_bias = original_layer.bias.data.clone() if original_layer.bias is not None else None
        
        device = original_weight.device
        dtype = original_weight.dtype
        
        print(f"   Hybrid {layer_name}: {original_weight.shape} â†’ ì••ì¶•ë¥ : {compression_ratio:.1%}")
        
        # 1. ì¤‘ìš”ë„ ë¶„ì„ (ê·¸ë¼ë””ì–¸íŠ¸ ê¸°ë°˜)
        weight_importance = torch.abs(original_weight)
        
        # 2. SVD ì••ì¶•
        U, S, V = torch.svd(original_weight.float())
        
        # 3. ì ì‘ì  ë­í¬ ì„ íƒ
        total_rank = min(U.shape[1], V.shape[0])
        
        # ì¤‘ìš”ë„ ê¸°ë°˜ ì—ë„ˆì§€ ê³„ì‚°
        energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
        
        # ì••ì¶•ë¥ ì— ë”°ë¥¸ ë‹¤ë‹¨ê³„ ì „ëµ
        if compression_ratio <= 0.3:  # ê°•í•œ ì••ì¶• (30% ì´í•˜)
            energy_threshold = 0.98  # 98% ì—ë„ˆì§€ ë³´ì¡´
            target_rank = max(8, int(total_rank * compression_ratio))
        elif compression_ratio <= 0.5:  # ì¤‘ê°„ ì••ì¶• (30-50%)
            energy_threshold = 0.95  # 95% ì—ë„ˆì§€ ë³´ì¡´
            target_rank = max(12, int(total_rank * compression_ratio))
        else:  # ì•½í•œ ì••ì¶• (50% ì´ìƒ)
            energy_threshold = 0.90  # 90% ì—ë„ˆì§€ ë³´ì¡´
            target_rank = max(16, int(total_rank * compression_ratio))
        
        energy_rank = torch.sum(energy < energy_threshold).item() + 1
        final_rank = min(target_rank, energy_rank)
        final_rank = max(final_rank, 4)
        
        print(f"   ë­í¬ ë¶„ì„: ì „ì²´({total_rank}) â†’ ì—ë„ˆì§€({energy_rank}) â†’ íƒ€ê²Ÿ({target_rank}) â†’ ìµœì¢…({final_rank})")
        print(f"   ì—ë„ˆì§€ ë³´ì¡´: {energy[final_rank-1]:.4f} (ì„ê³„ê°’: {energy_threshold})")
        
        # 4. í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• ì ìš©
        # SVD ê¸°ë°˜ ì €ì°¨ì› ê·¼ì‚¬
        U_compressed = U[:, :final_rank]
        S_compressed = S[:final_rank]
        V_compressed = V[:, :final_rank]
        
        # ì¤‘ìš”ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •
        importance_factor = torch.sqrt(S_compressed / S_compressed[0])  # ì •ê·œí™”ëœ ì¤‘ìš”ë„
        S_compressed = S_compressed * importance_factor  # ì¤‘ìš”í•œ ì„±ë¶„ ê°•í™”
        
        # 5. ì••ì¶•ëœ ê°€ì¤‘ì¹˜ ì‚¬ì „ ê³„ì‚°
        compressed_weight = U_compressed @ torch.diag(S_compressed) @ V_compressed.T
        
        # 6. ì €ì¥
        self.weight = nn.Parameter(compressed_weight.to(dtype).to(device))
        
        if original_bias is not None:
            self.bias = nn.Parameter(original_bias.to(dtype).to(device))
        else:
            self.register_parameter('bias', None)
        
        # 7. ì••ì¶• í†µê³„
        original_params = original_weight.numel() + (original_bias.numel() if original_bias is not None else 0)
        compressed_params = self.weight.numel() + (self.bias.numel() if self.bias is not None else 0)
        self.actual_compression_ratio = compressed_params / original_params
        
        print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {compressed_params:,} ({self.actual_compression_ratio:.3f})")
        
    def forward(self, x):
        """í•˜ì´ë¸Œë¦¬ë“œ ìˆœì „íŒŒ"""
        return F.linear(x, self.weight, self.bias)


class AccuracyPreservingMLP(nn.Module):
    """ì •í™•ë„ ë³´ì¡´ MLP"""
    
    def __init__(self, original_mlp, layer_idx=0, target_compression=0.4):
        super().__init__()
        
        self.layer_idx = layer_idx
        
        # ë ˆì´ì–´ë³„ ì¤‘ìš”ë„ ê¸°ë°˜ ì••ì¶• ì „ëµ
        if layer_idx < 3:  # ì´ˆê¸° ë ˆì´ì–´ (íŠ¹ì§• ì¶”ì¶œ ì¤‘ìš”)
            compression_ratio = target_compression * 1.5  # ëœ ì••ì¶•
        elif layer_idx < 6:  # ì¤‘ê°„ ë ˆì´ì–´ (íŠ¹ì§• ë³€í™˜)
            compression_ratio = target_compression * 1.2  # ì•½ê°„ ëœ ì••ì¶•
        elif layer_idx < 9:  # í›„ë°˜ ë ˆì´ì–´ (ê³ ì°¨ì› íŠ¹ì§•)
            compression_ratio = target_compression  # ëª©í‘œ ì••ì¶•
        else:  # ìµœì¢… ë ˆì´ì–´ (ì¶œë ¥ ìƒì„±)
            compression_ratio = target_compression * 0.8  # ë” ì••ì¶•
        
        print(f"\nğŸ“ Layer {layer_idx} ì •í™•ë„ ë³´ì¡´ ì••ì¶• (ëª©í‘œ: {target_compression:.1%}, ì ìš©: {compression_ratio:.1%})")
        
        # c_fc ì••ì¶•
        if hasattr(original_mlp, 'c_fc'):
            self.c_fc = HybridCompressedLinear(
                original_mlp.c_fc, compression_ratio, f"L{layer_idx}_c_fc"
            )
        
        # c_proj ì••ì¶• (ì¶œë ¥ì¸µì´ë¯€ë¡œ ë” ë³´ìˆ˜ì )
        if hasattr(original_mlp, 'c_proj'):
            conservative_ratio = compression_ratio * 1.4  # 40% ë” ë³´ìˆ˜ì 
            self.c_proj = HybridCompressedLinear(
                original_mlp.c_proj, conservative_ratio, f"L{layer_idx}_c_proj"
            )
        
        # í™œì„±í™” í•¨ìˆ˜
        self.activation = nn.GELU()
        
    def forward(self, x):
        """ì •í™•ë„ ë³´ì¡´ ìˆœì „íŒŒ"""
        h = self.c_fc(x)
        h = self.activation(h)
        output = self.c_proj(h)
        return output


def apply_accuracy_focused_compression(model, target_compression=0.4, target_layers=None):
    """ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì ìš©"""
    
    print(f"\nğŸš€ ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì ìš© (ëª©í‘œ: {target_compression:.1%})")
    print("   ì „ëµ: í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶• + ë ˆì´ì–´ë³„ ìµœì í™” + ì •í™•ë„ ë³´ì¡´")
    
    if target_layers is None:
        # ì „ì²´ ë ˆì´ì–´ ì••ì¶• (ë” ë†’ì€ ì••ì¶•ë¥  ë‹¬ì„±)
        target_layers = list(range(len(model.transformer.h)))
    
    print(f"   ëŒ€ìƒ ë ˆì´ì–´: {len(target_layers)}ê°œ (ì „ì²´)")
    
    compressed_count = 0
    total_original = 0
    total_compressed = 0
    
    for layer_idx in target_layers:
        if layer_idx < len(model.transformer.h):
            layer = model.transformer.h[layer_idx]
            
            try:
                # ì›ë³¸ MLP íŒŒë¼ë¯¸í„° ìˆ˜
                original_mlp = layer.mlp
                original_params = sum(p.numel() for p in original_mlp.parameters())
                
                # AccuracyPreservingMLPë¡œ êµì²´
                compressed_mlp = AccuracyPreservingMLP(
                    original_mlp, layer_idx, target_compression
                )
                
                # MLP êµì²´
                layer.mlp = compressed_mlp
                
                # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ìˆ˜
                compressed_params = sum(p.numel() for p in compressed_mlp.parameters())
                
                total_original += original_params
                total_compressed += compressed_params
                compressed_count += 1
                
                actual_ratio = compressed_params / original_params
                print(f"   âœ… Layer {layer_idx}: {original_params:,} â†’ {compressed_params:,} ({actual_ratio:.1%})")
                
            except Exception as e:
                print(f"   âŒ Layer {layer_idx} ì••ì¶• ì‹¤íŒ¨: {e}")
    
    # ì „ì²´ ëª¨ë¸ ì••ì¶•ë¥  ê³„ì‚°
    total_model_params = sum(p.numel() for p in model.parameters())
    mlp_compression_ratio = total_compressed / total_original if total_original > 0 else 1.0
    overall_compression_ratio = (total_model_params - total_original + total_compressed) / total_model_params
    memory_saved = (total_original - total_compressed) * 4 / (1024**2)
    
    print(f"\nğŸ¯ ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì™„ë£Œ:")
    print(f"   ì••ì¶•ëœ ë ˆì´ì–´: {compressed_count}ê°œ")
    print(f"   MLP ì••ì¶•ë¥ : {mlp_compression_ratio:.1%}")
    print(f"   ì „ì²´ ëª¨ë¸ ì••ì¶•ë¥ : {overall_compression_ratio:.1%}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}MB")
    
    return model, overall_compression_ratio


def load_korean_model():
    """í•œê¸€ ëª¨ë¸ ë¡œë“œ"""
    
    print("ğŸ“¥ í•œê¸€ ëª¨ë¸ ë¡œë”©...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"   ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"   âœ… ë¡œë“œ ì„±ê³µ!")
        return model, tokenizer, model_name
        
    except Exception as e:
        print(f"   âŒ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None


def evaluate_korean_accuracy(model, tokenizer):
    """í•œêµ­ì–´ ì •í™•ë„ í‰ê°€"""
    
    print("ğŸ“Š í•œêµ­ì–´ ì •í™•ë„ í‰ê°€")
    
    # ë‹¤ì–‘í•œ í•œêµ­ì–´ íƒœìŠ¤í¬
    tasks = {
        "ë¬¸ì¥ì™„ì„±": [
            ("í•œêµ­ì˜ ìˆ˜ë„ëŠ”", "ì„œìš¸"),
            ("ì•ˆë…•í•˜ì„¸ìš”ëŠ”", "ì¸ì‚¬"),
            ("ê¹€ì¹˜ëŠ” í•œêµ­ì˜", "ìŒì‹"),
            ("íƒœê·¹ê¸°ëŠ” í•œêµ­ì˜", "êµ­ê¸°"),
            ("í•œê¸€ì€ í•œêµ­ì˜", "ë¬¸ì")
        ],
        "ë¬¸ë§¥ì´í•´": [
            ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ", ["ì¢‹", "ë‚ ì”¨", "ë§‘", "ì‚°ì±…"]),
            ("ë°°ê°€ ê³ íŒŒì„œ", ["ë°¥", "ìŒì‹", "ë¨¹", "ì‹ë‹¹"]),
            ("ê³µë¶€ë¥¼ ì—´ì‹¬íˆ í•´ì„œ", ["ì‹œí—˜", "ì„±ì ", "ì¢‹", "í•©ê²©"]),
            ("ì¹œêµ¬ì™€ í•¨ê»˜", ["ë†€", "ì˜í™”", "ê²Œì„", "ì¦ê±°"]),
            ("ìƒˆí•´ê°€ ë˜ì–´ì„œ", ["ìƒˆí•´", "ìƒˆ", "í¬ë§", "ê²°ì‹¬"])
        ],
        "í•œê¸€ìƒì„±": [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ëŒ€í•œë¯¼êµ­",
            "í•œêµ­ì–´",
            "ì„œìš¸íŠ¹ë³„ì‹œ",
            "ì¸ê³µì§€ëŠ¥"
        ]
    }
    
    total_score = 0
    total_tests = 0
    
    # 1. ë¬¸ì¥ ì™„ì„± í‰ê°€
    print("\n1ï¸âƒ£ ë¬¸ì¥ ì™„ì„± ì •í™•ë„:")
    completion_score = 0
    for prompt, expected in tasks["ë¬¸ì¥ì™„ì„±"]:
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=len(inputs.input_ids[0]) + 10,
                    temperature=0.5,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated = generated[len(prompt):].strip()
            
            # ì •í™•ë„ ì²´í¬ (ì˜ˆìƒ ë‹¨ì–´ í¬í•¨ ì—¬ë¶€)
            score = 1 if expected in generated else 0
            completion_score += score
            
            print(f"   '{prompt}' â†’ '{generated[:20]}...' ({'âœ…' if score else 'âŒ'})")
            
        except Exception as e:
            print(f"   '{prompt}' â†’ ì˜¤ë¥˜: {e} (âŒ)")
        
        total_tests += 1
    
    completion_accuracy = completion_score / len(tasks["ë¬¸ì¥ì™„ì„±"]) if tasks["ë¬¸ì¥ì™„ì„±"] else 0
    total_score += completion_accuracy
    
    # 2. ë¬¸ë§¥ ì´í•´ í‰ê°€
    print("\n2ï¸âƒ£ ë¬¸ë§¥ ì´í•´ ì •í™•ë„:")
    context_score = 0
    for prompt, keywords in tasks["ë¬¸ë§¥ì´í•´"]:
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=len(inputs.input_ids[0]) + 15,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated = generated[len(prompt):].strip()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            keyword_matches = sum(1 for keyword in keywords if keyword in generated)
            score = keyword_matches / len(keywords)
            context_score += score
            
            print(f"   '{prompt}' â†’ '{generated[:30]}...' ({score:.1%})")
            
        except Exception as e:
            print(f"   '{prompt}' â†’ ì˜¤ë¥˜: {e} (0%)")
        
        total_tests += 1
    
    context_accuracy = context_score / len(tasks["ë¬¸ë§¥ì´í•´"]) if tasks["ë¬¸ë§¥ì´í•´"] else 0
    total_score += context_accuracy
    
    # 3. í•œê¸€ ìƒì„± í’ˆì§ˆ
    print("\n3ï¸âƒ£ í•œê¸€ ìƒì„± í’ˆì§ˆ:")
    generation_score = 0
    for prompt in tasks["í•œê¸€ìƒì„±"]:
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=len(inputs.input_ids[0]) + 20,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated = generated[len(prompt):].strip()
            
            # í•œê¸€ ë¹„ìœ¨ ê³„ì‚°
            korean_chars = sum(1 for c in generated if 'ê°€' <= c <= 'í£')
            total_chars = len(generated.replace(' ', ''))
            korean_ratio = korean_chars / total_chars if total_chars > 0 else 0
            
            # ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´ ìƒì„± ì—¬ë¶€)
            length_score = min(len(generated) / 20, 1.0) if len(generated) > 0 else 0
            
            # ì¢…í•© ì ìˆ˜
            score = (korean_ratio + length_score) / 2
            generation_score += score
            
            print(f"   '{prompt}' â†’ '{generated[:30]}...' (í•œê¸€:{korean_ratio:.1%}, ì ìˆ˜:{score:.1%})")
            
        except Exception as e:
            print(f"   '{prompt}' â†’ ì˜¤ë¥˜: {e} (0%)")
        
        total_tests += 1
    
    generation_accuracy = generation_score / len(tasks["í•œê¸€ìƒì„±"]) if tasks["í•œê¸€ìƒì„±"] else 0
    total_score += generation_accuracy
    
    # ì „ì²´ ì •í™•ë„
    overall_accuracy = total_score / 3  # 3ê°œ íƒœìŠ¤í¬ í‰ê· 
    
    print(f"\nğŸ“Š í•œêµ­ì–´ ì •í™•ë„ ìš”ì•½:")
    print(f"   ë¬¸ì¥ ì™„ì„±: {completion_accuracy:.1%}")
    print(f"   ë¬¸ë§¥ ì´í•´: {context_accuracy:.1%}")
    print(f"   í•œê¸€ ìƒì„±: {generation_accuracy:.1%}")
    print(f"   ì „ì²´ ì •í™•ë„: {overall_accuracy:.1%}")
    
    return {
        'completion': completion_accuracy,
        'context': context_accuracy,
        'generation': generation_accuracy,
        'overall': overall_accuracy
    }


def accuracy_focused_research():
    """ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì—°êµ¬"""
    
    print("ğŸš€ Reality Stone ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì—°êµ¬")
    print("=" * 80)
    print("   ëª©í‘œ: ì••ì¶•ë¥  30-50% + í•œêµ­ì–´ ì •í™•ë„ 90%+ + í’ˆì§ˆ ë³´ì¡´")
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer, model_name = load_korean_model()
    if model is None:
        return
    
    original_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ“Š ëª¨ë¸ ì •ë³´:")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   í¬ê¸°: {original_params * 4 / (1024**2):.1f}MB")
    
    # ì›ë³¸ ëª¨ë¸ ì •í™•ë„ í‰ê°€
    print(f"\nğŸ“‹ ì›ë³¸ ëª¨ë¸ í‰ê°€")
    print("-" * 60)
    original_accuracy = evaluate_korean_accuracy(model, tokenizer)
    
    # ë‹¤ì–‘í•œ ì••ì¶•ë¥ ë¡œ ì‹¤í—˜
    compression_ratios = [0.3, 0.4, 0.5, 0.6]  # 30%, 40%, 50%, 60%
    
    best_result = None
    best_score = 0
    
    for compression_ratio in compression_ratios:
        print(f"\nğŸ”§ ì••ì¶•ë¥  {compression_ratio:.1%} ì—°êµ¬")
        print("=" * 60)
        
        try:
            # ëª¨ë¸ ë³µì‚¬ ë° ì••ì¶•
            compressed_model = copy.deepcopy(model)
            compressed_model, actual_compression = apply_accuracy_focused_compression(
                compressed_model, compression_ratio
            )
            
            # ì••ì¶• ëª¨ë¸ ì •í™•ë„ í‰ê°€
            print(f"\nğŸ“‹ ì••ì¶• ëª¨ë¸ í‰ê°€")
            print("-" * 40)
            compressed_accuracy = evaluate_korean_accuracy(compressed_model, tokenizer)
            
            # ì„±ëŠ¥ ë¹„êµ
            accuracy_retention = compressed_accuracy['overall'] / original_accuracy['overall'] if original_accuracy['overall'] > 0 else 0
            memory_saved_ratio = 1 - actual_compression
            
            # ì¢…í•© ì ìˆ˜ (ì •í™•ë„ ë³´ì¡´ * ì••ì¶•ë¥ )
            overall_score = accuracy_retention * memory_saved_ratio
            
            print(f"\nğŸ“Š ì••ì¶•ë¥  {compression_ratio:.1%} ê²°ê³¼:")
            print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {actual_compression:.1%}")
            print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved_ratio:.1%}")
            print(f"   ì •í™•ë„ ë³´ì¡´: {accuracy_retention:.1%}")
            print(f"   ë¬¸ì¥ì™„ì„± ë³´ì¡´: {compressed_accuracy['completion'] / original_accuracy['completion']:.1%}")
            print(f"   ë¬¸ë§¥ì´í•´ ë³´ì¡´: {compressed_accuracy['context'] / original_accuracy['context']:.1%}")
            print(f"   í•œê¸€ìƒì„± ë³´ì¡´: {compressed_accuracy['generation'] / original_accuracy['generation']:.1%}")
            print(f"   ì¢…í•© ì ìˆ˜: {overall_score:.3f}")
            
            # ìµœê³  ì„±ëŠ¥ ê¸°ë¡
            if overall_score > best_score:
                best_score = overall_score
                best_result = {
                    'compression_ratio': compression_ratio,
                    'actual_compression': actual_compression,
                    'memory_saved': memory_saved_ratio,
                    'accuracy_retention': accuracy_retention,
                    'original_accuracy': original_accuracy,
                    'compressed_accuracy': compressed_accuracy,
                    'overall_score': overall_score
                }
            
        except Exception as e:
            print(f"   âŒ ì••ì¶• ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # ìµœì¢… ì—°êµ¬ ê²°ê³¼
    print(f"\nğŸ† ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì—°êµ¬ ê²°ê³¼")
    print("=" * 80)
    
    if best_result:
        print(f"ğŸ¥‡ ìµœê³  ì„±ëŠ¥:")
        print(f"   ëª©í‘œ ì••ì¶•ë¥ : {best_result['compression_ratio']:.1%}")
        print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {best_result['actual_compression']:.1%}")
        print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {best_result['memory_saved']:.1%}")
        print(f"   ì „ì²´ ì •í™•ë„ ë³´ì¡´: {best_result['accuracy_retention']:.1%}")
        print(f"   ë¬¸ì¥ì™„ì„± ë³´ì¡´: {best_result['compressed_accuracy']['completion'] / best_result['original_accuracy']['completion']:.1%}")
        print(f"   ë¬¸ë§¥ì´í•´ ë³´ì¡´: {best_result['compressed_accuracy']['context'] / best_result['original_accuracy']['context']:.1%}")
        print(f"   í•œê¸€ìƒì„± ë³´ì¡´: {best_result['compressed_accuracy']['generation'] / best_result['original_accuracy']['generation']:.1%}")
        print(f"   ì¢…í•© ì ìˆ˜: {best_result['overall_score']:.3f}")
        
        print(f"\nğŸ¯ ì—°êµ¬ ëª©í‘œ ë‹¬ì„±ë„:")
        compress_ok = best_result['memory_saved'] >= 0.3  # 30% ì••ì¶•ë¥ 
        accuracy_ok = best_result['accuracy_retention'] >= 0.9  # 90% ì •í™•ë„ ë³´ì¡´
        quality_ok = best_result['compressed_accuracy']['generation'] / best_result['original_accuracy']['generation'] >= 0.85  # 85% í’ˆì§ˆ ë³´ì¡´
        
        print(f"   ì••ì¶•ë¥ : {'âœ…' if compress_ok else 'âš ï¸'} (ëª©í‘œ: 30%+, ë‹¬ì„±: {best_result['memory_saved']:.1%})")
        print(f"   ì •í™•ë„ ë³´ì¡´: {'âœ…' if accuracy_ok else 'âš ï¸'} (ëª©í‘œ: 90%+, ë‹¬ì„±: {best_result['accuracy_retention']:.1%})")
        print(f"   í’ˆì§ˆ ë³´ì¡´: {'âœ…' if quality_ok else 'âš ï¸'} (ëª©í‘œ: 85%+, ë‹¬ì„±: {best_result['compressed_accuracy']['generation'] / best_result['original_accuracy']['generation']:.1%})")
        
        if compress_ok and accuracy_ok and quality_ok:
            print(f"\nğŸ‰ ëª¨ë“  ì—°êµ¬ ëª©í‘œ ë‹¬ì„±! ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì„±ê³µ!")
        else:
            print(f"\nğŸ”¬ ì¼ë¶€ ëª©í‘œ ë¯¸ë‹¬ì„±, ì¶”ê°€ ì—°êµ¬ í•„ìš”")
    else:
        print("âŒ ì„±ê³µì ì¸ ì••ì¶• ê²°ê³¼ ì—†ìŒ")
    
    print(f"\nâœ… ì •í™•ë„ ì¤‘ì‹¬ ì••ì¶• ì—°êµ¬ ì™„ë£Œ!")


if __name__ == "__main__":
    accuracy_focused_research() 