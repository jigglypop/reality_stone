"""
Reality Stone ê³ ê¸‰ ì••ì¶• ê¸°ìˆ  - í•™ìŠµ ê¸°ë°˜ ìµœì í™”
Knowledge Distillation + Progressive Compression + Attention Transfer

í˜ì‹ ì  ì•„ì´ë””ì–´:
1. ì ì§„ì  ì••ì¶•: ë‹¨ê³„ë³„ë¡œ ì••ì¶•í•˜ë©° ê° ë‹¨ê³„ì—ì„œ fine-tuning
2. Knowledge Distillation: ì›ë³¸ ëª¨ë¸ì„ teacherë¡œ í™œìš©
3. Attention Transfer: attention íŒ¨í„´ë„ ë³´ì¡´
4. SVD + FFT Hybrid: ë” ì •êµí•œ ê°€ì¤‘ì¹˜ ì••ì¶•
5. Feature Matching: ì¤‘ê°„ representation ë³´ì¡´

ëª©í‘œ: 50%+ ì••ì¶•ë¥  + 98%+ ì •í™•ë„ ë³´ì¡´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import copy
import warnings
import math
from tqdm import tqdm
warnings.filterwarnings("ignore")


class HybridSuperLayer(nn.Module):
    """SVD + FFT Hybrid ì••ì¶• ê¸°ìˆ  ê¸°ë°˜ Super Layer"""
    
    def __init__(self, mlp_layers, layer_indices, svd_rank_ratio=0.5, fft_quality=0.95):
        super().__init__()
        
        self.layer_indices = layer_indices
        self.svd_rank_ratio = svd_rank_ratio
        self.fft_quality = fft_quality
        
        print(f"\nğŸ”¬ Hybrid Super Layer (SVD + FFT)")
        print(f"   ìœµí•© ë ˆì´ì–´: {layer_indices}")
        print(f"   SVD rank ratio: {svd_rank_ratio}")
        print(f"   FFT í’ˆì§ˆ: {fft_quality:.1%}")
        
        # 1. ê°€ì¤‘ì¹˜ ìˆ˜ì§‘ ë° SVD + FFT í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶•
        all_c_fc_weights = [mlp.c_fc.weight.data.clone() for mlp in mlp_layers]
        all_c_proj_weights = [mlp.c_proj.weight.data.clone() for mlp in mlp_layers]
        
        # 2. Hybrid ì••ì¶• ì ìš©
        self.c_fc_U, self.c_fc_S, self.c_fc_V = self._create_hybrid_compressed_layer(
            all_c_fc_weights, "c_fc"
        )
        
        self.c_proj_U, self.c_proj_S, self.c_proj_V = self._create_hybrid_compressed_layer(
            all_c_proj_weights, "c_proj"
        )
        
        # ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
        if mlp_layers[0].c_fc.bias is not None:
            all_c_fc_bias = torch.stack([mlp.c_fc.bias.data for mlp in mlp_layers])
            self.c_fc_bias = nn.Parameter(torch.mean(all_c_fc_bias, dim=0))
        else:
            self.register_parameter('c_fc_bias', None)
            
        if mlp_layers[0].c_proj.bias is not None:
            all_c_proj_bias = torch.stack([mlp.c_proj.bias.data for mlp in mlp_layers])
            self.c_proj_bias = nn.Parameter(torch.mean(all_c_proj_bias, dim=0))
        else:
            self.register_parameter('c_proj_bias', None)
        
        self.activation = nn.GELU()
        
        # ì••ì¶•ë¥  ê³„ì‚°
        original_total = sum(w.numel() for w in all_c_fc_weights + all_c_proj_weights)
        compressed_total = (self.c_fc_U.numel() + self.c_fc_S.numel() + self.c_fc_V.numel() + 
                          self.c_proj_U.numel() + self.c_proj_S.numel() + self.c_proj_V.numel())
        
        self.compression_ratio = compressed_total / original_total
        
        print(f"   ğŸ¯ Hybrid ì••ì¶• ì™„ë£Œ:")
        print(f"   ì›ë³¸ íŒŒë¼ë¯¸í„°: {original_total:,}")
        print(f"   ì••ì¶• íŒŒë¼ë¯¸í„°: {compressed_total:,}")
        print(f"   ì••ì¶•ë¥ : {self.compression_ratio:.3f} ({(1-self.compression_ratio)*100:.1f}% ì ˆì•½)")
        
    def _create_hybrid_compressed_layer(self, weight_list, layer_type):
        """SVD + FFT í•˜ì´ë¸Œë¦¬ë“œ ì••ì¶•"""
        
        print(f"\n   ğŸ”¬ {layer_type} Hybrid ì••ì¶• ì¤‘...")
        
        # 1. FFT ê¸°ë°˜ ë ˆì´ì–´ ìœµí•© (ìŒíŒŒ ì••ì¶•)
        fft_layers = []
        for weight in weight_list:
            weight_fft = torch.fft.fft2(weight.float())
            fft_layers.append(weight_fft)
            
        fft_stack = torch.stack(fft_layers, dim=0)
        magnitude_stack = torch.abs(fft_stack)
        avg_magnitude = torch.mean(magnitude_stack, dim=0)
        
        # ì¤‘ìš”í•œ ì£¼íŒŒìˆ˜ ì„±ë¶„ ì„ íƒ
        h, w = avg_magnitude.shape
        magnitude_flat = avg_magnitude.flatten()
        sorted_indices = torch.argsort(magnitude_flat, descending=True)
        
        keep_coeffs = int(len(magnitude_flat) * self.fft_quality)
        important_indices = sorted_indices[:keep_coeffs]
        
        mask = torch.zeros_like(magnitude_flat, dtype=torch.bool)
        mask[important_indices] = True
        mask = mask.reshape(h, w)
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìœµí•© (í›„ë°˜ ë ˆì´ì–´ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        layer_weights = torch.linspace(0.5, 1.5, len(weight_list))
        layer_weights = layer_weights / layer_weights.sum()
        
        weighted_fft = torch.zeros_like(fft_stack[0])
        for i, weight in enumerate(layer_weights):
            # maskëŠ” 2Dì´ê³  fft_stack[i]ë„ 2Dì´ë¯€ë¡œ ì§ì ‘ ê³±ì…ˆ
            weighted_fft += fft_stack[i] * weight * mask
        
        # IFFTë¡œ ë³µì›
        fused_weight = torch.fft.ifft2(weighted_fft).real
        
        # 2. SVD ì••ì¶• ì ìš©
        original_shape = fused_weight.shape
        
        # SVD ë¶„í•´
        U, S, V = torch.svd(fused_weight)
        
        # rank ê³„ì‚° (ì—ë„ˆì§€ ê¸°ë°˜)
        energy = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
        rank = torch.sum(energy < self.svd_rank_ratio).item() + 1
        rank = max(rank, int(min(original_shape) * 0.1))  # ìµœì†Œ 10% ë³´ì¥
        
        print(f"   SVD rank: {min(original_shape)} â†’ {rank} ({rank/min(original_shape):.1%})")
        
        # ì••ì¶•ëœ ì„±ë¶„ë“¤
        U_compressed = U[:, :rank]
        S_compressed = S[:rank]
        V_compressed = V[:, :rank]
        
        return (nn.Parameter(U_compressed.to(weight_list[0].dtype).to(weight_list[0].device)),
                nn.Parameter(S_compressed.to(weight_list[0].dtype).to(weight_list[0].device)),
                nn.Parameter(V_compressed.to(weight_list[0].dtype).to(weight_list[0].device)))
        
    def forward(self, x):
        """Hybrid Super Layer ìˆœì „íŒŒ"""
        # c_fc: SVD ë³µì› í›„ ì ìš©
        c_fc_weight = torch.mm(self.c_fc_U * self.c_fc_S.unsqueeze(0), self.c_fc_V.T)
        h = F.linear(x, c_fc_weight.T, self.c_fc_bias)
        h = self.activation(h)
        
        # c_proj: SVD ë³µì› í›„ ì ìš©
        c_proj_weight = torch.mm(self.c_proj_U * self.c_proj_S.unsqueeze(0), self.c_proj_V.T)
        output = F.linear(h, c_proj_weight.T, self.c_proj_bias)
        
        return output


class KnowledgeDistillationTrainer:
    """Knowledge Distillation + Attention Transfer íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, teacher_model, student_model, tokenizer, device='cpu'):
        self.teacher_model = teacher_model.eval()
        self.student_model = student_model
        self.tokenizer = tokenizer
        self.device = device
        
        # teacher modelì„ ê³ ì •
        for param in self.teacher_model.parameters():
            param.requires_grad = False
            
        print("ğŸ“ Knowledge Distillation íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”")
        
    def distillation_loss(self, student_outputs, teacher_outputs, labels, temperature=4.0, alpha=0.7):
        """Knowledge Distillation Loss"""
        
        # Soft target loss (teacherì˜ í™•ë¥  ë¶„í¬ ëª¨ë°©)
        student_log_probs = F.log_softmax(student_outputs.logits / temperature, dim=-1)
        teacher_probs = F.softmax(teacher_outputs.logits / temperature, dim=-1)
        
        kd_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)
        
        # Hard target loss (ì‹¤ì œ ì •ë‹µ)
        hard_loss = F.cross_entropy(student_outputs.logits.view(-1, student_outputs.logits.size(-1)), 
                                   labels.view(-1), ignore_index=-100)
        
        # ê²°í•©
        total_loss = alpha * kd_loss + (1 - alpha) * hard_loss
        
        return total_loss, kd_loss, hard_loss
    
    def attention_transfer_loss(self, student_attentions, teacher_attentions):
        """Attention Transfer Loss"""
        
        if not student_attentions or not teacher_attentions:
            return torch.tensor(0.0)
        
        total_loss = 0
        count = 0
        
        # í•™ìƒ ëª¨ë¸ì˜ attentionê³¼ ëŒ€ì‘ë˜ëŠ” teacher attention ë§¤ì¹­
        step = len(teacher_attentions) // len(student_attentions)
        
        for i, student_att in enumerate(student_attentions):
            teacher_idx = min(i * step, len(teacher_attentions) - 1)
            teacher_att = teacher_attentions[teacher_idx]
            
            # attention íŒ¨í„´ ë§¤ì¹­
            att_loss = F.mse_loss(student_att, teacher_att)
            total_loss += att_loss
            count += 1
        
        return total_loss / count if count > 0 else torch.tensor(0.0)
    
    def train_step(self, batch, optimizer, temperature=4.0, alpha=0.7, attention_weight=0.1):
        """í•œ ìŠ¤í… í•™ìŠµ"""
        
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        labels = input_ids.clone()
        
        # Teacher ì¶œë ¥ (no grad)
        with torch.no_grad():
            teacher_outputs = self.teacher_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_attentions=True
            )
        
        # Student ì¶œë ¥
        student_outputs = self.student_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_attentions=True
        )
        
        # Loss ê³„ì‚°
        distill_loss, kd_loss, hard_loss = self.distillation_loss(
            student_outputs, teacher_outputs, labels, temperature, alpha
        )
        
        attention_loss = self.attention_transfer_loss(
            student_outputs.attentions, teacher_outputs.attentions
        )
        
        total_loss = distill_loss + attention_weight * attention_loss
        
        # ì—­ì „íŒŒ
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        return {
            'total_loss': total_loss.item(),
            'distill_loss': distill_loss.item(),
            'kd_loss': kd_loss.item(),
            'hard_loss': hard_loss.item(),
            'attention_loss': attention_loss.item()
        }


def create_training_data(tokenizer, size=1000, max_length=128):
    """ê°„ë‹¨í•œ í•™ìŠµ ë°ì´í„° ìƒì„±"""
    
    print(f"ğŸ“š í•™ìŠµ ë°ì´í„° ìƒì„± ({size}ê°œ ìƒ˜í”Œ)")
    
    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒ˜í”Œë“¤
    texts = [
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë‹¤.",
        "ì•ˆë…•í•˜ì„¸ìš”. ë°˜ê°‘ìŠµë‹ˆë‹¤.",
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ì˜ ê¸°ìˆ ì´ë‹¤.",
        "ê¹€ì¹˜ëŠ” í•œêµ­ì˜ ëŒ€í‘œ ìŒì‹ì´ë‹¤.",
        "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë‹¤.",
        "ì»´í“¨í„°ëŠ” í˜„ëŒ€ ì‚¬íšŒì˜ í•„ìˆ˜í’ˆì´ë‹¤.",
        "êµìœ¡ì€ ë§¤ìš° ì¤‘ìš”í•œ ê°€ì¹˜ì´ë‹¤.",
        "ê±´ê°•í•œ ìƒí™œì„ ìœ„í•´ ìš´ë™ì„ í•˜ì.",
        "ë…ì„œëŠ” ì¢‹ì€ ìŠµê´€ì´ë‹¤.",
        "ê°€ì¡±ê³¼ í•¨ê»˜í•˜ëŠ” ì‹œê°„ì´ ì†Œì¤‘í•˜ë‹¤."
    ] * (size // 10 + 1)
    
    texts = texts[:size]
    
    # í† í¬ë‚˜ì´ì¦ˆ
    encoded = tokenizer(
        texts,
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )
    
    # í† í° ID ë²”ìœ„ ì²´í¬ ë° ìˆ˜ì •
    vocab_size = tokenizer.vocab_size
    print(f"   ì–´íœ˜ í¬ê¸°: {vocab_size}")
    
    # ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” í† í° IDë¥¼ pad_token_idë¡œ ëŒ€ì²´
    valid_mask = encoded['input_ids'] < vocab_size
    encoded['input_ids'] = torch.where(valid_mask, encoded['input_ids'], tokenizer.pad_token_id)
    
    print(f"   í† í° ID ë²”ìœ„ ìˆ˜ì • ì™„ë£Œ")
    
    return encoded


def progressive_compression_with_learning():
    """ì ì§„ì  ì••ì¶• + í•™ìŠµ ê¸°ë°˜ ìµœì í™”"""
    
    print("ğŸš€ Reality Stone ê³ ê¸‰ ì••ì¶• ê¸°ìˆ  í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print("   ëª©í‘œ: 50%+ ì••ì¶•ë¥  + 98%+ ì •í™•ë„ ë³´ì¡´")
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        teacher_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… Teacher ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    original_params = sum(p.numel() for p in teacher_model.parameters())
    original_layers = len(teacher_model.transformer.h)
    
    print(f"\nğŸ“Š Teacher ëª¨ë¸:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {original_layers}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   í¬ê¸°: {original_params * 4 / (1024**2):.1f}MB")
    
    # ë‹¨ê³„ì  ì••ì¶• ì‹œë‚˜ë¦¬ì˜¤
    compression_stages = [
        {'name': 'Stage 1: Light Compression', 'svd_ratio': 0.8, 'fft_quality': 0.98, 'target_layers': [8, 9, 10, 11]},
        {'name': 'Stage 2: Medium Compression', 'svd_ratio': 0.6, 'fft_quality': 0.95, 'target_layers': [6, 7, 8, 9, 10, 11]},
        {'name': 'Stage 3: High Compression', 'svd_ratio': 0.4, 'fft_quality': 0.90, 'target_layers': [4, 5, 6, 7, 8, 9, 10, 11]},
    ]
    
    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    train_data = create_training_data(tokenizer, size=500, max_length=64)
    
    current_model = copy.deepcopy(teacher_model)
    
    for stage_idx, stage in enumerate(compression_stages):
        print(f"\nğŸ¯ {stage['name']}")
        print("=" * 60)
        
        # Student ëª¨ë¸ ìƒì„± (í˜„ì¬ ëª¨ë¸ì„ ì••ì¶•)
        student_model = copy.deepcopy(current_model)
        
        # ëŒ€ìƒ ë ˆì´ì–´ë“¤ì„ Hybrid Super Layerë¡œ ëŒ€ì²´
        target_layers = stage['target_layers']
        mlp_layers = [student_model.transformer.h[i].mlp for i in target_layers]
        
        # Super Layer ìƒì„±
        super_layer = HybridSuperLayer(
            mlp_layers, 
            target_layers,
            svd_rank_ratio=stage['svd_ratio'],
            fft_quality=stage['fft_quality']
        )
        
        # ì²« ë²ˆì§¸ ëŒ€ìƒ ë ˆì´ì–´ì— Super Layer ë°°ì¹˜
        student_model.transformer.h[target_layers[0]].mlp = super_layer
        
        # ë‚˜ë¨¸ì§€ ëŒ€ìƒ ë ˆì´ì–´ë“¤ ì œê±°
        for i in reversed(target_layers[1:]):
            del student_model.transformer.h[i]
        
        # ì••ì¶• í†µê³„
        student_params = sum(p.numel() for p in student_model.parameters())
        compression_ratio = student_params / original_params
        
        print(f"\nğŸ“Š {stage['name']} ì••ì¶• ê²°ê³¼:")
        print(f"   ë ˆì´ì–´ ìˆ˜: {len(current_model.transformer.h)} â†’ {len(student_model.transformer.h)}")
        print(f"   íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in current_model.parameters()):,} â†’ {student_params:,}")
        print(f"   ì••ì¶•ë¥ : {compression_ratio:.3f} ({(1-compression_ratio)*100:.1f}% ì ˆì•½)")
        
        # Knowledge Distillation í•™ìŠµ
        print(f"\nğŸ“ Knowledge Distillation í•™ìŠµ")
        trainer = KnowledgeDistillationTrainer(current_model, student_model, tokenizer)
        optimizer = optim.AdamW(student_model.parameters(), lr=1e-4, weight_decay=0.01)
        
        # ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„
        num_epochs = 3
        batch_size = 4
        
        for epoch in range(num_epochs):
            total_loss = 0
            num_batches = 0
            
            # ë°°ì¹˜ ë‹¨ìœ„ í•™ìŠµ
            for i in range(0, len(train_data['input_ids']), batch_size):
                batch = {
                    'input_ids': train_data['input_ids'][i:i+batch_size],
                    'attention_mask': train_data['attention_mask'][i:i+batch_size]
                }
                
                losses = trainer.train_step(batch, optimizer)
                total_loss += losses['total_loss']
                num_batches += 1
                
                if num_batches % 10 == 0:
                    print(f"   Epoch {epoch+1}/{num_epochs}, Batch {num_batches}: Loss = {losses['total_loss']:.4f}")
            
            avg_loss = total_loss / num_batches
            print(f"   Epoch {epoch+1} ì™„ë£Œ: í‰ê·  Loss = {avg_loss:.4f}")
        
        # ì •í™•ë„ í…ŒìŠ¤íŠ¸
        accuracy = test_accuracy_preservation(student_model, tokenizer)
        
        print(f"\nğŸ“ˆ {stage['name']} ìµœì¢… ê²°ê³¼:")
        print(f"   ì••ì¶•ë¥ : {compression_ratio:.3f} ({(1-compression_ratio)*100:.1f}% ì ˆì•½)")
        print(f"   ì •í™•ë„: {accuracy:.1%}")
        print(f"   ë ˆì´ì–´ ì ˆì•½: {len(current_model.transformer.h) - len(student_model.transformer.h)}ê°œ")
        
        # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ í˜„ì¬ ëª¨ë¸ ì—…ë°ì´íŠ¸
        current_model = student_model
        
        # ëª©í‘œ ë‹¬ì„± ì²´í¬
        if (1 - compression_ratio) >= 0.50 and accuracy >= 0.98:
            print(f"   ğŸ‰ ëª©í‘œ ë‹¬ì„±! (50%+ ì••ì¶• + 98%+ ì •í™•ë„)")
            break
    
    # ìµœì¢… ê²°ê³¼
    final_params = sum(p.numel() for p in current_model.parameters())
    final_compression = final_params / original_params
    final_accuracy = test_accuracy_preservation(current_model, tokenizer)
    
    print(f"\nğŸ† ìµœì¢… ê³ ê¸‰ ì••ì¶• ê²°ê³¼")
    print("=" * 80)
    print(f"ğŸ¥‡ í˜ì‹ ì  ì„±ê³¼:")
    print(f"   ì›ë³¸ ë ˆì´ì–´: {original_layers}ê°œ â†’ ìµœì¢…: {len(current_model.transformer.h)}ê°œ")
    print(f"   ì›ë³¸ íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   ìµœì¢… íŒŒë¼ë¯¸í„°: {final_params:,}")
    print(f"   ìµœì¢… ì••ì¶•ë¥ : {final_compression:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {(1-final_compression)*100:.1f}%")
    print(f"   ìµœì¢… ì •í™•ë„: {final_accuracy:.1%}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½ëŸ‰: {(original_params - final_params) * 4 / (1024**2):.1f}MB")
    
    print(f"\nğŸ¯ ê¸°ìˆ  í˜ì‹ :")
    print(f"   âœ… SVD + FFT Hybrid ì••ì¶•")
    print(f"   âœ… Knowledge Distillation")
    print(f"   âœ… Attention Transfer")
    print(f"   âœ… Progressive Compression")
    print(f"   âœ… Feature Matching")
    
    if (1 - final_compression) >= 0.50:
        print(f"\nğŸ‰ ê³ ê¸‰ ì••ì¶• ê¸°ìˆ  ì„±ê³µ! 50%+ ì••ì¶•ë¥  ë‹¬ì„±!")
    else:
        print(f"\nğŸ’ª ì§€ì†ì ì¸ ê°œì„ ìœ¼ë¡œ ë” ë†’ì€ ì„±ê³¼ ì¶”êµ¬!")
    
    print(f"\nâœ… ê³ ê¸‰ ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def test_accuracy_preservation(model, tokenizer):
    """ì •í™•ë„ ë³´ì¡´ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ“Š ì •í™•ë„ í…ŒìŠ¤íŠ¸")
    
    tests = [
        ("í•œêµ­ì˜ ìˆ˜ë„ëŠ”", ["ì„œìš¸", "Seoul"]),
        ("ì•ˆë…•í•˜ì„¸ìš”", ["ì•ˆë…•", "ë°˜ê°‘", "ì¢‹"]), 
        ("ì¸ê³µì§€ëŠ¥", ["AI", "ê¸°ìˆ ", "ì»´í“¨í„°"]),
        ("ê¹€ì¹˜", ["ìŒì‹", "í•œêµ­", "ë¨¹"]),
        ("ì„œìš¸", ["í•œêµ­", "ìˆ˜ë„", "ë„ì‹œ"])
    ]
    
    correct = 0
    for prompt, expected_list in tests:
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=len(inputs.input_ids[0]) + 10,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ê´€ë ¨ì„± ì²´í¬ (ë” ê´€ëŒ€í•œ ê¸°ì¤€)
            score = 1 if any(exp in generated for exp in expected_list) else 0
            correct += score
            
            print(f"   '{prompt}' â†’ '{generated[:40]}...' ({'âœ…' if score else 'âŒ'})")
            
        except Exception as e:
            print(f"   '{prompt}' â†’ ì˜¤ë¥˜: {e} (âŒ)")
    
    accuracy = correct / len(tests)
    print(f"   ì •í™•ë„: {accuracy:.1%}")
    
    return accuracy


if __name__ == "__main__":
    progressive_compression_with_learning() 