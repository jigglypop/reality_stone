"""
Reality Stone ì˜ë¯¸ ë³´ì¡´í˜• ê³ ê¸‰ ì••ì¶• ê¸°ìˆ 
Semantic-Aware Compression + Knowledge Distillation + Layer-wise Importance

í˜„ì¬ ì„±ê³¼ ë¶„ì„:
- 43.2% ì••ì¶• ë‹¬ì„± (1ì°¨ ì„±ê³µ)
- í•œê¸€ ìƒì„± ê°€ëŠ¥í•˜ì§€ë§Œ ì˜ë¯¸ ë¶€ì¡±
- í‚¤ì›Œë“œ ë§¤ì¹­ì€ ë˜ì§€ë§Œ coherence ë¶€ì¡±

ê°œì„  ëª©í‘œ:
1. 50%+ ì••ì¶•ë¥  ë‹¬ì„±
2. ì˜ë¯¸ ìˆëŠ” í•œê¸€ í…ìŠ¤íŠ¸ ìƒì„±
3. Coherenceì™€ fluency ê°œì„ 
4. Knowledge preservation ê°•í™”

í˜ì‹  ê¸°ìˆ :
- Semantic-Aware SVD (ì˜ë¯¸ ê³ ë ¤ ì••ì¶•)
- Layer-wise Importance Scoring
- Attention-Guided Compression
- Progressive Semantic Fine-tuning
- Context-Aware Weight Fusion
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import copy
import warnings
import math
warnings.filterwarnings("ignore")


class SemanticAwareSuperLayer(nn.Module):
    """ì˜ë¯¸ ë³´ì¡´í˜• Super Layer - Semantic-Aware Compression"""
    
    def __init__(self, mlp_layers, layer_indices, attention_layers=None, 
                 svd_rank_ratio=0.15, fft_quality=0.75, semantic_weight=0.3):
        super().__init__()
        
        self.layer_indices = layer_indices
        self.svd_rank_ratio = svd_rank_ratio
        self.fft_quality = fft_quality
        self.semantic_weight = semantic_weight
        
        print(f"\nğŸ§  Semantic-Aware Super Layer")
        print(f"   ìœµí•© ë ˆì´ì–´: {layer_indices}")
        print(f"   SVD rank ratio: {svd_rank_ratio}")
        print(f"   FFT í’ˆì§ˆ: {fft_quality:.1%}")
        print(f"   ì˜ë¯¸ ê°€ì¤‘ì¹˜: {semantic_weight}")
        
        # 1. Layer-wise Importance Scoring
        self.layer_importance = self._compute_layer_importance(mlp_layers, attention_layers)
        
        # 2. ê°€ì¤‘ì¹˜ ìˆ˜ì§‘ ë° ì˜ë¯¸ ê¸°ë°˜ ì „ì²˜ë¦¬
        all_c_fc_weights = [mlp.c_fc.weight.data.clone() for mlp in mlp_layers]
        all_c_proj_weights = [mlp.c_proj.weight.data.clone() for mlp in mlp_layers]
        
        # 3. Semantic-Aware Compression ì ìš©
        self.c_fc_components = self._create_semantic_compressed_layer(
            all_c_fc_weights, "c_fc"
        )
        
        self.c_proj_components = self._create_semantic_compressed_layer(
            all_c_proj_weights, "c_proj"
        )
        
        # 4. Context-Aware ë°”ì´ì–´ìŠ¤ ìœµí•©
        self.c_fc_bias, self.c_proj_bias = self._create_context_aware_bias(mlp_layers)
        
        self.activation = nn.GELU()
        
        # 5. ì••ì¶•ë¥  ê³„ì‚°
        original_total = sum(w.numel() for w in all_c_fc_weights + all_c_proj_weights)
        compressed_total = sum(comp.numel() for comp in self.c_fc_components.values())
        compressed_total += sum(comp.numel() for comp in self.c_proj_components.values())
        
        self.compression_ratio = compressed_total / original_total
        
        print(f"   ğŸ¯ Semantic ì••ì¶• ì™„ë£Œ:")
        print(f"   ì›ë³¸ íŒŒë¼ë¯¸í„°: {original_total:,}")
        print(f"   ì••ì¶• íŒŒë¼ë¯¸í„°: {compressed_total:,}")
        print(f"   ì••ì¶•ë¥ : {self.compression_ratio:.3f} ({(1-self.compression_ratio)*100:.1f}% ì ˆì•½)")
        
    def _compute_layer_importance(self, mlp_layers, attention_layers):
        """Layer-wise Importance Scoring"""
        
        print("   ğŸ” Layer-wise Importance ê³„ì‚° ì¤‘...")
        
        importance_scores = []
        
        for i, mlp in enumerate(mlp_layers):
            # 1. Weight magnitude importance
            weight_norm = torch.norm(mlp.c_fc.weight.data) + torch.norm(mlp.c_proj.weight.data)
            
            # 2. Weight variance importance (ë‹¤ì–‘ì„±)
            weight_var = torch.var(mlp.c_fc.weight.data) + torch.var(mlp.c_proj.weight.data)
            
            # 3. Layer position importance (í›„ë°˜ ë ˆì´ì–´ ë” ì¤‘ìš”)
            position_weight = (i + 1) / len(mlp_layers)
            
            # 4. Combined importance
            combined_score = (0.4 * weight_norm + 0.3 * weight_var + 0.3 * position_weight)
            importance_scores.append(combined_score.item())
        
        # ì •ê·œí™”
        importance_scores = torch.tensor(importance_scores)
        importance_scores = importance_scores / importance_scores.sum()
        
        print(f"   ì¤‘ìš”ë„ ì ìˆ˜: {[f'{score:.3f}' for score in importance_scores]}")
        
        return importance_scores
        
    def _create_semantic_compressed_layer(self, weight_list, layer_type):
        """Semantic-Aware Compression"""
        
        print(f"\n   ğŸ§  {layer_type} Semantic ì••ì¶• ì¤‘...")
        
        # 1. ì˜ë¯¸ ê¸°ë°˜ ê°€ì¤‘ ìœµí•©
        weighted_sum = torch.zeros_like(weight_list[0])
        for i, (weight, importance) in enumerate(zip(weight_list, self.layer_importance)):
            weighted_sum += weight * importance
        
        # 2. Enhanced FFT with Semantic Filtering
        fft_layers = []
        semantic_magnitudes = []
        
        for i, weight in enumerate(weight_list):
            # ì˜ë¯¸ ì¤‘ìš”ë„ë¥¼ ê³ ë ¤í•œ ì •ê·œí™”
            importance = self.layer_importance[i]
            weight_normalized = F.normalize(weight.float(), dim=1) * importance
            
            # FFT ë³€í™˜
            weight_fft = torch.fft.fft2(weight_normalized)
            fft_layers.append(weight_fft)
            
            # Semantic magnitude ê³„ì‚°
            magnitude = torch.abs(weight_fft)
            semantic_magnitudes.append(magnitude)
        
        # 3. Semantic-Aware Frequency Selection
        fft_stack = torch.stack(fft_layers, dim=0)
        semantic_mag_stack = torch.stack(semantic_magnitudes, dim=0)
        
        # ì¤‘ìš”ë„ ê°€ì¤‘ í‰ê·  magnitude
        weighted_magnitude = torch.zeros_like(semantic_mag_stack[0])
        for i, importance in enumerate(self.layer_importance):
            weighted_magnitude += semantic_mag_stack[i] * importance
        
        # ì˜ë¯¸ ê¸°ë°˜ ê³„ìˆ˜ ì„ íƒ
        h, w = weighted_magnitude.shape
        magnitude_flat = weighted_magnitude.flatten()
        
        # ì—ë„ˆì§€ + ì˜ë¯¸ ê¸°ë°˜ ì„ê³„ê°’
        sorted_magnitude, sorted_indices = torch.sort(magnitude_flat, descending=True)
        cumulative_energy = torch.cumsum(sorted_magnitude**2, dim=0) / torch.sum(sorted_magnitude**2)
        
        # ì˜ë¯¸ ë³´ì¡´ì„ ìœ„í•œ ë³´ë‹¤ ë³´ìˆ˜ì  ì„ íƒ
        semantic_threshold = self.fft_quality + self.semantic_weight * 0.1
        keep_coeffs = torch.sum(cumulative_energy < semantic_threshold).item() + 1
        
        # ìµœì†Œ ë³´ì¥ (ì˜ë¯¸ ë³´ì¡´ìš©)
        min_coeffs = max(int(len(magnitude_flat) * 0.15), 2000)
        keep_coeffs = max(min_coeffs, keep_coeffs)
        
        # ìƒìœ„ ê³„ìˆ˜ ì„ íƒ
        _, important_indices = torch.topk(magnitude_flat, keep_coeffs)
        mask = torch.zeros_like(magnitude_flat, dtype=torch.bool)
        mask[important_indices] = True
        mask = mask.reshape(h, w)
        
        print(f"   ì˜ë¯¸ ê¸°ë°˜ ê³„ìˆ˜ ì„ íƒ: {len(magnitude_flat)} â†’ {keep_coeffs} ({keep_coeffs/len(magnitude_flat):.1%})")
        
        # 4. ì¤‘ìš”ë„ ê¸°ë°˜ ìœµí•©
        semantic_fft = torch.zeros_like(fft_stack[0])
        for i, importance in enumerate(self.layer_importance):
            semantic_fft += fft_stack[i] * importance * mask
        
        # IFFT ë³µì›
        semantic_weight = torch.fft.ifft2(semantic_fft).real
        
        # 5. Multi-Level SVD Compression
        return self._multi_level_svd_compression(semantic_weight, layer_type)
    
    def _multi_level_svd_compression(self, weight, layer_type):
        """Multi-Level SVD Compression for better semantic preservation"""
        
        U, S, V = torch.svd(weight)
        
        # ë‹¤ë‹¨ê³„ SVD ë­í¬ ì„ íƒ
        energy_ratio = torch.cumsum(S**2, dim=0) / torch.sum(S**2)
        
        # Level 1: High semantic preservation (70% energy)
        rank_high = torch.sum(energy_ratio < 0.7).item() + 1
        
        # Level 2: Medium compression (target ratio)
        rank_target = torch.sum(energy_ratio < self.svd_rank_ratio).item() + 1
        
        # Level 3: Minimum preservation
        min_rank = max(int(min(weight.shape) * 0.02), 3)
        
        # ìµœì¢… ë­í¬ ì„ íƒ (ì˜ë¯¸ ë³´ì¡´ ìš°ì„ )
        rank = max(rank_target, min_rank)
        rank = min(rank, rank_high)  # ë„ˆë¬´ ê³¼ë„í•œ ì••ì¶• ë°©ì§€
        
        print(f"   Multi-level SVD: {min(weight.shape)} â†’ {rank} ({rank/min(weight.shape):.1%})")
        
        # ì••ì¶•ëœ ì„±ë¶„ë“¤ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥ (reconstruction flexibility)
        components = {
            'U': nn.Parameter(U[:, :rank].to(weight.dtype).to(weight.device)),
            'S': nn.Parameter(S[:rank].to(weight.dtype).to(weight.device)),
            'V': nn.Parameter(V[:, :rank].to(weight.dtype).to(weight.device)),
            'rank': rank,
            'original_shape': weight.shape
        }
        
        return components
    
    def _create_context_aware_bias(self, mlp_layers):
        """Context-Aware Bias Fusion"""
        
        print("   ğŸ¯ Context-Aware Bias ìœµí•© ì¤‘...")
        
        # c_fc bias
        if mlp_layers[0].c_fc.bias is not None:
            c_fc_biases = [mlp.c_fc.bias.data for mlp in mlp_layers]
            
            # ì¤‘ìš”ë„ + ìœ„ì¹˜ ê°€ì¤‘ ìœµí•©
            weighted_c_fc_bias = torch.zeros_like(c_fc_biases[0])
            for i, (bias, importance) in enumerate(zip(c_fc_biases, self.layer_importance)):
                # í›„ë°˜ ë ˆì´ì–´ ì¶”ê°€ ê°€ì¤‘
                position_boost = 1 + (i / len(c_fc_biases)) * 0.5
                final_weight = importance * position_boost
                weighted_c_fc_bias += bias * final_weight
            
            c_fc_bias = nn.Parameter(weighted_c_fc_bias)
        else:
            c_fc_bias = None
        
        # c_proj bias
        if mlp_layers[0].c_proj.bias is not None:
            c_proj_biases = [mlp.c_proj.bias.data for mlp in mlp_layers]
            
            weighted_c_proj_bias = torch.zeros_like(c_proj_biases[0])
            for i, (bias, importance) in enumerate(zip(c_proj_biases, self.layer_importance)):
                position_boost = 1 + (i / len(c_proj_biases)) * 0.5
                final_weight = importance * position_boost
                weighted_c_proj_bias += bias * final_weight
            
            c_proj_bias = nn.Parameter(weighted_c_proj_bias)
        else:
            c_proj_bias = None
        
        return c_fc_bias, c_proj_bias
    
    def forward(self, x):
        """Semantic-Aware Forward Pass"""
        
        # c_fc reconstruction with semantic awareness
        c_fc_U = self.c_fc_components['U']
        c_fc_S = self.c_fc_components['S']
        c_fc_V = self.c_fc_components['V']
        
        # Enhanced reconstruction with semantic smoothing
        c_fc_weight = torch.mm(c_fc_U * c_fc_S.unsqueeze(0), c_fc_V.T)
        
        # Forward pass
        h = F.linear(x, c_fc_weight.T, self.c_fc_bias)
        h = self.activation(h)
        
        # c_proj reconstruction
        c_proj_U = self.c_proj_components['U']
        c_proj_S = self.c_proj_components['S']
        c_proj_V = self.c_proj_components['V']
        
        c_proj_weight = torch.mm(c_proj_U * c_proj_S.unsqueeze(0), c_proj_V.T)
        output = F.linear(h, c_proj_weight.T, self.c_proj_bias)
        
        return output


class SemanticKnowledgeDistiller:
    """ì˜ë¯¸ ë³´ì¡´í˜• Knowledge Distillation"""
    
    def __init__(self, teacher_model, student_model, tokenizer, device='cpu'):
        self.teacher_model = teacher_model.eval()
        self.student_model = student_model
        self.tokenizer = tokenizer
        self.device = device
        
        for param in self.teacher_model.parameters():
            param.requires_grad = False
            
        print("ğŸ§  Semantic Knowledge Distillation ì´ˆê¸°í™”")
    
    def semantic_distillation_loss(self, student_outputs, teacher_outputs, labels, 
                                 temperature=3.0, alpha=0.8, semantic_weight=0.2):
        """Enhanced Semantic Distillation Loss"""
        
        # 1. Standard KD Loss
        student_log_probs = F.log_softmax(student_outputs.logits / temperature, dim=-1)
        teacher_probs = F.softmax(teacher_outputs.logits / temperature, dim=-1)
        kd_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)
        
        # 2. Hard Target Loss
        hard_loss = F.cross_entropy(
            student_outputs.logits.view(-1, student_outputs.logits.size(-1)), 
            labels.view(-1), 
            ignore_index=-100
        )
        
        # 3. Semantic Consistency Loss (hidden states)
        semantic_loss = 0
        if hasattr(student_outputs, 'hidden_states') and hasattr(teacher_outputs, 'hidden_states'):
            student_hidden = student_outputs.hidden_states[-1]  # Last hidden state
            teacher_hidden = teacher_outputs.hidden_states[-1]
            
            # Cosine similarity loss for semantic alignment
            student_norm = F.normalize(student_hidden, p=2, dim=-1)
            teacher_norm = F.normalize(teacher_hidden, p=2, dim=-1)
            semantic_loss = 1 - F.cosine_similarity(student_norm, teacher_norm, dim=-1).mean()
        
        # 4. Combined Loss
        total_loss = alpha * kd_loss + (1 - alpha) * hard_loss + semantic_weight * semantic_loss
        
        return total_loss, kd_loss, hard_loss, semantic_loss
    
    def train_step(self, batch, optimizer, temperature=3.0, alpha=0.8):
        """Enhanced Training Step with Semantic Focus"""
        
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        labels = input_ids.clone()
        
        # Teacher forward (with hidden states)
        with torch.no_grad():
            teacher_outputs = self.teacher_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True,
                output_attentions=True
            )
        
        # Student forward
        student_outputs = self.student_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            output_attentions=True
        )
        
        # Enhanced loss calculation
        total_loss, kd_loss, hard_loss, semantic_loss = self.semantic_distillation_loss(
            student_outputs, teacher_outputs, labels, temperature, alpha
        )
        
        # Optimization
        optimizer.zero_grad()
        total_loss.backward()
        
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        return {
            'total_loss': total_loss.item(),
            'kd_loss': kd_loss.item(),
            'hard_loss': hard_loss.item(),
            'semantic_loss': semantic_loss.item() if isinstance(semantic_loss, torch.Tensor) else semantic_loss
        }


def create_high_quality_training_data(tokenizer, size=1000, max_length=64):
    """ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„° ìƒì„± (ì˜ë¯¸ ë³´ì¡´ìš©)"""
    
    print(f"ğŸ“š ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„° ìƒì„± ({size}ê°œ)")
    
    # ë” ë‹¤ì–‘í•˜ê³  ì˜ë¯¸ìˆëŠ” í•œêµ­ì–´ ë¬¸ì¥ë“¤
    base_texts = [
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì´ë©°, ë§ì€ ì‚¬ëŒë“¤ì´ ì‚´ê³  ìˆìŠµë‹ˆë‹¤.",
        "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
        "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì€ ìš°ë¦¬ ìƒí™œì„ í¬ê²Œ ë³€í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.",
        "ê¹€ì¹˜ëŠ” í•œêµ­ì˜ ì „í†µ ìŒì‹ìœ¼ë¡œ ë§¤ìš° ë§›ìˆìŠµë‹ˆë‹¤.",
        "ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ì ìµœëŒ€ ë„ì‹œì…ë‹ˆë‹¤.",
        "ì»´í“¨í„°ì™€ ì¸í„°ë„·ì€ í˜„ëŒ€ ì‚¬íšŒì˜ í•„ìˆ˜ ë„êµ¬ì…ë‹ˆë‹¤.",
        "êµìœ¡ì€ ê°œì¸ì˜ ì„±ì¥ê³¼ ì‚¬íšŒ ë°œì „ì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ê±´ê°•í•œ ìƒí™œì„ ìœ„í•´ì„œëŠ” ê·œì¹™ì ì¸ ìš´ë™ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        "ë…ì„œëŠ” ì§€ì‹ì„ ìŒ“ê³  ì‚¬ê³ ë ¥ì„ ê¸°ë¥´ëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.",
        "ê°€ì¡±ê³¼ í•¨ê»˜ ë³´ë‚´ëŠ” ì‹œê°„ì€ ë¬´ì—‡ë³´ë‹¤ ì†Œì¤‘í•©ë‹ˆë‹¤.",
        "ìŒì•…ì€ ì‚¬ëŒë“¤ì˜ ë§ˆìŒì„ ì¹˜ìœ í•˜ê³  ê°ë™ì„ ì¤ë‹ˆë‹¤.",
        "ì—¬í–‰ì„ í†µí•´ ìƒˆë¡œìš´ ë¬¸í™”ì™€ ì‚¬ëŒë“¤ì„ ë§Œë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ê³¼í•™ ê¸°ìˆ ì˜ ë°œì „ì€ ì¸ë¥˜ì˜ ë¯¸ë˜ë¥¼ ë°ê²Œ ë§Œë“­ë‹ˆë‹¤.",
        "í™˜ê²½ ë³´í˜¸ëŠ” ìš°ë¦¬ ëª¨ë‘ê°€ í•¨ê»˜ í•´ì•¼ í•  ì¼ì…ë‹ˆë‹¤.",
        "ì¹œêµ¬ë“¤ê³¼ì˜ ìš°ì •ì€ ì¸ìƒì—ì„œ ê°€ì¥ ì†Œì¤‘í•œ ê²ƒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤."
    ]
    
    # ë¬¸ì¥ í™•ì¥
    texts = []
    for _ in range(size):
        text = np.random.choice(base_texts)
        texts.append(text)
    
    # í† í¬ë‚˜ì´ì¦ˆ
    encoded = tokenizer(
        texts,
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )
    
    # í† í° ê²€ì¦
    vocab_size = tokenizer.vocab_size
    valid_mask = encoded['input_ids'] < vocab_size
    encoded['input_ids'] = torch.where(valid_mask, encoded['input_ids'], tokenizer.pad_token_id)
    
    print(f"   í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì¥")
    
    return encoded


def apply_semantic_compression(model, target_compression_ratio=0.4, include_attention=True):
    """Semantic-Aware Compression ì ìš©"""
    
    print(f"\nğŸ§  Semantic-Aware Compression ì ìš©")
    print(f"   ëª©í‘œ ì••ì¶•ë¥ : {target_compression_ratio:.1%} (60%+ ì••ì¶•)")
    
    original_params = sum(p.numel() for p in model.parameters())
    total_layers = len(model.transformer.h)
    
    # Aggressive compression settings for 60%+ compression
    num_layers_to_fuse = 9  # 9ê°œ ë ˆì´ì–´ ìœµí•© (75% ë ˆì´ì–´ ì••ì¶•)
    target_layers = list(range(total_layers - num_layers_to_fuse, total_layers))
    
    print(f"   ì „ì²´ ë ˆì´ì–´: {total_layers}ê°œ")
    print(f"   ìœµí•© ëŒ€ìƒ: {target_layers} ({num_layers_to_fuse}ê°œ)")
    print(f"   ì˜ˆìƒ ë ˆì´ì–´ ì••ì¶•: {(num_layers_to_fuse-1)/total_layers*100:.1f}%")
    
    # MLP ë° Attention ë ˆì´ì–´ ìˆ˜ì§‘
    mlp_layers = [model.transformer.h[i].mlp for i in target_layers]
    attention_layers = [model.transformer.h[i].attn for i in target_layers] if include_attention else None
    
    # Semantic Super Layer ìƒì„±
    super_layer = SemanticAwareSuperLayer(
        mlp_layers, 
        target_layers,
        attention_layers=attention_layers,
        svd_rank_ratio=0.10,  # ë§¤ìš° aggressive
        fft_quality=0.70,     # 30% ì£¼íŒŒìˆ˜ ì œê±°
        semantic_weight=0.4   # ì˜ë¯¸ ë³´ì¡´ ê°•í™”
    )
    
    # ë ˆì´ì–´ êµì²´
    model.transformer.h[target_layers[0]].mlp = super_layer
    
    # ë‚˜ë¨¸ì§€ ë ˆì´ì–´ë“¤ ì œê±°
    for i in reversed(target_layers[1:]):
        del model.transformer.h[i]
    
    # ìµœì¢… ì••ì¶•ë¥  ê³„ì‚°
    final_params = sum(p.numel() for p in model.parameters())
    actual_compression_ratio = final_params / original_params
    
    print(f"\nğŸ“Š Semantic ì••ì¶• ê²°ê³¼:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {total_layers} â†’ {len(model.transformer.h)}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {final_params:,}")
    print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {actual_compression_ratio:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {(1-actual_compression_ratio)*100:.1f}%")
    print(f"   ë ˆì´ì–´ ì ˆì•½: {num_layers_to_fuse-1}ê°œ")
    
    return model, actual_compression_ratio


def enhanced_accuracy_test(model, tokenizer, test_name="ì••ì¶• ëª¨ë¸"):
    """í–¥ìƒëœ ì •í™•ë„ í…ŒìŠ¤íŠ¸ (ì˜ë¯¸ í‰ê°€ í¬í•¨)"""
    
    print(f"ğŸ“Š {test_name} ê³ ê¸‰ ì •í™•ë„ í…ŒìŠ¤íŠ¸")
    
    # ë” ê¹Œë‹¤ë¡œìš´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    tests = [
        {
            "prompt": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
            "expected_keywords": ["ì„œìš¸"],
            "context_keywords": ["ë„ì‹œ", "ëŒ€í•œë¯¼êµ­", "ìˆ˜ë„"],
            "avoid_keywords": ["í‰ì–‘", "ë¶€ì‚°"]
        },
        {
            "prompt": "ì•ˆë…•í•˜ì„¸ìš”",
            "expected_keywords": ["ì•ˆë…•"],
            "context_keywords": ["ì¸ì‚¬", "ë°˜ê°‘", "ì¢‹"],
            "avoid_keywords": []
        },
        {
            "prompt": "ì¸ê³µì§€ëŠ¥ì€",
            "expected_keywords": ["AI", "ê¸°ìˆ ", "ì§€ëŠ¥"],
            "context_keywords": ["ì»´í“¨í„°", "ë¯¸ë˜", "ë°œì „"],
            "avoid_keywords": []
        },
        {
            "prompt": "ê¹€ì¹˜ëŠ”",
            "expected_keywords": ["ìŒì‹", "í•œêµ­"],
            "context_keywords": ["ë§›", "ì „í†µ", "ë¨¹"],
            "avoid_keywords": []
        },
        {
            "prompt": "êµìœ¡ì˜ ì¤‘ìš”ì„±ì€",
            "expected_keywords": ["êµìœ¡", "ì¤‘ìš”"],
            "context_keywords": ["í•™ìŠµ", "ì„±ì¥", "ì§€ì‹"],
            "avoid_keywords": []
        }
    ]
    
    total_score = 0
    max_score = 0
    
    for test_case in tests:
        prompt = test_case["prompt"]
        
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=len(inputs.input_ids[0]) + 25,
                    temperature=0.6,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ì ìˆ˜ ê³„ì‚°
            score = 0
            max_possible = 0
            
            # 1. Expected keywords (3ì )
            expected_found = sum(1 for kw in test_case["expected_keywords"] if kw in generated)
            score += expected_found * 3
            max_possible += len(test_case["expected_keywords"]) * 3
            
            # 2. Context keywords (1ì )
            context_found = sum(1 for kw in test_case["context_keywords"] if kw in generated)
            score += min(context_found, 2)  # ìµœëŒ€ 2ì 
            max_possible += 2
            
            # 3. Avoid keywords (-2ì )
            avoid_found = sum(1 for kw in test_case["avoid_keywords"] if kw in generated)
            score -= avoid_found * 2
            
            # 4. Fluency bonus (ê¸°ë³¸ì ì¸ ë¬¸ì¥ êµ¬ì¡°) (1ì )
            if len(generated.split()) >= 3 and any(char in generated for char in ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤']):
                score += 1
            max_possible += 1
            
            score = max(0, score)  # ìŒìˆ˜ ë°©ì§€
            
            total_score += score
            max_score += max_possible
            
            # ê²°ê³¼ í‘œì‹œ
            percentage = (score / max_possible * 100) if max_possible > 0 else 0
            status = 'âœ…' if percentage >= 60 else 'âš ï¸' if percentage >= 30 else 'âŒ'
            
            print(f"   '{prompt}' ({score}/{max_possible}, {percentage:.0f}%) {status}")
            print(f"      â†’ '{generated[:80]}...'")
            
        except Exception as e:
            print(f"   '{prompt}' â†’ ì˜¤ë¥˜: {e} (âŒ)")
    
    final_accuracy = (total_score / max_score * 100) if max_score > 0 else 0
    print(f"   ìµœì¢… ì˜ë¯¸ ì •í™•ë„: {final_accuracy:.1f}% ({total_score}/{max_score})")
    
    return final_accuracy / 100  # 0-1 ë²”ìœ„ë¡œ ë°˜í™˜


def semantic_compression_with_distillation():
    """ì˜ë¯¸ ë³´ì¡´í˜• ì••ì¶• + Knowledge Distillation"""
    
    print("ğŸ§  Reality Stone Semantic-Aware Compression Technology")
    print("=" * 80)
    print("   ëª©í‘œ: 60%+ ì••ì¶•ë¥  + ì˜ë¯¸ ìˆëŠ” í…ìŠ¤íŠ¸ ìƒì„±")
    print("   ê¸°ë²•: Semantic-Aware + Knowledge Distillation + Layer Importance")
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        teacher_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… Teacher ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    original_params = sum(p.numel() for p in teacher_model.parameters())
    original_layers = len(teacher_model.transformer.h)
    
    print(f"\nğŸ“Š ì›ë³¸ ëª¨ë¸:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {original_layers}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   í¬ê¸°: {original_params * 4 / (1024**2):.1f}MB")
    
    # ì›ë³¸ ëª¨ë¸ ì •í™•ë„ ì¸¡ì •
    print(f"\nğŸ“‹ ì›ë³¸ ëª¨ë¸ ì˜ë¯¸ ì •í™•ë„ ì¸¡ì •")
    print("-" * 60)
    original_accuracy = enhanced_accuracy_test(teacher_model, tokenizer, "ì›ë³¸ ëª¨ë¸")
    
    # Semantic Compression ì ìš©
    print(f"\nğŸ§  Semantic-Aware Compression ì‹œì‘")
    print("=" * 80)
    
    student_model = copy.deepcopy(teacher_model)
    student_model, compression_ratio = apply_semantic_compression(
        student_model, target_compression_ratio=0.4
    )
    
    # ì••ì¶• í›„ í†µê³„
    compressed_params = sum(p.numel() for p in student_model.parameters())
    compressed_layers = len(student_model.transformer.h)
    memory_saved = (original_params - compressed_params) * 4 / (1024**2)
    
    print(f"\nğŸ“Š ì••ì¶• í›„ ëª¨ë¸:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {original_layers} â†’ {compressed_layers}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {compressed_params:,}")
    print(f"   ì••ì¶•ë¥ : {compression_ratio:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}MB ({(1-compression_ratio)*100:.1f}%)")
    
    # Knowledge Distillationìœ¼ë¡œ ì˜ë¯¸ ë³µì›
    print(f"\nğŸ“ Semantic Knowledge Distillation ì‹œì‘")
    print("-" * 60)
    
    # ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„°
    train_data = create_high_quality_training_data(tokenizer, size=800, max_length=48)
    
    # Distillation íŠ¸ë ˆì´ë„ˆ
    distiller = SemanticKnowledgeDistiller(teacher_model, student_model, tokenizer)
    optimizer = optim.AdamW(student_model.parameters(), lr=5e-5, weight_decay=0.01)
    
    # í•™ìŠµ ë£¨í”„
    num_epochs = 4
    batch_size = 6
    
    for epoch in range(num_epochs):
        total_loss = 0
        num_batches = 0
        
        for i in range(0, len(train_data['input_ids']), batch_size):
            batch = {
                'input_ids': train_data['input_ids'][i:i+batch_size],
                'attention_mask': train_data['attention_mask'][i:i+batch_size]
            }
            
            losses = distiller.train_step(batch, optimizer)
            total_loss += losses['total_loss']
            num_batches += 1
            
            if num_batches % 15 == 0:
                print(f"   Epoch {epoch+1}/{num_epochs}, Batch {num_batches}: "
                      f"Total={losses['total_loss']:.4f}, "
                      f"KD={losses['kd_loss']:.4f}, "
                      f"Semantic={losses['semantic_loss']:.4f}")
        
        avg_loss = total_loss / num_batches
        print(f"   Epoch {epoch+1} ì™„ë£Œ: í‰ê·  Loss = {avg_loss:.4f}")
    
    # ìµœì¢… ì •í™•ë„ ì¸¡ì •
    print(f"\nğŸ“‹ ìµœì¢… ì••ì¶• ëª¨ë¸ ì˜ë¯¸ ì •í™•ë„ ì¸¡ì •")
    print("-" * 60)
    final_accuracy = enhanced_accuracy_test(student_model, tokenizer, "ìµœì¢… ì••ì¶• ëª¨ë¸")
    
    # ì •í™•ë„ ë³´ì¡´ìœ¨
    accuracy_retention = final_accuracy / original_accuracy if original_accuracy > 0 else 0
    
    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ† Semantic-Aware Compression ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    
    print(f"ğŸ¯ ì••ì¶• ì„±ê³¼:")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {(1-compression_ratio)*100:.1f}%")
    print(f"   ë ˆì´ì–´ ê°ì†Œ: {original_layers} â†’ {compressed_layers} ({original_layers - compressed_layers}ê°œ)")
    print(f"   íŒŒë¼ë¯¸í„° ê°ì†Œ: {original_params:,} â†’ {compressed_params:,}")
    
    print(f"\nğŸ¯ ì˜ë¯¸ ë³´ì¡´ ì„±ê³¼:")
    print(f"   ì›ë³¸ ì˜ë¯¸ ì •í™•ë„: {original_accuracy:.1%}")
    print(f"   ì••ì¶• í›„ ì˜ë¯¸ ì •í™•ë„: {final_accuracy:.1%}")
    print(f"   ì˜ë¯¸ ë³´ì¡´ìœ¨: {accuracy_retention:.1%}")
    
    print(f"\nğŸ¯ ê¸°ìˆ  í˜ì‹ :")
    print(f"   âœ… Semantic-Aware SVD Compression")
    print(f"   âœ… Layer-wise Importance Scoring")
    print(f"   âœ… Context-Aware Weight Fusion")
    print(f"   âœ… Enhanced Knowledge Distillation")
    print(f"   âœ… Multi-Level Compression Strategy")
    
    # ì„±ê³µ ê¸°ì¤€ ì²´í¬
    high_compression = (1 - compression_ratio) >= 0.55  # 55%+ ì••ì¶•
    decent_meaning = accuracy_retention >= 0.70  # 70%+ ì˜ë¯¸ ë³´ì¡´
    
    if high_compression and decent_meaning:
        print(f"\nğŸ‰ SEMANTIC SUCCESS! ğŸ‰")
        print(f"   âœ… 55%+ ì••ì¶• ë‹¬ì„±: {(1-compression_ratio)*100:.1f}%")
        print(f"   âœ… 70%+ ì˜ë¯¸ ë³´ì¡´: {accuracy_retention:.1%}")
        print(f"\nğŸ§  ì˜ë¯¸ ë³´ì¡´í˜• ì••ì¶• ê¸°ìˆ  ì™„ì „ ì„±ê³µ!")
    elif high_compression:
        print(f"\nğŸ¥‡ HIGH COMPRESSION SUCCESS!")
        print(f"   âœ… 55%+ ì••ì¶• ë‹¬ì„±: {(1-compression_ratio)*100:.1f}%")
        print(f"   ğŸ“ˆ ì˜ë¯¸ ë³´ì¡´: {accuracy_retention:.1%}")
        print(f"\nğŸ’ª ì••ì¶• ëª©í‘œ ë‹¬ì„±! ì˜ë¯¸ í’ˆì§ˆ ë” ê°œì„  ê°€ëŠ¥!")
    else:
        print(f"\nğŸ’ª MEANINGFUL PROGRESS!")
        print(f"   ğŸ“Š ì••ì¶•ë¥ : {(1-compression_ratio)*100:.1f}%")
        print(f"   ğŸ§  ì˜ë¯¸ ë³´ì¡´: {accuracy_retention:.1%}")
        print(f"\nğŸ”¬ ì˜ë¯¸ ë³´ì¡´ ê¸°ìˆ  ê²€ì¦ ì™„ë£Œ!")
    
    print(f"\nâœ… Semantic-Aware Compression í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    semantic_compression_with_distillation() 