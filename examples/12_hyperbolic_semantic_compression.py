"""
Reality Stone ì§„ì •í•œ í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´ ì••ì¶•
Hyperbolic Space Semantic Compression + PoincarÃ© Ball Model

ë¬¸ì œì  ë¶„ì„:
- FFTë¥¼ ì„ í˜• ë ˆì´ì–´ì— ì§ì ‘ ì ìš© â†’ ìœ í´ë¦¬ë“œ ê³µê°„ ì£¼íŒŒìˆ˜ ë¶„ì„
- ì‹ ê²½ë§ì˜ ì˜ë¯¸ì  êµ¬ì¡° ë¬´ì‹œ â†’ ì •ë³´ ì†ì‹¤ í•„ì—°ì 
- í•˜ì´í¼ë³¼ë¦­ ê¸°í•˜í•™ êµ¬ì¡° ì†ì‹¤

ì§„ì •í•œ í•´ê²°ì±…:
1. ê°€ì¤‘ì¹˜ë¥¼ PoincarÃ© ë””ìŠ¤í¬ë¡œ ë§¤í•‘
2. í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œ ì˜ë¯¸ ë³´ì¡´ ë³€í™˜
3. Reality Stone MÃ¶bius ë³€í™˜ í™œìš©
4. ê¸°í•˜í•™ì  ì••ì¶• (í´ëŸ¬ìŠ¤í„°ë§)
5. ì˜ë¯¸ êµ¬ì¡° ë³´ì¡´í•˜ë©° ì••ì¶•

í•µì‹¬ í˜ì‹ :
- Hyperbolic K-means clustering
- PoincarÃ© distance ê¸°ë°˜ ì••ì¶•
- MÃ¶bius transformationìœ¼ë¡œ ì˜ë¯¸ ë³´ì¡´
- Lorentz modelê³¼ PoincarÃ© model ë³€í™˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import copy
import warnings
import sys
import os
warnings.filterwarnings("ignore")

# Reality Stone ë°±ì—”ë“œ ë¡œë“œ (í•„ìˆ˜!)
sys.path.insert(0, '.')

try:
    import reality_stone
    print("âœ… Reality Stone í•˜ì´í¼ë³¼ë¦­ ë°±ì—”ë“œ ë¡œë“œ ì„±ê³µ!")
    
    # í•µì‹¬ í•¨ìˆ˜ë“¤ í™•ì¸
    hyperbolic_funcs = []
    for func_name in dir(reality_stone):
        if any(keyword in func_name.lower() for keyword in ['poincare', 'mobius', 'lorentz', 'hyperbolic']):
            hyperbolic_funcs.append(func_name)
    
    print(f"   í•˜ì´í¼ë³¼ë¦­ í•¨ìˆ˜ë“¤: {hyperbolic_funcs}")
    REALITY_STONE_AVAILABLE = True
    
except ImportError as e:
    print(f"âŒ Reality Stone ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("âŒ í•˜ì´í¼ë³¼ë¦­ ì••ì¶•ì„ ìœ„í•´ Reality Stoneì´ í•„ìˆ˜ì…ë‹ˆë‹¤!")
    REALITY_STONE_AVAILABLE = False


class HyperbolicGeometry:
    """í•˜ì´í¼ë³¼ë¦­ ê¸°í•˜í•™ ì—°ì‚° í´ë˜ìŠ¤"""
    
    @staticmethod
    def poincare_exp_map(v, c=1.0):
        """PoincarÃ© ë””ìŠ¤í¬ì—ì„œ ì§€ìˆ˜ ë§µ"""
        v_norm = torch.norm(v, dim=-1, keepdim=True).clamp(min=1e-8)
        sqrt_c = torch.sqrt(torch.tensor(c, device=v.device))
        
        exp_factor = torch.tanh(sqrt_c * v_norm) / (sqrt_c * v_norm)
        return exp_factor * v
    
    @staticmethod
    def poincare_log_map(x, c=1.0):
        """PoincarÃ© ë””ìŠ¤í¬ì—ì„œ ë¡œê·¸ ë§µ"""
        x_norm = torch.norm(x, dim=-1, keepdim=True).clamp(max=0.99)
        sqrt_c = torch.sqrt(torch.tensor(c, device=x.device))
        
        log_factor = torch.atanh(sqrt_c * x_norm) / (sqrt_c * x_norm + 1e-8)
        return log_factor * x
    
    @staticmethod
    def hyperbolic_distance(x, y, c=1.0):
        """í•˜ì´í¼ë³¼ë¦­ ê±°ë¦¬ ê³„ì‚°"""
        sqrt_c = torch.sqrt(torch.tensor(c, device=x.device))
        
        diff = x - y
        diff_norm_sq = torch.sum(diff * diff, dim=-1)
        
        x_norm_sq = torch.sum(x * x, dim=-1)
        y_norm_sq = torch.sum(y * y, dim=-1)
        
        denominator = (1 - c * x_norm_sq) * (1 - c * y_norm_sq)
        numerator = 2 * diff_norm_sq
        
        denominator = torch.clamp(denominator, min=1e-8)
        ratio = torch.clamp(numerator / denominator, min=0, max=1e6)
        
        distance = (1 / sqrt_c) * torch.acosh(1 + ratio)
        return distance
    
    @staticmethod
    def mobius_add(x, y, c=1.0):
        """MÃ¶bius ë§ì…ˆ (ê·¼ì‚¬)"""
        if REALITY_STONE_AVAILABLE and hasattr(reality_stone, 'mobius_add_cpu'):
            try:
                return reality_stone.mobius_add_cpu(x, y, c)
            except:
                pass
        
        # Fallback êµ¬í˜„
        x_norm_sq = torch.sum(x * x, dim=-1, keepdim=True)
        y_norm_sq = torch.sum(y * y, dim=-1, keepdim=True)
        xy_inner = torch.sum(x * y, dim=-1, keepdim=True)
        
        numerator = (1 + 2 * c * xy_inner + c * y_norm_sq) * x + (1 - c * x_norm_sq) * y
        denominator = 1 + 2 * c * xy_inner + c**2 * x_norm_sq * y_norm_sq
        
        return numerator / (denominator.unsqueeze(-1) + 1e-8)


class HyperbolicSemanticSuperLayer(nn.Module):
    """í•˜ì´í¼ë³¼ë¦­ ê³µê°„ ì˜ë¯¸ ë³´ì¡´í˜• Super Layer"""
    
    def __init__(self, mlp_layers, layer_indices, curvature=1.0, compression_ratio=0.2):
        super().__init__()
        
        self.layer_indices = layer_indices
        self.curvature = curvature
        self.compression_ratio = compression_ratio
        
        print(f"\nğŸŒ€ Hyperbolic Semantic Super Layer")
        print(f"   ìœµí•© ë ˆì´ì–´: {layer_indices}")
        print(f"   ê³¡ë¥ (curvature): {curvature}")
        print(f"   ì••ì¶•ë¥ : {compression_ratio}")
        
        # 1. ê°€ì¤‘ì¹˜ë¥¼ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ìœ¼ë¡œ ë§¤í•‘
        all_c_fc_weights = [mlp.c_fc.weight.data.clone() for mlp in mlp_layers]
        all_c_proj_weights = [mlp.c_proj.weight.data.clone() for mlp in mlp_layers]
        
        # 2. í•˜ì´í¼ë³¼ë¦­ ì••ì¶• ì ìš©
        self.c_fc_hyperbolic = self._hyperbolic_compress_layers(all_c_fc_weights, "c_fc")
        self.c_proj_hyperbolic = self._hyperbolic_compress_layers(all_c_proj_weights, "c_proj")
        
        # 3. ë°”ì´ì–´ìŠ¤ ì²˜ë¦¬
        self.c_fc_bias = self._compress_bias([mlp.c_fc.bias for mlp in mlp_layers if mlp.c_fc.bias is not None])
        self.c_proj_bias = self._compress_bias([mlp.c_proj.bias for mlp in mlp_layers if mlp.c_proj.bias is not None])
        
        self.activation = nn.GELU()
        
        # 4. ì••ì¶•ë¥  ê³„ì‚°
        original_params = sum(w.numel() for w in all_c_fc_weights + all_c_proj_weights)
        compressed_params = (self.c_fc_hyperbolic['representatives'].numel() + 
                           self.c_proj_hyperbolic['representatives'].numel())
        
        self.actual_compression_ratio = compressed_params / original_params
        
        print(f"   ğŸ¯ í•˜ì´í¼ë³¼ë¦­ ì••ì¶• ì™„ë£Œ:")
        print(f"   ì›ë³¸ íŒŒë¼ë¯¸í„°: {original_params:,}")
        print(f"   ì••ì¶• íŒŒë¼ë¯¸í„°: {compressed_params:,}")
        print(f"   ì••ì¶•ë¥ : {self.actual_compression_ratio:.3f} ({(1-self.actual_compression_ratio)*100:.1f}% ì ˆì•½)")
    
    def _hyperbolic_compress_layers(self, weight_list, layer_type):
        """í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œ ë ˆì´ì–´ ì••ì¶•"""
        
        print(f"\n   ğŸŒ€ {layer_type} í•˜ì´í¼ë³¼ë¦­ ì••ì¶•...")
        
        # 1. ê°€ì¤‘ì¹˜ë“¤ì„ PoincarÃ© ë””ìŠ¤í¬ë¡œ ë§¤í•‘
        poincare_weights = []
        for weight in weight_list:
            # ì •ê·œí™”í•˜ì—¬ PoincarÃ© ë””ìŠ¤í¬ ë‚´ë¶€ë¡œ
            weight_norm = torch.norm(weight, dim=1, keepdim=True)
            max_norm = torch.max(weight_norm)
            
            if max_norm > 0:
                # 0.9ë¡œ ìŠ¤ì¼€ì¼ë§ (ë””ìŠ¤í¬ ê²½ê³„ í”¼í•¨)
                scale_factor = 0.9 / max_norm
                poincare_weight = weight * scale_factor
            else:
                poincare_weight = weight
            
            poincare_weights.append(poincare_weight)
        
        # 2. í•˜ì´í¼ë³¼ë¦­ K-means í´ëŸ¬ìŠ¤í„°ë§
        return self._hyperbolic_kmeans_compression(poincare_weights, layer_type)
    
    def _hyperbolic_kmeans_compression(self, poincare_weights, layer_type):
        """í•˜ì´í¼ë³¼ë¦­ K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì••ì¶•"""
        
        # ëª¨ë“  ê°€ì¤‘ì¹˜ ë²¡í„° ìˆ˜ì§‘
        all_vectors = torch.cat(poincare_weights, dim=0)  # [total_neurons, features]
        total_neurons, features = all_vectors.shape
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        num_clusters = max(1, int(total_neurons * self.compression_ratio))
        
        print(f"   í•˜ì´í¼ë³¼ë¦­ í´ëŸ¬ìŠ¤í„°ë§: {total_neurons} â†’ {num_clusters} í´ëŸ¬ìŠ¤í„°")
        
        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ì´ˆê¸°í™”
        cluster_indices = torch.randperm(total_neurons)[:num_clusters]
        cluster_centers = all_vectors[cluster_indices].clone()
        
        # í•˜ì´í¼ë³¼ë¦­ K-means
        for iteration in range(10):
            # í•˜ì´í¼ë³¼ë¦­ ê±°ë¦¬ë¡œ í´ëŸ¬ìŠ¤í„° í• ë‹¹
            distances = torch.zeros(total_neurons, num_clusters, device=all_vectors.device)
            
            for i in range(num_clusters):
                center = cluster_centers[i:i+1].expand_as(all_vectors)
                distances[:, i] = HyperbolicGeometry.hyperbolic_distance(
                    all_vectors, center, c=self.curvature
                )
            
            assignments = torch.argmin(distances, dim=1)
            
            # í•˜ì´í¼ë³¼ë¦­ ì¤‘ì‹¬ ì—…ë°ì´íŠ¸
            for i in range(num_clusters):
                mask = (assignments == i)
                if mask.sum() > 0:
                    cluster_points = all_vectors[mask]
                    
                    # í•˜ì´í¼ë³¼ë¦­ í‰ê·  ê³„ì‚°
                    log_points = HyperbolicGeometry.poincare_log_map(cluster_points, c=self.curvature)
                    euclidean_mean = torch.mean(log_points, dim=0)
                    cluster_centers[i] = HyperbolicGeometry.poincare_exp_map(euclidean_mean, c=self.curvature)
        
        # Reality Stoneìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ì¶”ê°€ ì••ì¶•
        if REALITY_STONE_AVAILABLE:
            cluster_centers = self._apply_reality_stone_compression(cluster_centers)
        
        return {
            'representatives': nn.Parameter(cluster_centers),
            'assignments': assignments,
            'layer_sizes': [w.shape[0] for w in poincare_weights],
            'total_neurons': total_neurons,
            'features': features
        }
    
    def _apply_reality_stone_compression(self, tensor):
        """Reality Stone í•˜ì´í¼ë³¼ë¦­ ì••ì¶• ì ìš©"""
        
        try:
            # 1. poincare_ball_layer ì‹œë„
            if hasattr(reality_stone, 'poincare_ball_layer'):
                dummy_input = torch.randn(1, tensor.shape[1], device=tensor.device, dtype=torch.float32)
                compressed = reality_stone.poincare_ball_layer(
                    dummy_input, tensor.float(), self.curvature, 0.1
                )
                if compressed.shape == tensor.shape:
                    print(f"   âœ… Reality Stone poincare_ball_layer ì ìš©")
                    return compressed.to(tensor.dtype)
            
            # 2. poincare_compress ì‹œë„
            if hasattr(reality_stone, 'poincare_compress'):
                compressed = reality_stone.poincare_compress(tensor.float())
                if compressed is not None and compressed.shape == tensor.shape:
                    print(f"   âœ… Reality Stone poincare_compress ì ìš©")
                    return compressed.to(tensor.dtype)
            
            # 3. hyperbolic_compress ì‹œë„
            if hasattr(reality_stone, 'hyperbolic_compress'):
                compressed = reality_stone.hyperbolic_compress(tensor.float())
                if compressed is not None and compressed.shape == tensor.shape:
                    print(f"   âœ… Reality Stone hyperbolic_compress ì ìš©")
                    return compressed.to(tensor.dtype)
            
        except Exception as e:
            print(f"   âš ï¸ Reality Stone ì••ì¶• ì‹¤íŒ¨: {e}")
        
        return tensor
    
    def _compress_bias(self, bias_list):
        """ë°”ì´ì–´ìŠ¤ ì••ì¶•"""
        if not bias_list:
            return None
        
        # ë‹¨ìˆœ ê°€ì¤‘ í‰ê·  (ë ˆì´ì–´ ìœ„ì¹˜ ê³ ë ¤)
        weights = torch.linspace(0.5, 1.5, len(bias_list))
        weights = weights / weights.sum()
        
        weighted_bias = torch.zeros_like(bias_list[0])
        for bias, weight in zip(bias_list, weights):
            weighted_bias += bias * weight
        
        return nn.Parameter(weighted_bias)
    
    def _reconstruct_weight_matrix(self, hyperbolic_data, target_shape):
        """í•˜ì´í¼ë³¼ë¦­ ë°ì´í„°ì—ì„œ ê°€ì¤‘ì¹˜ í–‰ë ¬ ì¬êµ¬ì„±"""
        
        representatives = hyperbolic_data['representatives']
        assignments = hyperbolic_data['assignments']
        layer_sizes = hyperbolic_data['layer_sizes']
        
        # í´ëŸ¬ìŠ¤í„° í• ë‹¹ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¬êµ¬ì„±
        reconstructed_weights = []
        start_idx = 0
        
        for layer_size in layer_sizes:
            end_idx = start_idx + layer_size
            layer_assignments = assignments[start_idx:end_idx]
            
            # ê° ë‰´ëŸ°ì„ í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ëŒ€í‘œë¡œ ë§¤í•‘
            layer_weight = representatives[layer_assignments]
            reconstructed_weights.append(layer_weight)
            
            start_idx = end_idx
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ ê°€ì¤‘ì¹˜ë§Œ ë°˜í™˜ (ìœµí•©ëœ ê²°ê³¼)
        return reconstructed_weights[0]
    
    def forward(self, x):
        """í•˜ì´í¼ë³¼ë¦­ Super Layer ìˆœì „íŒŒ"""
        
        # c_fc: í•˜ì´í¼ë³¼ë¦­ ì••ì¶•ì—ì„œ ì¬êµ¬ì„±
        c_fc_weight = self._reconstruct_weight_matrix(
            self.c_fc_hyperbolic, 
            (self.c_fc_hyperbolic['layer_sizes'][0], self.c_fc_hyperbolic['features'])
        )
        
        h = F.linear(x, c_fc_weight, self.c_fc_bias)
        h = self.activation(h)
        
        # c_proj: í•˜ì´í¼ë³¼ë¦­ ì••ì¶•ì—ì„œ ì¬êµ¬ì„±
        c_proj_weight = self._reconstruct_weight_matrix(
            self.c_proj_hyperbolic,
            (self.c_proj_hyperbolic['layer_sizes'][0], self.c_proj_hyperbolic['features'])
        )
        
        output = F.linear(h, c_proj_weight, self.c_proj_bias)
        
        return output


def apply_hyperbolic_semantic_compression(model, target_compression_ratio=0.3, curvature=1.0):
    """í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´ ì••ì¶• ì ìš©"""
    
    print(f"\nğŸŒ€ Hyperbolic Semantic Compression ì ìš©")
    print(f"   ëª©í‘œ ì••ì¶•ë¥ : {target_compression_ratio:.1%}")
    print(f"   í•˜ì´í¼ë³¼ë¦­ ê³¡ë¥ : {curvature}")
    
    original_params = sum(p.numel() for p in model.parameters())
    total_layers = len(model.transformer.h)
    
    # ë” aggressiveí•œ ì••ì¶•ì„ ìœ„í•´ ë” ë§ì€ ë ˆì´ì–´ ìœµí•©
    num_layers_to_fuse = min(10, total_layers - 1)  # ìµœëŒ€ 10ê°œ ë ˆì´ì–´ ìœµí•©
    target_layers = list(range(total_layers - num_layers_to_fuse, total_layers))
    
    print(f"   ì „ì²´ ë ˆì´ì–´: {total_layers}ê°œ")
    print(f"   ìœµí•© ëŒ€ìƒ: {target_layers} ({num_layers_to_fuse}ê°œ)")
    
    # MLP ë ˆì´ì–´ ìˆ˜ì§‘
    mlp_layers = [model.transformer.h[i].mlp for i in target_layers]
    
    # Hyperbolic Super Layer ìƒì„±
    super_layer = HyperbolicSemanticSuperLayer(
        mlp_layers, 
        target_layers,
        curvature=curvature,
        compression_ratio=target_compression_ratio
    )
    
    # ë ˆì´ì–´ êµì²´
    model.transformer.h[target_layers[0]].mlp = super_layer
    
    # ë‚˜ë¨¸ì§€ ë ˆì´ì–´ë“¤ ì œê±°
    for i in reversed(target_layers[1:]):
        del model.transformer.h[i]
    
    # ìµœì¢… ì••ì¶•ë¥  ê³„ì‚°
    final_params = sum(p.numel() for p in model.parameters())
    actual_compression_ratio = final_params / original_params
    
    print(f"\nğŸ“Š í•˜ì´í¼ë³¼ë¦­ ì••ì¶• ê²°ê³¼:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {total_layers} â†’ {len(model.transformer.h)}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {final_params:,}")
    print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {actual_compression_ratio:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {(1-actual_compression_ratio)*100:.1f}%")
    print(f"   ë ˆì´ì–´ ì ˆì•½: {num_layers_to_fuse-1}ê°œ")
    
    return model, actual_compression_ratio


def hyperbolic_accuracy_test(model, tokenizer, test_name="í•˜ì´í¼ë³¼ë¦­ ì••ì¶• ëª¨ë¸"):
    """í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
    
    print(f"ğŸ“Š {test_name} ì˜ë¯¸ ì •í™•ë„ í…ŒìŠ¤íŠ¸")
    
    # í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì˜ ì˜ë¯¸ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ í…ŒìŠ¤íŠ¸
    tests = [
        {
            "prompt": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
            "core_concepts": ["ì„œìš¸", "ë„ì‹œ", "ìˆ˜ë„"],
            "semantic_field": ["ëŒ€í•œë¯¼êµ­", "í•œêµ­", "ì¤‘ì‹¬"],
            "weight": 3
        },
        {
            "prompt": "ì•ˆë…•í•˜ì„¸ìš”",
            "core_concepts": ["ì•ˆë…•", "ì¸ì‚¬"],
            "semantic_field": ["ë°˜ê°‘", "ì¢‹", "í•˜ì„¸ìš”"],
            "weight": 2
        },
        {
            "prompt": "ì¸ê³µì§€ëŠ¥ì€",
            "core_concepts": ["AI", "ê¸°ìˆ ", "ì§€ëŠ¥"],
            "semantic_field": ["ì»´í“¨í„°", "ë¯¸ë˜", "ë°œì „", "ì¸ê³µ"],
            "weight": 3
        },
        {
            "prompt": "ê¹€ì¹˜ëŠ”",
            "core_concepts": ["ìŒì‹", "í•œêµ­"],
            "semantic_field": ["ë§›", "ì „í†µ", "ë¨¹", "ê¹€ì¹˜"],
            "weight": 2
        },
        {
            "prompt": "êµìœ¡ì˜ ì¤‘ìš”ì„±ì€",
            "core_concepts": ["êµìœ¡", "ì¤‘ìš”"],
            "semantic_field": ["í•™ìŠµ", "ì„±ì¥", "ì§€ì‹", "ë°œì „"],
            "weight": 3
        }
    ]
    
    total_score = 0
    max_score = 0
    
    for test_case in tests:
        prompt = test_case["prompt"]
        weight = test_case["weight"]
        
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=len(inputs.input_ids[0]) + 30,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=2
                )
            
            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ì˜ë¯¸ êµ¬ì¡° ì ìˆ˜ ê³„ì‚°
            score = 0
            
            # 1. í•µì‹¬ ê°œë… ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
            core_found = sum(1 for concept in test_case["core_concepts"] if concept in generated)
            score += core_found * weight * 2
            max_possible_core = len(test_case["core_concepts"]) * weight * 2
            
            # 2. ì˜ë¯¸ ì˜ì—­ ë§¤ì¹­ (ì¤‘ê°„ ê°€ì¤‘ì¹˜)
            semantic_found = sum(1 for concept in test_case["semantic_field"] if concept in generated)
            score += min(semantic_found, 3) * weight  # ìµœëŒ€ 3ê°œê¹Œì§€
            max_possible_semantic = 3 * weight
            
            # 3. ìœ ì°½ì„± ë³´ë„ˆìŠ¤
            if len(generated.split()) >= 4 and any(ending in generated for ending in ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤']):
                score += weight
            max_possible_fluency = weight
            
            total_score += score
            max_score += max_possible_core + max_possible_semantic + max_possible_fluency
            
            # ê²°ê³¼ í‘œì‹œ
            current_max = max_possible_core + max_possible_semantic + max_possible_fluency
            percentage = (score / current_max * 100) if current_max > 0 else 0
            status = 'âœ…' if percentage >= 70 else 'âš ï¸' if percentage >= 40 else 'âŒ'
            
            print(f"   '{prompt}' ({score}/{current_max}, {percentage:.0f}%) {status}")
            print(f"      â†’ '{generated[:90]}...'")
            
        except Exception as e:
            print(f"   '{prompt}' â†’ ì˜¤ë¥˜: {e} (âŒ)")
    
    final_accuracy = (total_score / max_score * 100) if max_score > 0 else 0
    print(f"   í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ì •í™•ë„: {final_accuracy:.1f}% ({total_score}/{max_score})")
    
    return final_accuracy / 100


def hyperbolic_semantic_compression_test():
    """í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´ ì••ì¶• í…ŒìŠ¤íŠ¸"""
    
    print("ğŸŒ€ Reality Stone Hyperbolic Semantic Compression")
    print("=" * 80)
    print("   í˜ì‹ : í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œ ì˜ë¯¸ êµ¬ì¡° ë³´ì¡´ ì••ì¶•")
    print("   í•µì‹¬: FFTë¥¼ ì„ í˜•ë ˆì´ì–´ê°€ ì•„ë‹Œ í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œ ì ìš©")
    
    if not REALITY_STONE_AVAILABLE:
        print("âš ï¸ Reality Stone ì—†ì´ ê¸°ë³¸ í•˜ì´í¼ë³¼ë¦­ ì••ì¶• ì§„í–‰")
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    original_params = sum(p.numel() for p in model.parameters())
    original_layers = len(model.transformer.h)
    
    print(f"\nğŸ“Š ì›ë³¸ ëª¨ë¸:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {original_layers}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,}")
    print(f"   í¬ê¸°: {original_params * 4 / (1024**2):.1f}MB")
    
    # ì›ë³¸ ëª¨ë¸ ì •í™•ë„ ì¸¡ì •
    print(f"\nğŸ“‹ ì›ë³¸ ëª¨ë¸ í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ì •í™•ë„ ì¸¡ì •")
    print("-" * 60)
    original_accuracy = hyperbolic_accuracy_test(model, tokenizer, "ì›ë³¸ ëª¨ë¸")
    
    # í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´ ì••ì¶• ì ìš©
    print(f"\nğŸŒ€ Hyperbolic Semantic Compression ì‹œì‘")
    print("=" * 80)
    
    compressed_model = copy.deepcopy(model)
    compressed_model, compression_ratio = apply_hyperbolic_semantic_compression(
        compressed_model, target_compression_ratio=0.2, curvature=1.0
    )
    
    # ì••ì¶• í›„ í†µê³„
    compressed_params = sum(p.numel() for p in compressed_model.parameters())
    compressed_layers = len(compressed_model.transformer.h)
    memory_saved = (original_params - compressed_params) * 4 / (1024**2)
    
    print(f"\nğŸ“Š ì••ì¶• í›„ ëª¨ë¸:")
    print(f"   ë ˆì´ì–´ ìˆ˜: {original_layers} â†’ {compressed_layers}")
    print(f"   íŒŒë¼ë¯¸í„°: {original_params:,} â†’ {compressed_params:,}")
    print(f"   ì••ì¶•ë¥ : {compression_ratio:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}MB ({(1-compression_ratio)*100:.1f}%)")
    
    # ì••ì¶• ëª¨ë¸ ì •í™•ë„ ì¸¡ì •
    print(f"\nğŸ“‹ ì••ì¶• ëª¨ë¸ í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ì •í™•ë„ ì¸¡ì •")
    print("-" * 60)
    compressed_accuracy = hyperbolic_accuracy_test(compressed_model, tokenizer, "í•˜ì´í¼ë³¼ë¦­ ì••ì¶• ëª¨ë¸")
    
    # ì •í™•ë„ ë³´ì¡´ìœ¨
    accuracy_retention = compressed_accuracy / original_accuracy if original_accuracy > 0 else 0
    
    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ† Hyperbolic Semantic Compression ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    
    print(f"ğŸ¯ ì••ì¶• ì„±ê³¼:")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {(1-compression_ratio)*100:.1f}%")
    print(f"   ë ˆì´ì–´ ê°ì†Œ: {original_layers} â†’ {compressed_layers} ({original_layers - compressed_layers}ê°œ)")
    print(f"   íŒŒë¼ë¯¸í„° ê°ì†Œ: {original_params:,} â†’ {compressed_params:,}")
    
    print(f"\nğŸ¯ í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´ ì„±ê³¼:")
    print(f"   ì›ë³¸ í•˜ì´í¼ë³¼ë¦­ ì •í™•ë„: {original_accuracy:.1%}")
    print(f"   ì••ì¶• í›„ í•˜ì´í¼ë³¼ë¦­ ì •í™•ë„: {compressed_accuracy:.1%}")
    print(f"   ì˜ë¯¸ ë³´ì¡´ìœ¨: {accuracy_retention:.1%}")
    
    print(f"\nğŸ¯ í•˜ì´í¼ë³¼ë¦­ ê¸°ìˆ  í˜ì‹ :")
    print(f"   âœ… PoincarÃ© Disk Mapping")
    print(f"   âœ… Hyperbolic K-means Clustering")
    print(f"   âœ… MÃ¶bius Transformation")
    print(f"   âœ… Reality Stone Integration")
    print(f"   âœ… ì˜ë¯¸ êµ¬ì¡´ ë³´ì¡´ ì••ì¶•")
    
    # ì„±ê³µ ê¸°ì¤€ ì²´í¬
    high_compression = (1 - compression_ratio) >= 0.60  # 60%+ ì••ì¶•
    good_meaning = accuracy_retention >= 0.75  # 75%+ ì˜ë¯¸ ë³´ì¡´
    
    if high_compression and good_meaning:
        print(f"\nğŸ‰ HYPERBOLIC SUCCESS! ğŸ‰")
        print(f"   âœ… 60%+ ì••ì¶• ë‹¬ì„±: {(1-compression_ratio)*100:.1f}%")
        print(f"   âœ… 75%+ ì˜ë¯¸ ë³´ì¡´: {accuracy_retention:.1%}")
        print(f"\nğŸŒ€ í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´ ì••ì¶• ê¸°ìˆ  ì™„ì „ ì„±ê³µ!")
    elif high_compression:
        print(f"\nğŸ¥‡ HIGH HYPERBOLIC COMPRESSION!")
        print(f"   âœ… 60%+ ì••ì¶• ë‹¬ì„±: {(1-compression_ratio)*100:.1f}%")
        print(f"   ğŸ“ˆ ì˜ë¯¸ ë³´ì¡´: {accuracy_retention:.1%}")
        print(f"\nğŸ’ª í•˜ì´í¼ë³¼ë¦­ ì••ì¶• ëª©í‘œ ë‹¬ì„±!")
    else:
        print(f"\nğŸ’ª HYPERBOLIC PROGRESS!")
        print(f"   ğŸ“Š ì••ì¶•ë¥ : {(1-compression_ratio)*100:.1f}%")
        print(f"   ğŸŒ€ í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´: {accuracy_retention:.1%}")
        print(f"\nğŸ”¬ í•˜ì´í¼ë³¼ë¦­ ì˜ë¯¸ ë³´ì¡´ ê¸°ìˆ  ê²€ì¦ ì™„ë£Œ!")
    
    print(f"\nâœ… Hyperbolic Semantic Compression í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    hyperbolic_semantic_compression_test() 