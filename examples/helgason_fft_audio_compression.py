"""
FFT ì‹ í˜¸ì²˜ë¦¬ ê¸°ë°˜ ìŒí–¥ ê²€ì¶œ ì‘ìš© ì‹ ê²½ë§ ì••ì¶•
ê°€ì¤‘ì¹˜ë¥¼ ìŒí–¥ ì‹ í˜¸ë¡œ ì·¨ê¸‰í•˜ì—¬ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ì••ì¶•

í•µì‹¬ ì•„ì´ë””ì–´:
1. ê°€ì¤‘ì¹˜ â†’ ìŒí–¥ ì‹ í˜¸ ë³€í™˜
2. FFTë¡œ ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
3. ì¤‘ìš”í•œ ì£¼íŒŒìˆ˜ ì„±ë¶„ ê²€ì¶œ (ìŒí–¥ ê²€ì¶œ ë°©ì‹)
4. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ì••ì¶•
5. Reality Stone + Helgason ê²°í•©
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import copy
import sys
import os
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

# Reality Stone ë°±ì—”ë“œ ë¡œë“œ
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import reality_stone
    print("âœ… Reality Stone ë°±ì—”ë“œ ë¡œë“œ ì„±ê³µ!")
    
    # í•¨ìˆ˜ í™•ì¸
    all_funcs = [name for name in dir(reality_stone) if not name.startswith('_')]
    print(f"   ì „ì²´ í•¨ìˆ˜: {len(all_funcs)}ê°œ")
    
    layer_funcs = [f for f in all_funcs if 'layer' in f.lower()]
    print(f"   ë ˆì´ì–´ í•¨ìˆ˜: {layer_funcs}")
    
    REALITY_STONE_AVAILABLE = True
    
except ImportError as e:
    print(f"âŒ Reality Stone ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    REALITY_STONE_AVAILABLE = False


class FFTAudioCompressionEngine:
    """FFT ê¸°ë°˜ ìŒí–¥ ê²€ì¶œ ë°©ì‹ ì••ì¶• ì—”ì§„"""
    
    def __init__(self, compression_ratio=0.3, quality_threshold=0.95):
        self.compression_ratio = compression_ratio
        self.quality_threshold = quality_threshold
        
        # ìŒí–¥ ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
        self.sample_rate = 44100  # í‘œì¤€ ìƒ˜í”Œë§ ë ˆì´íŠ¸
        self.window_size = 2048   # FFT ìœˆë„ìš° í¬ê¸°
        self.hop_length = 512     # í™‰ ê¸¸ì´
        self.energy_threshold = 0.01  # ì—ë„ˆì§€ ì„ê³„ê°’
        
    def weight_to_audio_signal(self, weight_matrix):
        """ê°€ì¤‘ì¹˜ë¥¼ ìŒí–¥ ì‹ í˜¸ë¡œ ë³€í™˜"""
        
        device = weight_matrix.device
        dtype = weight_matrix.dtype
        
        # 2D ê°€ì¤‘ì¹˜ë¥¼ 1D ìŒí–¥ ì‹ í˜¸ë¡œ ë³€í™˜
        if len(weight_matrix.shape) == 2:
            # í–‰ ìš°ì„ ìœ¼ë¡œ flatten
            audio_signal = weight_matrix.flatten().float()
        else:
            audio_signal = weight_matrix.view(-1).float()
        
        # ì‹ í˜¸ ì •ê·œí™” (-1, 1 ë²”ìœ„ë¡œ)
        signal_max = torch.max(torch.abs(audio_signal))
        if signal_max > 0:
            audio_signal = audio_signal / signal_max
        else:
            signal_max = 1.0
        
        return {
            'signal': audio_signal,
            'original_shape': weight_matrix.shape,
            'normalization_factor': signal_max,
            'device': device,
            'dtype': dtype
        }
    
    def audio_signal_to_weight(self, audio_data):
        """ìŒí–¥ ì‹ í˜¸ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ë³µì›"""
        
        signal = audio_data['signal']
        original_shape = audio_data['original_shape']
        norm_factor = audio_data['normalization_factor']
        device = audio_data['device']
        dtype = audio_data['dtype']
        
        # ì •ê·œí™” ë³µì›
        restored_signal = signal * norm_factor
        
        # ì›ë³¸ í˜•íƒœë¡œ ë³µì›
        if len(original_shape) == 2:
            total_elements = original_shape[0] * original_shape[1]
            if len(restored_signal) < total_elements:
                # íŒ¨ë”©
                padding = torch.zeros(total_elements - len(restored_signal), 
                                    device=device, dtype=torch.float32)
                restored_signal = torch.cat([restored_signal, padding])
            elif len(restored_signal) > total_elements:
                # ìë¥´ê¸°
                restored_signal = restored_signal[:total_elements]
            
            weight_matrix = restored_signal.view(original_shape)
        else:
            weight_matrix = restored_signal.view(original_shape)
        
        return weight_matrix.to(dtype).to(device)
    
    def spectral_analysis(self, audio_signal):
        """ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ (ìŒí–¥ ê²€ì¶œ ë°©ì‹)"""
        
        signal = audio_signal['signal']
        
        # ìœˆë„ìš° í¬ê¸° ì¡°ì • (ì‹ í˜¸ ê¸¸ì´ì— ë§ê²Œ)
        actual_window_size = min(self.window_size, len(signal))
        if actual_window_size < 32:
            actual_window_size = 32  # ìµœì†Œ ìœˆë„ìš° í¬ê¸°
        
        # ì‹ í˜¸ ê¸¸ì´ë¥¼ ìœˆë„ìš° í¬ê¸°ì˜ ë°°ìˆ˜ë¡œ ë§ì¶”ê¸°
        signal_length = len(signal)
        padded_length = ((signal_length + actual_window_size - 1) // actual_window_size) * actual_window_size
        
        if padded_length > signal_length:
            padding = torch.zeros(padded_length - signal_length, device=signal.device)
            padded_signal = torch.cat([signal, padding])
        else:
            padded_signal = signal
        
        # ìœˆë„ìš°ë³„ FFT
        windows = padded_signal.view(-1, actual_window_size)
        fft_results = []
        
        for window in windows:
            # FFT ì ìš©
            fft_window = torch.fft.fft(window)
            fft_results.append(fft_window)
        
        # ìŠ¤í™íŠ¸ëŸ¼ ê²°í•©
        spectrogram = torch.stack(fft_results, dim=0)  # [num_windows, window_size]
        
        return {
            'spectrogram': spectrogram,
            'window_size': actual_window_size,
            'num_windows': len(windows),
            'original_length': signal_length
        }
    
    def audio_feature_detection(self, spectrum_data):
        """ìŒí–¥ íŠ¹ì„± ê²€ì¶œ (ì¤‘ìš”í•œ ì£¼íŒŒìˆ˜ ì„±ë¶„ ì°¾ê¸°)"""
        
        spectrogram = spectrum_data['spectrogram']
        
        # 1. ì—ë„ˆì§€ ê³„ì‚°
        magnitude = torch.abs(spectrogram)
        energy = magnitude ** 2
        
        # 2. ì£¼íŒŒìˆ˜ë³„ í‰ê·  ì—ë„ˆì§€
        freq_energy = torch.mean(energy, dim=0)  # [window_size]
        
        # 3. ì‹œê°„ë³„ í‰ê·  ì—ë„ˆì§€
        time_energy = torch.mean(energy, dim=1)   # [num_windows]
        
        # 4. ì „ì²´ ì—ë„ˆì§€
        total_energy = torch.sum(energy)
        
        # 5. ì¤‘ìš”í•œ ì£¼íŒŒìˆ˜ ê²€ì¶œ (ì—ë„ˆì§€ ê¸°ë°˜)
        energy_threshold = self.energy_threshold * torch.max(freq_energy)
        important_freqs = freq_energy > energy_threshold
        
        # 6. ì¤‘ìš”í•œ ì‹œê°„ êµ¬ê°„ ê²€ì¶œ
        time_threshold = self.energy_threshold * torch.max(time_energy)
        important_times = time_energy > time_threshold
        
        return {
            'magnitude': magnitude,
            'energy': energy,
            'freq_energy': freq_energy,
            'time_energy': time_energy,
            'total_energy': total_energy,
            'important_freqs': important_freqs,
            'important_times': important_times,
            'freq_mask': important_freqs,
            'time_mask': important_times
        }
    
    def frequency_domain_compression(self, spectrum_data, features):
        """ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ì••ì¶•"""
        
        spectrogram = spectrum_data['spectrogram']
        freq_mask = features['freq_mask']
        time_mask = features['time_mask']
        
        # 1. ì£¼íŒŒìˆ˜ ì¶• ì••ì¶•
        compressed_freqs = torch.sum(freq_mask).item()
        target_freqs = max(1, int(compressed_freqs * (1 - self.compression_ratio)))
        
        if target_freqs < compressed_freqs:
            # ê°€ì¥ ì¤‘ìš”í•œ ì£¼íŒŒìˆ˜ë§Œ ì„ íƒ
            freq_energy = features['freq_energy']
            _, top_freq_indices = torch.topk(freq_energy, target_freqs)
            
            # ìƒˆë¡œìš´ ë§ˆìŠ¤í¬ ìƒì„±
            new_freq_mask = torch.zeros_like(freq_mask)
            new_freq_mask[top_freq_indices] = True
        else:
            new_freq_mask = freq_mask
        
        # 2. ì‹œê°„ ì¶• ì••ì¶•
        compressed_times = torch.sum(time_mask).item()
        target_times = max(1, int(compressed_times * (1 - self.compression_ratio)))
        
        if target_times < compressed_times:
            # ê°€ì¥ ì¤‘ìš”í•œ ì‹œê°„ êµ¬ê°„ë§Œ ì„ íƒ
            time_energy = features['time_energy']
            _, top_time_indices = torch.topk(time_energy, target_times)
            
            # ìƒˆë¡œìš´ ë§ˆìŠ¤í¬ ìƒì„±
            new_time_mask = torch.zeros_like(time_mask)
            new_time_mask[top_time_indices] = True
        else:
            new_time_mask = time_mask
        
        # 3. ì••ì¶•ëœ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
        # ì¤‘ìš”í•œ ì‹œê°„-ì£¼íŒŒìˆ˜ ì„±ë¶„ë§Œ ìœ ì§€
        compressed_spectrogram = torch.zeros_like(spectrogram)
        
        for t_idx, t_selected in enumerate(new_time_mask):
            if t_selected:
                for f_idx, f_selected in enumerate(new_freq_mask):
                    if f_selected:
                        compressed_spectrogram[t_idx, f_idx] = spectrogram[t_idx, f_idx]
        
        # ì••ì¶•ë¥  ê³„ì‚°
        original_nonzero = torch.sum(spectrogram != 0).item()
        compressed_nonzero = torch.sum(compressed_spectrogram != 0).item()
        actual_compression_ratio = compressed_nonzero / max(1, original_nonzero)
        
        return {
            'compressed_spectrogram': compressed_spectrogram,
            'freq_mask': new_freq_mask,
            'time_mask': new_time_mask,
            'compression_ratio': actual_compression_ratio
        }
    
    def spectral_reconstruction(self, compressed_data, spectrum_data):
        """ìŠ¤í™íŠ¸ëŸ¼ ì¬êµ¬ì„±"""
        
        compressed_spectrogram = compressed_data['compressed_spectrogram']
        window_size = spectrum_data['window_size']
        original_length = spectrum_data['original_length']
        
        # IFFTë¡œ ì‹œê°„ ë„ë©”ì¸ ë³µì›
        reconstructed_windows = []
        
        for window_spectrum in compressed_spectrogram:
            # IFFT ì ìš©
            time_domain = torch.fft.ifft(window_spectrum)
            # ì‹¤ìˆ˜ë¶€ë§Œ ì‚¬ìš© (ì›ë³¸ì´ ì‹¤ìˆ˜ ì‹ í˜¸ì´ë¯€ë¡œ)
            real_signal = torch.real(time_domain)
            reconstructed_windows.append(real_signal)
        
        # ìœˆë„ìš°ë“¤ì„ ì—°ê²°
        if reconstructed_windows:
            reconstructed_signal = torch.cat(reconstructed_windows, dim=0)
        else:
            reconstructed_signal = torch.zeros(original_length)
        
        # ì›ë³¸ ê¸¸ì´ë¡œ ë§ì¶”ê¸°
        if len(reconstructed_signal) > original_length:
            reconstructed_signal = reconstructed_signal[:original_length]
        elif len(reconstructed_signal) < original_length:
            padding = torch.zeros(original_length - len(reconstructed_signal))
            reconstructed_signal = torch.cat([reconstructed_signal, padding])
        
        return reconstructed_signal
    
    def compress_weight_matrix(self, weight_matrix):
        """í†µí•© ê°€ì¤‘ì¹˜ ì••ì¶• íŒŒì´í”„ë¼ì¸"""
        
        print(f"      FFT ìŒí–¥ ì²˜ë¦¬ ê¸°ë°˜ ì••ì¶•: {weight_matrix.shape}")
        
        try:
            # 1. ê°€ì¤‘ì¹˜ â†’ ìŒí–¥ ì‹ í˜¸ ë³€í™˜
            audio_data = self.weight_to_audio_signal(weight_matrix)
            
            # 2. ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
            spectrum_data = self.spectral_analysis(audio_data)
            
            # 3. ìŒí–¥ íŠ¹ì„± ê²€ì¶œ
            features = self.audio_feature_detection(spectrum_data)
            
            # 4. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ì••ì¶•
            compressed_data = self.frequency_domain_compression(spectrum_data, features)
            
            # 5. ìŠ¤í™íŠ¸ëŸ¼ ì¬êµ¬ì„±
            reconstructed_signal = self.spectral_reconstruction(compressed_data, spectrum_data)
            
            # 6. ìŒí–¥ ì‹ í˜¸ â†’ ê°€ì¤‘ì¹˜ ë³µì›
            audio_data['signal'] = reconstructed_signal
            compressed_weight = self.audio_signal_to_weight(audio_data)
            
            # 7. Reality Stone í›„ì²˜ë¦¬ (ì„ íƒì )
            if REALITY_STONE_AVAILABLE:
                try:
                    # poincare_ball_layer ì ìš©
                    dummy_input = torch.randn(1, weight_matrix.shape[1], 
                                            device=weight_matrix.device, dtype=torch.float32)
                    enhanced_weight = reality_stone.poincare_ball_layer(
                        dummy_input, compressed_weight.float(), 1.0, 0.1
                    )
                    if enhanced_weight.shape == weight_matrix.shape:
                        compressed_weight = enhanced_weight.to(weight_matrix.dtype)
                        method_name = "fft_audio_reality_stone"
                    else:
                        method_name = "fft_audio_processing"
                except:
                    method_name = "fft_audio_processing"
            else:
                method_name = "fft_audio_processing"
            
            print(f"      âœ… FFT ìŒí–¥ ì••ì¶• ì„±ê³µ: {compressed_data['compression_ratio']:.3f}")
            
            return {
                'method': method_name,
                'compressed_weight': compressed_weight,
                'compression_ratio': compressed_data['compression_ratio'],
                'success': True,
                'details': {
                    'spectral_compression': compressed_data['compression_ratio'],
                    'freq_components': torch.sum(compressed_data['freq_mask']).item(),
                    'time_segments': torch.sum(compressed_data['time_mask']).item()
                }
            }
            
        except Exception as e:
            print(f"      FFT ìŒí–¥ ì••ì¶• ì‹¤íŒ¨: {e}")
            return {
                'method': 'original',
                'compressed_weight': weight_matrix,
                'compression_ratio': 1.0,
                'success': False
            }


class AudioCompressedLayer(nn.Module):
    """FFT ìŒí–¥ ì²˜ë¦¬ ê¸°ë°˜ ì••ì¶• ë ˆì´ì–´"""
    
    def __init__(self, original_layer, compression_ratio=0.3, layer_name="unknown"):
        super().__init__()
        
        self.layer_name = layer_name
        self.compression_ratio = compression_ratio
        
        # ì›ë³¸ ì •ë³´
        original_weight = original_layer.weight.data.clone()
        original_bias = original_layer.bias.data.clone() if original_layer.bias is not None else None
        
        self.out_features = original_weight.shape[0]
        self.in_features = original_weight.shape[1]
        
        print(f"   ğŸµ {layer_name} FFT ìŒí–¥ ì••ì¶• ì¤‘... {original_weight.shape}")
        
        # FFT ìŒí–¥ ì••ì¶•ê¸°
        compressor = FFTAudioCompressionEngine(compression_ratio, quality_threshold=0.95)
        
        # ê°€ì¤‘ì¹˜ ì••ì¶•
        compression_result = compressor.compress_weight_matrix(original_weight)
        
        # ì••ì¶•ëœ ê°€ì¤‘ì¹˜ ì €ì¥
        compressed_weight = compression_result['compressed_weight']
        
        # ì°¨ì› ì•ˆì „ì„± í™•ì¸
        if compressed_weight.shape != original_weight.shape:
            print(f"      âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜ ê°ì§€, ì›ë³¸ ì‚¬ìš©: {compressed_weight.shape} vs {original_weight.shape}")
            compressed_weight = original_weight
            compression_result['method'] = 'forced_original'
            compression_result['success'] = False
        
        self.register_buffer('compressed_weight', compressed_weight)
        self.register_buffer('compression_success', torch.tensor(compression_result['success']))
        
        # ë°”ì´ì–´ìŠ¤ ì €ì¥
        if original_bias is not None:
            self.bias = nn.Parameter(original_bias)
        else:
            self.bias = None
        
        # í†µê³„
        self.method_used = compression_result['method']
        self.actual_compression_ratio = compression_result['compression_ratio']
        self.compression_details = compression_result.get('details', {})
        
        print(f"      âœ… ì••ì¶• ì™„ë£Œ: {self.method_used}")
        print(f"      ğŸ“Š ì••ì¶•ë¥ : {self.actual_compression_ratio:.3f}")
        if self.compression_details:
            print(f"      ğŸ¼ ì£¼íŒŒìˆ˜ ì„±ë¶„: {self.compression_details.get('freq_components', 0)}ê°œ")
            print(f"      â±ï¸ ì‹œê°„ ì„¸ê·¸ë¨¼íŠ¸: {self.compression_details.get('time_segments', 0)}ê°œ")
    
    def forward(self, x):
        """ì••ì¶•ëœ ê°€ì¤‘ì¹˜ë¡œ ìˆœì „íŒŒ"""
        
        try:
            # ì°¨ì› í™•ì¸
            if (self.compressed_weight.shape[0] != self.out_features or 
                self.compressed_weight.shape[1] != self.in_features):
                print(f"   âš ï¸ {self.layer_name} ì°¨ì› ì˜¤ë¥˜!")
                raise ValueError("ì°¨ì› ë¶ˆì¼ì¹˜")
            
            # FFT ì••ì¶•ëœ ê°€ì¤‘ì¹˜ë¡œ ê³„ì‚°
            return F.linear(x, self.compressed_weight, self.bias)
            
        except Exception as e:
            print(f"   âš ï¸ {self.layer_name} ìˆœì „íŒŒ ì‹¤íŒ¨: {e}")
            print(f"   ğŸ”§ ì•ˆì „ ëª¨ë“œ í™œì„±í™”")
            
            # ì•ˆì „í•œ fallback
            safe_weight = torch.eye(min(self.out_features, self.in_features), 
                                  device=x.device, dtype=x.dtype)
            if safe_weight.shape[0] < self.out_features:
                padding_rows = torch.zeros(self.out_features - safe_weight.shape[0], 
                                         safe_weight.shape[1], device=x.device, dtype=x.dtype)
                safe_weight = torch.cat([safe_weight, padding_rows], dim=0)
            if safe_weight.shape[1] < self.in_features:
                padding_cols = torch.zeros(safe_weight.shape[0], 
                                         self.in_features - safe_weight.shape[1], 
                                         device=x.device, dtype=x.dtype)
                safe_weight = torch.cat([safe_weight, padding_cols], dim=1)
            
            return F.linear(x, safe_weight[:self.out_features, :self.in_features], self.bias)


def load_korean_model():
    """í•œê¸€ ëª¨ë¸ ë¡œë“œ"""
    print("\nğŸ”„ í•œê¸€ ëª¨ë¸ ë¡œë”©...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            torch_dtype=torch.float32
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
        return model, tokenizer, model_name
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None


def apply_fft_audio_compression(model, compression_ratio=0.2):
    """FFT ìŒí–¥ ì²˜ë¦¬ ê¸°ë°˜ ì••ì¶• ì ìš©"""
    
    print(f"\nğŸµ FFT ìŒí–¥ ì²˜ë¦¬ ê¸°ë°˜ ì••ì¶• ì ìš© (ì••ì¶•ë¥ : {compression_ratio:.1%})")
    
    compressed_count = 0
    successful_compressions = 0
    total_original = 0
    total_compressed = 0
    methods_used = {}
    spectral_details = []
    
    # ì„ íƒì  ë ˆì´ì–´ ì••ì¶•
    if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):
        num_layers = len(model.transformer.h)
        layers_to_process = min(2, num_layers)  # ì²˜ìŒ 2ê°œ ë ˆì´ì–´
        print(f"   ì²˜ë¦¬ ëŒ€ìƒ: {layers_to_process}ê°œ ë ˆì´ì–´ (ìŒí–¥ ì²˜ë¦¬ ëª¨ë“œ)")
        
        for layer_idx in range(layers_to_process):
            layer = model.transformer.h[layer_idx]
            
            print(f"\nğŸ¼ Layer {layer_idx+1}/{layers_to_process} ìŒí–¥ ì²˜ë¦¬ ì¤‘...")
            
            try:
                # MLP c_fc ì••ì¶•
                if hasattr(layer, 'mlp') and hasattr(layer.mlp, 'c_fc'):
                    original_params = layer.mlp.c_fc.weight.numel()
                    original_shape = layer.mlp.c_fc.weight.shape
                    
                    compressed_fc = AudioCompressedLayer(
                        layer.mlp.c_fc, 
                        compression_ratio, 
                        f"layer{layer_idx}_mlp_c_fc"
                    )
                    
                    # ì°¨ì› í™•ì¸ í›„ êµì²´
                    if compressed_fc.compressed_weight.shape == original_shape:
                        layer.mlp.c_fc = compressed_fc
                        print(f"   âœ… êµì²´ ì„±ê³µ: {original_shape}")
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        total_original += original_params
                        total_compressed += sum(p.numel() for p in compressed_fc.parameters())
                        
                        method = compressed_fc.method_used
                        methods_used[method] = methods_used.get(method, 0) + 1
                        
                        if compressed_fc.compression_success:
                            successful_compressions += 1
                            
                        # ìŠ¤í™íŠ¸ëŸ¼ ì„¸ë¶€ì‚¬í•­ ì €ì¥
                        spectral_details.append({
                            'layer': f"layer{layer_idx}_mlp_c_fc",
                            'compression_ratio': compressed_fc.actual_compression_ratio,
                            'details': compressed_fc.compression_details
                        })
                        
                        compressed_count += 1
                    else:
                        print(f"   âŒ ì°¨ì› ë¶ˆì¼ì¹˜ë¡œ êµì²´ ì·¨ì†Œ")
                
                print(f"   âœ… Layer {layer_idx+1} ì™„ë£Œ")
                
            except Exception as e:
                print(f"   âŒ Layer {layer_idx+1} ì‹¤íŒ¨: {e}")
    
    # ìµœì¢… í†µê³„
    actual_ratio = total_compressed / total_original if total_original > 0 else 1.0
    memory_saved = (total_original - total_compressed) * 4 / (1024**2)
    success_rate = successful_compressions / compressed_count if compressed_count > 0 else 0.0
    
    print(f"\nğŸ“Š FFT ìŒí–¥ ì²˜ë¦¬ ì••ì¶• ê²°ê³¼:")
    print(f"   ì••ì¶•ëœ ë ˆì´ì–´: {compressed_count}ê°œ")
    print(f"   ì„±ê³µí•œ ì••ì¶•: {successful_compressions}ê°œ ({success_rate:.1%})")
    print(f"   íŒŒë¼ë¯¸í„°: {total_original:,} â†’ {total_compressed:,}")
    print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {actual_ratio:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}MB")
    print(f"   ì‚¬ìš©ëœ ì••ì¶• ë°©ë²•: {methods_used}")
    
    # ìŠ¤í™íŠ¸ëŸ¼ ì„¸ë¶€ì‚¬í•­
    if spectral_details:
        print(f"\nğŸ¼ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ ì„¸ë¶€ì‚¬í•­:")
        for detail in spectral_details:
            if detail['details']:
                print(f"   â€¢ {detail['layer']}: "
                      f"ì£¼íŒŒìˆ˜ {detail['details'].get('freq_components', 0)}ê°œ, "
                      f"ì‹œê°„ {detail['details'].get('time_segments', 0)}ê°œ")
    
    return model, actual_ratio, success_rate


def test_audio_compressed_model(model, tokenizer, test_prompts):
    """ìŒí–¥ ì••ì¶• ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    if not tokenizer:
        return [], 0.0, 0.0
    
    print("\nğŸ§ª ìŒí–¥ ì••ì¶• ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    
    results = []
    total_time = 0
    successful_generations = 0
    
    for i, prompt in enumerate(test_prompts[:3]):
        try:
            print(f"\n{i+1}. í”„ë¡¬í”„íŠ¸: '{prompt}'")
            
            inputs = tokenizer(prompt, return_tensors="pt")
            
            start_time = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 12,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            gen_time = time.time() - start_time
            total_time += gen_time
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            results.append(generated_text)
            successful_generations += 1
            
            print(f"   âœ… ìƒì„±: {generated_text}")
            print(f"   â±ï¸ ì‹œê°„: {gen_time*1000:.1f}ms")
            
        except Exception as e:
            print(f"   âŒ ìƒì„± ì‹¤íŒ¨: {e}")
            results.append("")
    
    avg_time = total_time / len(test_prompts) if test_prompts else 0
    success_rate = successful_generations / len(test_prompts)
    
    print(f"\nğŸ“ˆ ìŒí–¥ ì••ì¶• ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   ì„±ê³µë¥ : {success_rate:.1%} ({successful_generations}/{len(test_prompts)})")
    print(f"   í‰ê·  ì‹œê°„: {avg_time*1000:.1f}ms")
    
    return results, avg_time, success_rate


def run_fft_audio_compression_test():
    """FFT ìŒí–¥ ì²˜ë¦¬ ê¸°ë°˜ ì••ì¶• ì¢…í•© í…ŒìŠ¤íŠ¸"""
    
    print("=" * 80)
    print("ğŸµ FFT ìŒí–¥ ì²˜ë¦¬ ê¸°ë°˜ ì‹ ê²½ë§ ì••ì¶• í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print("ğŸ”Š í•µì‹¬ ê¸°ìˆ :")
    print("   â€¢ ê°€ì¤‘ì¹˜ â†’ ìŒí–¥ ì‹ í˜¸ ë³€í™˜")
    print("   â€¢ FFT ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„")
    print("   â€¢ ìŒí–¥ íŠ¹ì„± ê²€ì¶œ (ì¤‘ìš”í•œ ì£¼íŒŒìˆ˜ ì„±ë¶„)")
    print("   â€¢ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ì••ì¶•")
    print("   â€¢ Reality Stone + Helgason ê²°í•©")
    print("=" * 80)
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model, tokenizer, model_name = load_korean_model()
    
    if not model:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
        return
    
    # ëª¨ë¸ ì •ë³´
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ“‹ ëª¨ë¸ ì •ë³´:")
    print(f"   ì´ë¦„: {model_name}")
    print(f"   íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"   í¬ê¸°: {total_params * 4 / (1024**2):.1f}MB")
    
    # 2. ì›ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    test_prompts = [
        "ìŒì•…ì€ ë§ˆìŒì„",
        "ì˜¤ëŠ˜ ì•„ì¹¨ì—",
        "ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ"
    ]
    
    print("\nğŸ” ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •")
    original_results, original_time, original_success = test_audio_compressed_model(
        model, tokenizer, test_prompts
    )
    
    # 3. FFT ìŒí–¥ ì••ì¶• í…ŒìŠ¤íŠ¸
    compression_ratios = [0.3, 0.2, 0.15]  # 30%, 20%, 15%
    
    best_result = None
    test_results = []
    
    for ratio in compression_ratios:
        print(f"\nğŸ¼ ì••ì¶•ë¥  {ratio:.1%} í…ŒìŠ¤íŠ¸ (FFT ìŒí–¥ ì²˜ë¦¬)")
        print("-" * 60)
        
        try:
            # ëª¨ë¸ ë³µì‚¬
            test_model = copy.deepcopy(model)
            
            # FFT ìŒí–¥ ì••ì¶• ì ìš©
            compressed_model, actual_ratio, compression_success = apply_fft_audio_compression(
                test_model, ratio
            )
            
            # ì••ì¶•ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
            compressed_results, compressed_time, generation_success = test_audio_compressed_model(
                compressed_model, tokenizer, test_prompts
            )
            
            # ì„±ëŠ¥ í‰ê°€
            speed_improvement = original_time / compressed_time if compressed_time > 0 else 1.0
            overall_success = compression_success * generation_success
            
            result = {
                'target_ratio': ratio,
                'actual_ratio': actual_ratio,
                'compression_success': compression_success,
                'generation_success': generation_success,
                'overall_success': overall_success,
                'speed_improvement': speed_improvement,
                'memory_saved': (1 - actual_ratio) * 100,
                'compressed_time': compressed_time * 1000
            }
            
            test_results.append(result)
            
            print(f"\nğŸ“Š {ratio:.1%} FFT ìŒí–¥ ì••ì¶• ê²°ê³¼:")
            print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {actual_ratio:.3f}")
            print(f"   ì••ì¶• ì„±ê³µë¥ : {compression_success:.1%}")
            print(f"   ìƒì„± ì„±ê³µë¥ : {generation_success:.1%}")
            print(f"   ì¢…í•© ì„±ê³µë¥ : {overall_success:.1%}")
            print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {result['memory_saved']:.1f}%")
            print(f"   ì†ë„ í–¥ìƒ: {speed_improvement:.2f}x")
            
            # ìµœê³  ì„±ëŠ¥ ì¶”ì 
            if overall_success > 0.8 and (not best_result or 
                                        result['memory_saved'] > best_result['memory_saved']):
                best_result = result
                
        except Exception as e:
            print(f"   âŒ {ratio:.1%} ì••ì¶• í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # 4. ìµœì¢… ê²°ê³¼ ë°œí‘œ
    print(f"\nğŸ† FFT ìŒí–¥ ì²˜ë¦¬ ê¸°ë°˜ ì••ì¶• ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    
    if best_result:
        print(f"ğŸ‰ ìŒí–¥ ì²˜ë¦¬ ì••ì¶• ì„±ê³µ!")
        print(f"   ìµœê³  ì••ì¶•ë¥ : {best_result['target_ratio']:.1%}")
        print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {best_result['actual_ratio']:.3f}")
        print(f"   ì¢…í•© ì„±ê³µë¥ : {best_result['overall_success']:.1%}")
        print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {best_result['memory_saved']:.1f}%")
        print(f"   ì†ë„ í–¥ìƒ: {best_result['speed_improvement']:.2f}x")
        print(f"\nğŸµ FFT ìŒí–¥ ê²€ì¶œ ë°©ì‹ ì••ì¶• ì„±ê³µ!")
        print(f"ğŸ’¡ ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ì••ì¶•ìœ¼ë¡œ ì •í™•ë„ ìœ ì§€")
        
        # ì„±ê³µ ìš”ì¸ ë¶„ì„
        print(f"\nğŸ”Š ìŒí–¥ ì²˜ë¦¬ ì„±ê³µ ìš”ì¸:")
        for result in test_results:
            if result['overall_success'] > 0.8:
                print(f"   â€¢ {result['target_ratio']:.1%} ì••ì¶•: "
                      f"ì••ì¶• {result['compression_success']:.1%} + "
                      f"ìƒì„± {result['generation_success']:.1%} = "
                      f"ì¢…í•© {result['overall_success']:.1%}")
    else:
        print("âš ï¸ ìŒí–¥ ì²˜ë¦¬ ì••ì¶• ê°œì„  í•„ìš”")
        print("ğŸ’¡ ê°œì„  ë°©í–¥:")
        print("   â€¢ ë” ì •êµí•œ ì£¼íŒŒìˆ˜ ì„ íƒ")
        print("   â€¢ ìŠ¤í™íŠ¸ëŸ¼ ì—ë„ˆì§€ ì„ê³„ê°’ ì¡°ì •")
        print("   â€¢ ìœˆë„ìš° í¬ê¸° ìµœì í™”")
    
    print(f"\nâœ… FFT ìŒí–¥ ì²˜ë¦¬ ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    return test_results


if __name__ == "__main__":
    # ìŒí–¥ ì²˜ë¦¬ ì••ì¶• í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = run_fft_audio_compression_test()
    
    if results:
        successful_results = [r for r in results if r['overall_success'] > 0.8]
        print(f"\nğŸš€ ìŒí–¥ ì²˜ë¦¬ ì••ì¶• ìµœì¢… í‰ê°€:")
        print(f"   ì„±ê³µí•œ ì••ì¶•: {len(successful_results)}ê°œ")
        print(f"   FFT ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ ê²€ì¦ë¨")
        print(f"   ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ì••ì¶• ê¸°ìˆ  í™•ì¸")
        print(f"   ìŒí–¥ ê²€ì¶œ ë°©ì‹ ì‘ìš© ì„±ê³µ âœ…")
    else:
        print(f"\nğŸ”§ ìŒí–¥ ì²˜ë¦¬ ì••ì¶• ì¶”ê°€ ìµœì í™” í•„ìš”") 