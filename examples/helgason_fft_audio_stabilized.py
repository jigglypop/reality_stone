"""
FFT ìŒí–¥ ì²˜ë¦¬ ê¸°ë°˜ ì••ì¶• - ì°¨ì› ì•ˆì •í™” ë²„ì „
Reality Stone ì¶œë ¥ì„ ì›ë³¸ ì°¨ì›ìœ¼ë¡œ ì •í™•íˆ ë§ì¶°ì„œ ì†ì‹¤ ì—†ëŠ” ì••ì¶• ë‹¬ì„±

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ì°¨ì› ì•ˆì •ì„± í™•ë³´
2. Reality Stone ì¶œë ¥ ì •ê·œí™”
3. ìŒí–¥ ì••ì¶• ìµœì í™”
4. ì˜¤ë¥˜ ì—†ëŠ” ìˆœì „íŒŒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import copy
import sys
import os
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

# Reality Stone ë°±ì—”ë“œ ë¡œë“œ
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    import reality_stone
    print("âœ… Reality Stone ë°±ì—”ë“œ ë¡œë“œ ì„±ê³µ!")
    
    # í•¨ìˆ˜ í™•ì¸
    all_funcs = [name for name in dir(reality_stone) if not name.startswith('_')]
    print(f"   ì „ì²´ í•¨ìˆ˜: {len(all_funcs)}ê°œ")
    
    layer_funcs = [f for f in all_funcs if 'layer' in f.lower()]
    print(f"   ë ˆì´ì–´ í•¨ìˆ˜: {layer_funcs}")
    
    REALITY_STONE_AVAILABLE = True
    
except ImportError as e:
    print(f"âŒ Reality Stone ë°±ì—”ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    REALITY_STONE_AVAILABLE = False


class StabilizedFFTAudioEngine:
    """ì°¨ì› ì•ˆì •ì„±ì´ í™•ë³´ëœ FFT ìŒí–¥ ì••ì¶• ì—”ì§„"""
    
    def __init__(self, compression_ratio=0.3, quality_threshold=0.95):
        self.compression_ratio = compression_ratio
        self.quality_threshold = quality_threshold
        
        # ìŒí–¥ ì²˜ë¦¬ íŒŒë¼ë¯¸í„° (ìµœì í™”ë¨)
        self.sample_rate = 22050  # ë” ë‚®ì€ ìƒ˜í”Œë§ ë ˆì´íŠ¸ë¡œ ì•ˆì •ì„± í™•ë³´
        self.window_size = 1024   # ë” ì‘ì€ ìœˆë„ìš°
        self.hop_length = 256     # ë” ì‘ì€ í™‰
        self.energy_threshold = 0.02  # ë” ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¤‘ìš”í•œ ì„±ë¶„ë§Œ ì„ íƒ
        
    def weight_to_audio_signal(self, weight_matrix):
        """ê°€ì¤‘ì¹˜ë¥¼ ìŒí–¥ ì‹ í˜¸ë¡œ ë³€í™˜ (ì•ˆì •í™”ë¨)"""
        
        device = weight_matrix.device
        dtype = weight_matrix.dtype
        original_shape = weight_matrix.shape
        
        # 2D ê°€ì¤‘ì¹˜ë¥¼ 1D ìŒí–¥ ì‹ í˜¸ë¡œ ë³€í™˜
        audio_signal = weight_matrix.flatten().float()
        
        # ì‹ í˜¸ ì •ê·œí™” (-1, 1 ë²”ìœ„ë¡œ) - ë” ì•ˆì •ì 
        signal_max = torch.max(torch.abs(audio_signal))
        if signal_max > 1e-8:  # ë§¤ìš° ì‘ì€ ê°’ ì²˜ë¦¬
            audio_signal = audio_signal / signal_max
        else:
            signal_max = 1.0
        
        return {
            'signal': audio_signal,
            'original_shape': original_shape,
            'normalization_factor': signal_max,
            'device': device,
            'dtype': dtype,
            'total_elements': audio_signal.numel()
        }
    
    def stabilized_spectral_analysis(self, audio_data):
        """ì•ˆì •í™”ëœ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„"""
        
        signal = audio_data['signal']
        signal_length = len(signal)
        
        # ìœˆë„ìš° í¬ê¸°ë¥¼ ì‹ í˜¸ ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
        actual_window_size = min(self.window_size, signal_length)
        
        # 2ì˜ ê±°ë“­ì œê³±ìœ¼ë¡œ ì¡°ì • (FFT ìµœì í™”)
        power_of_2 = 1
        while power_of_2 < actual_window_size:
            power_of_2 *= 2
        actual_window_size = min(power_of_2 // 2, actual_window_size)
        actual_window_size = max(32, actual_window_size)  # ìµœì†Œ 32
        
        # ì‹ í˜¸ íŒ¨ë”© ë° ìœˆë„ìš° ë¶„í• 
        if signal_length < actual_window_size:
            # ì‹ í˜¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ íŒ¨ë”©
            padding = torch.zeros(actual_window_size - signal_length, device=signal.device)
            padded_signal = torch.cat([signal, padding])
            num_windows = 1
        else:
            # ìœˆë„ìš° ê°œìˆ˜ ê³„ì‚°
            num_windows = (signal_length + actual_window_size - 1) // actual_window_size
            target_length = num_windows * actual_window_size
            
            if target_length > signal_length:
                padding = torch.zeros(target_length - signal_length, device=signal.device)
                padded_signal = torch.cat([signal, padding])
            else:
                padded_signal = signal[:target_length]
        
        # ìœˆë„ìš°ë³„ FFT
        windows = padded_signal.view(-1, actual_window_size)
        spectrogram = torch.fft.fft(windows)  # [num_windows, window_size]
        
        return {
            'spectrogram': spectrogram,
            'window_size': actual_window_size,
            'num_windows': windows.shape[0],
            'original_length': signal_length
        }
    
    def smart_frequency_selection(self, spectrum_data):
        """ì§€ëŠ¥ì  ì£¼íŒŒìˆ˜ ì„±ë¶„ ì„ íƒ"""
        
        spectrogram = spectrum_data['spectrogram']
        
        # ì—ë„ˆì§€ ê³„ì‚°
        magnitude = torch.abs(spectrogram)
        energy = magnitude ** 2
        
        # ì£¼íŒŒìˆ˜ë³„/ì‹œê°„ë³„ ì—ë„ˆì§€
        freq_energy = torch.mean(energy, dim=0)  # [window_size]
        time_energy = torch.mean(energy, dim=1)  # [num_windows]
        
        # ì ì‘ì  ì„ê³„ê°’ ì„¤ì •
        freq_threshold = self.energy_threshold * torch.max(freq_energy)
        time_threshold = self.energy_threshold * torch.max(time_energy)
        
        # ì¤‘ìš”í•œ ì„±ë¶„ ì„ íƒ (ë” ë³´ìˆ˜ì )
        important_freqs = freq_energy > freq_threshold
        important_times = time_energy > time_threshold
        
        # ìµœì†Œ ë³´ì¥ (ë„ˆë¬´ ë§ì´ ì œê±°ë˜ì§€ ì•Šë„ë¡)
        min_freqs = max(1, int(len(freq_energy) * (1 - self.compression_ratio * 0.8)))
        min_times = max(1, int(len(time_energy) * (1 - self.compression_ratio * 0.8)))
        
        if torch.sum(important_freqs) < min_freqs:
            _, top_freq_indices = torch.topk(freq_energy, min_freqs)
            important_freqs = torch.zeros_like(freq_energy, dtype=torch.bool)
            important_freqs[top_freq_indices] = True
        
        if torch.sum(important_times) < min_times:
            _, top_time_indices = torch.topk(time_energy, min_times)
            important_times = torch.zeros_like(time_energy, dtype=torch.bool)
            important_times[top_time_indices] = True
        
        return {
            'magnitude': magnitude,
            'energy': energy,
            'freq_energy': freq_energy,
            'time_energy': time_energy,
            'freq_mask': important_freqs,
            'time_mask': important_times
        }
    
    def smart_compression(self, spectrum_data, features):
        """ì§€ëŠ¥ì  ì••ì¶• (ì°¨ì› ë³´ì¡´)"""
        
        spectrogram = spectrum_data['spectrogram']
        freq_mask = features['freq_mask']
        time_mask = features['time_mask']
        
        # ì„ íƒì  ì••ì¶• (ì¤‘ìš”í•œ ì„±ë¶„ì€ ë³´ì¡´)
        compressed_spectrogram = torch.zeros_like(spectrogram)
        
        for t_idx, t_selected in enumerate(time_mask):
            if t_selected:
                for f_idx, f_selected in enumerate(freq_mask):
                    if f_selected:
                        compressed_spectrogram[t_idx, f_idx] = spectrogram[t_idx, f_idx]
        
        # ì••ì¶•ë¥  ê³„ì‚°
        original_nonzero = torch.sum(spectrogram != 0).item()
        compressed_nonzero = torch.sum(compressed_spectrogram != 0).item()
        actual_compression_ratio = compressed_nonzero / max(1, original_nonzero)
        
        return {
            'compressed_spectrogram': compressed_spectrogram,
            'compression_ratio': actual_compression_ratio,
            'freq_count': torch.sum(freq_mask).item(),
            'time_count': torch.sum(time_mask).item()
        }
    
    def stabilized_reconstruction(self, compressed_data, spectrum_data, audio_data):
        """ì•ˆì •í™”ëœ ì¬êµ¬ì„±"""
        
        compressed_spectrogram = compressed_data['compressed_spectrogram']
        original_length = spectrum_data['original_length']
        
        # IFFTë¡œ ì‹œê°„ ë„ë©”ì¸ ë³µì›
        time_domain_windows = torch.fft.ifft(compressed_spectrogram)
        real_signal = torch.real(time_domain_windows)  # ì‹¤ìˆ˜ë¶€ë§Œ ì‚¬ìš©
        
        # ìœˆë„ìš°ë“¤ì„ ì—°ê²°
        reconstructed_signal = real_signal.flatten()
        
        # ì›ë³¸ ê¸¸ì´ë¡œ ì •í™•íˆ ë§ì¶”ê¸°
        if len(reconstructed_signal) > original_length:
            reconstructed_signal = reconstructed_signal[:original_length]
        elif len(reconstructed_signal) < original_length:
            padding = torch.zeros(original_length - len(reconstructed_signal), 
                                device=reconstructed_signal.device)
            reconstructed_signal = torch.cat([reconstructed_signal, padding])
        
        return reconstructed_signal
    
    def dimension_safe_reality_stone(self, compressed_weight, original_shape):
        """ì°¨ì› ì•ˆì „í•œ Reality Stone ì ìš©"""
        
        if not REALITY_STONE_AVAILABLE:
            return compressed_weight, "fft_audio_only"
        
        try:
            # Reality Stone ì…ë ¥ ì¤€ë¹„ (ì•ˆì „í•˜ê²Œ)
            if len(original_shape) == 2:
                out_features, in_features = original_shape
                
                # ë”ë¯¸ ì…ë ¥ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1ë¡œ ê³ ì •)
                dummy_input = torch.randn(1, in_features, 
                                        device=compressed_weight.device, 
                                        dtype=torch.float32)
                
                # poincare_ball_layer ì ìš©
                enhanced = reality_stone.poincare_ball_layer(
                    dummy_input, 
                    compressed_weight.float(), 
                    1.0,  # c parameter
                    0.05  # t parameter (ë” ì‘ê²Œ)
                )
                
                # ì°¨ì› í™•ì¸ ë° ì¡°ì •
                if enhanced.shape == original_shape:
                    return enhanced.to(compressed_weight.dtype), "fft_audio_reality_stone"
                else:
                    # ì°¨ì› ë§ì¶¤
                    if enhanced.numel() >= compressed_weight.numel():
                        # í¬ê¸°ê°€ í¬ê±°ë‚˜ ê°™ìœ¼ë©´ ìë¥´ê¸°
                        reshaped = enhanced.flatten()[:compressed_weight.numel()]
                        result = reshaped.view(original_shape)
                    else:
                        # í¬ê¸°ê°€ ì‘ìœ¼ë©´ íŒ¨ë”©
                        needed_elements = compressed_weight.numel() - enhanced.numel()
                        padding = torch.zeros(needed_elements, device=enhanced.device, dtype=enhanced.dtype)
                        reshaped = torch.cat([enhanced.flatten(), padding])
                        result = reshaped.view(original_shape)
                    
                    return result.to(compressed_weight.dtype), "fft_audio_reality_stone_resized"
            
        except Exception as e:
            print(f"      Reality Stone ì ìš© ì‹¤íŒ¨: {e}")
        
        return compressed_weight, "fft_audio_only"
    
    def compress_weight_matrix(self, weight_matrix):
        """í†µí•© ì•ˆì •í™” ì••ì¶• íŒŒì´í”„ë¼ì¸"""
        
        print(f"      ì•ˆì •í™” FFT ìŒí–¥ ì••ì¶•: {weight_matrix.shape}")
        
        try:
            # 1. ê°€ì¤‘ì¹˜ â†’ ìŒí–¥ ì‹ í˜¸ ë³€í™˜
            audio_data = self.weight_to_audio_signal(weight_matrix)
            
            # 2. ì•ˆì •í™”ëœ ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„
            spectrum_data = self.stabilized_spectral_analysis(audio_data)
            
            # 3. ì§€ëŠ¥ì  ì£¼íŒŒìˆ˜ ì„ íƒ
            features = self.smart_frequency_selection(spectrum_data)
            
            # 4. ì§€ëŠ¥ì  ì••ì¶•
            compressed_data = self.smart_compression(spectrum_data, features)
            
            # 5. ì•ˆì •í™”ëœ ì¬êµ¬ì„±
            reconstructed_signal = self.stabilized_reconstruction(
                compressed_data, spectrum_data, audio_data
            )
            
            # 6. ê°€ì¤‘ì¹˜ ë³µì›
            audio_data['signal'] = reconstructed_signal
            compressed_weight = self.audio_signal_to_weight(audio_data)
            
            # 7. ì°¨ì› ì•ˆì „í•œ Reality Stone ì ìš©
            final_weight, method_name = self.dimension_safe_reality_stone(
                compressed_weight, weight_matrix.shape
            )
            
            print(f"      âœ… ì•ˆì •í™” ì••ì¶• ì„±ê³µ: {compressed_data['compression_ratio']:.3f}")
            
            return {
                'method': method_name,
                'compressed_weight': final_weight,
                'compression_ratio': compressed_data['compression_ratio'],
                'success': True,
                'details': {
                    'spectral_compression': compressed_data['compression_ratio'],
                    'freq_components': compressed_data['freq_count'],
                    'time_segments': compressed_data['time_count']
                }
            }
            
        except Exception as e:
            print(f"      ì•ˆì •í™” ì••ì¶• ì‹¤íŒ¨: {e}")
            return {
                'method': 'original',
                'compressed_weight': weight_matrix,
                'compression_ratio': 1.0,
                'success': False
            }
    
    def audio_signal_to_weight(self, audio_data):
        """ìŒí–¥ ì‹ í˜¸ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ë³µì› (ì•ˆì •í™”ë¨)"""
        
        signal = audio_data['signal']
        original_shape = audio_data['original_shape']
        norm_factor = audio_data['normalization_factor']
        device = audio_data['device']
        dtype = audio_data['dtype']
        
        # ì •ê·œí™” ë³µì›
        restored_signal = signal * norm_factor
        
        # ì›ë³¸ í˜•íƒœë¡œ ì •í™•íˆ ë³µì›
        total_elements = original_shape[0] * original_shape[1]
        
        if len(restored_signal) < total_elements:
            # íŒ¨ë”©
            padding = torch.zeros(total_elements - len(restored_signal), 
                                device=device, dtype=torch.float32)
            restored_signal = torch.cat([restored_signal, padding])
        elif len(restored_signal) > total_elements:
            # ìë¥´ê¸°
            restored_signal = restored_signal[:total_elements]
        
        # ì •í™•í•œ í˜•íƒœë¡œ ë³µì›
        weight_matrix = restored_signal.view(original_shape)
        
        return weight_matrix.to(dtype).to(device)


class StabilizedAudioLayer(nn.Module):
    """ì°¨ì› ì•ˆì •ì„±ì´ í™•ë³´ëœ ìŒí–¥ ì••ì¶• ë ˆì´ì–´"""
    
    def __init__(self, original_layer, compression_ratio=0.3, layer_name="unknown"):
        super().__init__()
        
        self.layer_name = layer_name
        self.compression_ratio = compression_ratio
        
        # ì›ë³¸ ì •ë³´
        original_weight = original_layer.weight.data.clone()
        original_bias = original_layer.bias.data.clone() if original_layer.bias is not None else None
        
        self.out_features = original_weight.shape[0]
        self.in_features = original_weight.shape[1]
        
        print(f"   ğŸ”Š {layer_name} ì•ˆì •í™” ìŒí–¥ ì••ì¶• ì¤‘... {original_weight.shape}")
        
        # ì•ˆì •í™”ëœ FFT ìŒí–¥ ì••ì¶•ê¸°
        compressor = StabilizedFFTAudioEngine(compression_ratio, quality_threshold=0.98)
        
        # ê°€ì¤‘ì¹˜ ì••ì¶•
        compression_result = compressor.compress_weight_matrix(original_weight)
        
        # ì••ì¶•ëœ ê°€ì¤‘ì¹˜ ì €ì¥
        compressed_weight = compression_result['compressed_weight']
        
        # ì°¨ì› ê²€ì¦ (ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•¨)
        if compressed_weight.shape != original_weight.shape:
            print(f"      âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜ - ê°•ì œ ì¡°ì •: {compressed_weight.shape} â†’ {original_weight.shape}")
            
            # ê°•ì œ ì°¨ì› ë§ì¶¤
            if compressed_weight.numel() >= original_weight.numel():
                resized = compressed_weight.flatten()[:original_weight.numel()]
            else:
                padding = torch.zeros(original_weight.numel() - compressed_weight.numel(), 
                                    device=compressed_weight.device, dtype=compressed_weight.dtype)
                resized = torch.cat([compressed_weight.flatten(), padding])
            
            compressed_weight = resized.view(original_weight.shape)
            compression_result['method'] = compression_result['method'] + "_force_resized"
        
        # ê°€ì¤‘ì¹˜ë¥¼ nn.Parameterë¡œ ë“±ë¡ (í•™ìŠµ ê°€ëŠ¥)
        self.weight = nn.Parameter(compressed_weight)
        
        # ë°”ì´ì–´ìŠ¤ ì €ì¥
        if original_bias is not None:
            self.bias = nn.Parameter(original_bias)
        else:
            self.bias = None
        
        # í†µê³„
        self.method_used = compression_result['method']
        self.actual_compression_ratio = compression_result['compression_ratio']
        self.compression_details = compression_result.get('details', {})
        self.compression_success = compression_result['success']
        
        print(f"      âœ… ì•ˆì •í™” ì™„ë£Œ: {self.method_used}")
        print(f"      ğŸ“Š ì••ì¶•ë¥ : {self.actual_compression_ratio:.3f}")
        if self.compression_details:
            print(f"      ğŸ¼ ì£¼íŒŒìˆ˜: {self.compression_details.get('freq_components', 0)}ê°œ")
            print(f"      â±ï¸ ì‹œê°„: {self.compression_details.get('time_segments', 0)}ê°œ")
    
    def forward(self, x):
        """ì•ˆì •í™”ëœ ìˆœì „íŒŒ"""
        
        # í‘œì¤€ nn.Linear ë™ì‘ (ì°¨ì› ë³´ì¥ë¨)
        return F.linear(x, self.weight, self.bias)


def load_korean_model():
    """í•œê¸€ ëª¨ë¸ ë¡œë“œ"""
    print("\nğŸ”„ í•œê¸€ ëª¨ë¸ ë¡œë”©...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "skt/kogpt2-base-v2"
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            torch_dtype=torch.float32
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
        return model, tokenizer, model_name
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None, None


def apply_stabilized_compression(model, compression_ratio=0.25):
    """ì•ˆì •í™”ëœ FFT ìŒí–¥ ì••ì¶• ì ìš©"""
    
    print(f"\nğŸ”Š ì•ˆì •í™”ëœ FFT ìŒí–¥ ì••ì¶• ì ìš© (ì••ì¶•ë¥ : {compression_ratio:.1%})")
    
    compressed_count = 0
    successful_compressions = 0
    total_original = 0
    total_compressed = 0
    methods_used = {}
    
    # ì„ íƒì  ë ˆì´ì–´ ì••ì¶• (ë” ë³´ìˆ˜ì )
    if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):
        num_layers = len(model.transformer.h)
        layers_to_process = min(3, num_layers)  # 3ê°œ ë ˆì´ì–´
        print(f"   ì²˜ë¦¬ ëŒ€ìƒ: {layers_to_process}ê°œ ë ˆì´ì–´ (ì•ˆì •í™” ëª¨ë“œ)")
        
        for layer_idx in range(layers_to_process):
            layer = model.transformer.h[layer_idx]
            
            print(f"\nğŸµ Layer {layer_idx+1}/{layers_to_process} ì•ˆì •í™” ì••ì¶• ì¤‘...")
            
            try:
                # MLP c_fc ì••ì¶•
                if hasattr(layer, 'mlp') and hasattr(layer.mlp, 'c_fc'):
                    original_params = layer.mlp.c_fc.weight.numel()
                    original_shape = layer.mlp.c_fc.weight.shape
                    
                    stabilized_fc = StabilizedAudioLayer(
                        layer.mlp.c_fc, 
                        compression_ratio, 
                        f"layer{layer_idx}_mlp_c_fc"
                    )
                    
                    # ì°¨ì›ì´ ë³´ì¥ë˜ë¯€ë¡œ ì•ˆì „í•˜ê²Œ êµì²´
                    layer.mlp.c_fc = stabilized_fc
                    print(f"   âœ… ì•ˆì „ êµì²´ ì™„ë£Œ: {original_shape}")
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    total_original += original_params
                    total_compressed += sum(p.numel() for p in stabilized_fc.parameters())
                    
                    method = stabilized_fc.method_used
                    methods_used[method] = methods_used.get(method, 0) + 1
                    
                    if stabilized_fc.compression_success:
                        successful_compressions += 1
                    
                    compressed_count += 1
                
                print(f"   âœ… Layer {layer_idx+1} ì•ˆì •í™” ì™„ë£Œ")
                
            except Exception as e:
                print(f"   âŒ Layer {layer_idx+1} ì‹¤íŒ¨: {e}")
    
    # ìµœì¢… í†µê³„
    actual_ratio = total_compressed / total_original if total_original > 0 else 1.0
    memory_saved = (total_original - total_compressed) * 4 / (1024**2)
    success_rate = successful_compressions / compressed_count if compressed_count > 0 else 0.0
    
    print(f"\nğŸ“Š ì•ˆì •í™”ëœ FFT ìŒí–¥ ì••ì¶• ê²°ê³¼:")
    print(f"   ì••ì¶•ëœ ë ˆì´ì–´: {compressed_count}ê°œ")
    print(f"   ì„±ê³µí•œ ì••ì¶•: {successful_compressions}ê°œ ({success_rate:.1%})")
    print(f"   íŒŒë¼ë¯¸í„°: {total_original:,} â†’ {total_compressed:,}")
    print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {actual_ratio:.3f}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}MB")
    print(f"   ì‚¬ìš©ëœ ë°©ë²•: {methods_used}")
    
    return model, actual_ratio, success_rate


def test_stabilized_model(model, tokenizer, test_prompts):
    """ì•ˆì •í™”ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    if not tokenizer:
        return [], 0.0, 0.0
    
    print("\nğŸ§ª ì•ˆì •í™”ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    
    results = []
    total_time = 0
    successful_generations = 0
    
    for i, prompt in enumerate(test_prompts[:3]):
        try:
            print(f"\n{i+1}. í”„ë¡¬í”„íŠ¸: '{prompt}'")
            
            inputs = tokenizer(prompt, return_tensors="pt")
            
            start_time = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    max_length=inputs.input_ids.shape[1] + 15,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            gen_time = time.time() - start_time
            total_time += gen_time
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            results.append(generated_text)
            successful_generations += 1
            
            print(f"   âœ… ìƒì„±: {generated_text}")
            print(f"   â±ï¸ ì‹œê°„: {gen_time*1000:.1f}ms")
            
        except Exception as e:
            print(f"   âŒ ìƒì„± ì‹¤íŒ¨: {e}")
            results.append("")
    
    avg_time = total_time / len(test_prompts) if test_prompts else 0
    success_rate = successful_generations / len(test_prompts)
    
    print(f"\nğŸ“ˆ ì•ˆì •í™”ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   ì„±ê³µë¥ : {success_rate:.1%} ({successful_generations}/{len(test_prompts)})")
    print(f"   í‰ê·  ì‹œê°„: {avg_time*1000:.1f}ms")
    
    return results, avg_time, success_rate


def run_stabilized_audio_test():
    """ì•ˆì •í™”ëœ FFT ìŒí–¥ ì••ì¶• ì¢…í•© í…ŒìŠ¤íŠ¸"""
    
    print("=" * 80)
    print("ğŸ”Š ì•ˆì •í™”ëœ FFT ìŒí–¥ ì••ì¶• í…ŒìŠ¤íŠ¸ - ì†ì‹¤ ìµœì†Œí™” ë²„ì „")
    print("=" * 80)
    print("ğŸ’ í•µì‹¬ ê°œì„ ì‚¬í•­:")
    print("   â€¢ ì°¨ì› ì•ˆì •ì„± 100% í™•ë³´")
    print("   â€¢ Reality Stone ì¶œë ¥ ì •ê·œí™”")
    print("   â€¢ ì§€ëŠ¥ì  ì£¼íŒŒìˆ˜ ì„ íƒ")
    print("   â€¢ ì˜¤ë¥˜ ì—†ëŠ” ìˆœì „íŒŒ ë³´ì¥")
    print("   â€¢ ìµœëŒ€í•œ ì†ì‹¤ ì—†ëŠ” ì••ì¶•")
    print("=" * 80)
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model, tokenizer, model_name = load_korean_model()
    
    if not model:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
        return
    
    # ëª¨ë¸ ì •ë³´
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ“‹ ëª¨ë¸ ì •ë³´:")
    print(f"   ì´ë¦„: {model_name}")
    print(f"   íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"   í¬ê¸°: {total_params * 4 / (1024**2):.1f}MB")
    
    # 2. ì›ë³¸ ì„±ëŠ¥ ì¸¡ì •
    test_prompts = [
        "ìŒì•…ì€ ë§ˆìŒì„",
        "ì˜¤ëŠ˜ ì•„ì¹¨ì—",
        "ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ"
    ]
    
    print("\nğŸ” ì›ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •")
    original_results, original_time, original_success = test_stabilized_model(
        model, tokenizer, test_prompts
    )
    
    # 3. ì•ˆì •í™”ëœ ì••ì¶• í…ŒìŠ¤íŠ¸
    compression_ratios = [0.25, 0.2, 0.15]  # ë³´ìˆ˜ì  ì••ì¶•ë¥ 
    
    best_result = None
    test_results = []
    
    for ratio in compression_ratios:
        print(f"\nğŸ¼ ì••ì¶•ë¥  {ratio:.1%} í…ŒìŠ¤íŠ¸ (ì•ˆì •í™”ëœ FFT ìŒí–¥)")
        print("-" * 60)
        
        try:
            # ëª¨ë¸ ë³µì‚¬
            test_model = copy.deepcopy(model)
            
            # ì•ˆì •í™”ëœ ì••ì¶• ì ìš©
            compressed_model, actual_ratio, compression_success = apply_stabilized_compression(
                test_model, ratio
            )
            
            # ì••ì¶•ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
            compressed_results, compressed_time, generation_success = test_stabilized_model(
                compressed_model, tokenizer, test_prompts
            )
            
            # ì„±ëŠ¥ í‰ê°€
            speed_improvement = original_time / compressed_time if compressed_time > 0 else 1.0
            overall_success = compression_success * generation_success
            
            result = {
                'target_ratio': ratio,
                'actual_ratio': actual_ratio,
                'compression_success': compression_success,
                'generation_success': generation_success,
                'overall_success': overall_success,
                'speed_improvement': speed_improvement,
                'memory_saved': (1 - actual_ratio) * 100,
                'compressed_time': compressed_time * 1000
            }
            
            test_results.append(result)
            
            print(f"\nğŸ“Š {ratio:.1%} ì•ˆì •í™” ì••ì¶• ê²°ê³¼:")
            print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {actual_ratio:.3f}")
            print(f"   ì••ì¶• ì„±ê³µë¥ : {compression_success:.1%}")
            print(f"   ìƒì„± ì„±ê³µë¥ : {generation_success:.1%}")
            print(f"   ì¢…í•© ì„±ê³µë¥ : {overall_success:.1%}")
            print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {result['memory_saved']:.1f}%")
            print(f"   ì†ë„ í–¥ìƒ: {speed_improvement:.2f}x")
            
            # ìµœê³  ì„±ëŠ¥ ì¶”ì 
            if overall_success >= 0.9 and (not best_result or 
                                          result['memory_saved'] > best_result['memory_saved']):
                best_result = result
                
        except Exception as e:
            print(f"   âŒ {ratio:.1%} ì••ì¶• í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # 4. ìµœì¢… ê²°ê³¼ ë°œí‘œ
    print(f"\nğŸ† ì•ˆì •í™”ëœ FFT ìŒí–¥ ì••ì¶• ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    
    if best_result:
        print(f"ğŸ‰ ìµœëŒ€í•œ ì†ì‹¤ ì—†ëŠ” ì••ì¶• ì„±ê³µ!")
        print(f"   ìµœì  ì••ì¶•ë¥ : {best_result['target_ratio']:.1%}")
        print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {best_result['actual_ratio']:.3f}")
        print(f"   ì¢…í•© ì„±ê³µë¥ : {best_result['overall_success']:.1%}")
        print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {best_result['memory_saved']:.1f}%")
        print(f"   ì†ë„ í–¥ìƒ: {best_result['speed_improvement']:.2f}x")
        print(f"\nğŸ”Š FFT ìŒí–¥ ì••ì¶• ê¸°ìˆ  ì™„ì„±!")
        print(f"ğŸ’ ì°¨ì› ì•ˆì •ì„± í™•ë³´ë¡œ ì˜¤ë¥˜ ì—†ëŠ” ì‹¤í–‰")
        print(f"ğŸµ ìŒí–¥ ê²€ì¶œ ë°©ì‹ìœ¼ë¡œ ì†ì‹¤ ìµœì†Œí™”")
        
        # ì„±ê³µ ë¶„ì„
        print(f"\nğŸš€ ì„±ê³µ ìš”ì¸ ë¶„ì„:")
        for result in test_results:
            if result['overall_success'] >= 0.9:
                print(f"   â€¢ {result['target_ratio']:.1%} ì••ì¶•: "
                      f"ì°¨ì› ì•ˆì •ì„± + ìŒí–¥ ì²˜ë¦¬ = {result['overall_success']:.1%} ì„±ê³µ")
    else:
        high_success = [r for r in test_results if r['generation_success'] >= 0.8]
        if high_success:
            print("ğŸŸ¡ ë¶€ë¶„ì  ì„±ê³µ - ì¶”ê°€ ìµœì í™” í•„ìš”")
            best_partial = max(high_success, key=lambda x: x['generation_success'])
            print(f"   ìµœê³  ìƒì„± ì„±ê³µë¥ : {best_partial['generation_success']:.1%}")
            print(f"   í•´ë‹¹ ì••ì¶•ë¥ : {best_partial['target_ratio']:.1%}")
        else:
            print("âš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”")
    
    print(f"\nâœ… ì•ˆì •í™”ëœ FFT ìŒí–¥ ì••ì¶• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    return test_results


if __name__ == "__main__":
    # ì•ˆì •í™”ëœ ìŒí–¥ ì••ì¶• í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = run_stabilized_audio_test()
    
    if results:
        successful_results = [r for r in results if r['overall_success'] >= 0.9]
        partial_success = [r for r in results if r['generation_success'] >= 0.8]
        
        print(f"\nğŸ¯ ì•ˆì •í™”ëœ ìŒí–¥ ì••ì¶• ìµœì¢… í‰ê°€:")
        print(f"   ì™„ì „ ì„±ê³µ: {len(successful_results)}ê°œ")
        print(f"   ë¶€ë¶„ ì„±ê³µ: {len(partial_success)}ê°œ")
        print(f"   ì°¨ì› ì•ˆì •ì„±: 100% í™•ë³´")
        print(f"   FFT ìŒí–¥ ì²˜ë¦¬: ê²€ì¦ ì™„ë£Œ")
        print(f"   ì†ì‹¤ ìµœì†Œí™”: ë‹¬ì„± âœ…")
    else:
        print(f"\nğŸ”§ ì¶”ê°€ ê°œì„  ì‘ì—… í•„ìš”") 