"""
í•œêµ­ì–´ ìµœì í™” ì••ì¶• ì‹œìŠ¤í…œ: ì‹¤ì œ íŒŒë¼ë¯¸í„° ê°ì†Œì™€ í•œê¸€ í’ˆì§ˆ í–¥ìƒ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import re
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")


class KoreanTokenizer:
    """ê°„ë‹¨í•œ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (ìëª¨ ë‹¨ìœ„)"""
    
    def __init__(self):
        # í•œê¸€ ìëª¨ ë¶„í•´ í…Œì´ë¸”
        self.cho = "ã„±ã„²ã„´ã„·ã„¸ã„¹ã…ã…‚ã…ƒã……ã…†ã…‡ã…ˆã…‰ã…Šã…‹ã…Œã…ã…"  # ì´ˆì„±
        self.jung = "ã…ã…ã…‘ã…’ã…“ã…”ã…•ã…–ã…—ã…˜ã…™ã…šã…›ã…œã…ã…ã…Ÿã… ã…¡ã…¢ã…£"  # ì¤‘ì„±
        self.jong = " ã„±ã„²ã„³ã„´ã„µã„¶ã„·ã„¹ã„ºã„»ã„¼ã„½ã„¾ã„¿ã…€ã…ã…‚ã…„ã……ã…†ã…‡ã…ˆã…Šã…‹ã…Œã…ã…"  # ì¢…ì„±
        
        # ì „ì²´ ì–´íœ˜
        self.vocab = ['<pad>', '<unk>', '<bos>', '<eos>'] + list(self.cho) + list(self.jung) + list(self.jong)
        self.vocab += [chr(i) for i in range(ord('a'), ord('z')+1)]  # ì˜ì–´ ì†Œë¬¸ì
        self.vocab += [chr(i) for i in range(ord('A'), ord('Z')+1)]  # ì˜ì–´ ëŒ€ë¬¸ì
        self.vocab += [chr(i) for i in range(ord('0'), ord('9')+1)]  # ìˆ«ì
        self.vocab += [' ', '.', ',', '!', '?', ':', ';', '-', '(', ')', '[', ']']
        
        self.token_to_id = {token: idx for idx, token in enumerate(self.vocab)}
        self.id_to_token = {idx: token for idx, token in enumerate(self.vocab)}
        self.vocab_size = len(self.vocab)
        
    def decompose_korean(self, char):
        """í•œê¸€ ë¬¸ìë¥¼ ìëª¨ë¡œ ë¶„í•´"""
        if not ('ê°€' <= char <= 'í£'):
            return [char]
        
        code = ord(char) - ord('ê°€')
        cho_idx = code // (21 * 28)
        jung_idx = (code % (21 * 28)) // 28
        jong_idx = code % 28
        
        result = [self.cho[cho_idx], self.jung[jung_idx]]
        if jong_idx > 0:
            result.append(self.jong[jong_idx])
        
        return result
    
    def encode(self, text: str) -> List[int]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ë³€í™˜"""
        tokens = [self.token_to_id.get('<bos>', 2)]
        
        for char in text:
            if 'ê°€' <= char <= 'í£':
                # í•œê¸€ ë¶„í•´
                jamos = self.decompose_korean(char)
                for jamo in jamos:
                    tokens.append(self.token_to_id.get(jamo, self.token_to_id['<unk>']))
            else:
                tokens.append(self.token_to_id.get(char, self.token_to_id['<unk>']))
        
        tokens.append(self.token_to_id.get('<eos>', 3))
        return tokens
    
    def decode(self, token_ids: List[int]) -> str:
        """í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text = ""
        i = 0
        
        while i < len(token_ids):
            token_id = token_ids[i]
            if token_id in [0, 1, 2, 3]:  # íŠ¹ìˆ˜ í† í° ê±´ë„ˆë›°ê¸°
                i += 1
                continue
                
            char = self.id_to_token.get(token_id, '')
            
            # í•œê¸€ ìëª¨ ì¡°í•© ì‹œë„
            if char in self.cho and i + 1 < len(token_ids):
                jung_char = self.id_to_token.get(token_ids[i + 1], '')
                if jung_char in self.jung:
                    # ì´ˆì„± + ì¤‘ì„±
                    cho_idx = self.cho.index(char)
                    jung_idx = self.jung.index(jung_char)
                    jong_idx = 0
                    
                    # ì¢…ì„± í™•ì¸
                    if i + 2 < len(token_ids):
                        jong_char = self.id_to_token.get(token_ids[i + 2], '')
                        if jong_char in self.jong and jong_char != ' ':
                            jong_idx = self.jong.index(jong_char)
                            i += 1
                    
                    # í•œê¸€ ì¡°í•©
                    korean_char = chr(ord('ê°€') + cho_idx * 21 * 28 + jung_idx * 28 + jong_idx)
                    text += korean_char
                    i += 2
                else:
                    text += char
                    i += 1
            else:
                text += char
                i += 1
        
        return text


class TrueHelgasonMLP(nn.Module):
    """ì‹¤ì œ íŒŒë¼ë¯¸í„° ê°ì†Œë¥¼ ë‹¬ì„±í•˜ëŠ” í—¬ê°€ì† MLP"""
    
    def __init__(self, hidden_size: int, intermediate_size: int, compression_ratio: float = 0.1):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.compression_ratio = compression_ratio
        
        # ì••ì¶•ëœ ì¤‘ê°„ ì°¨ì› ê³„ì‚°
        self.compressed_dim = max(4, int(min(hidden_size, intermediate_size) * compression_ratio))
        
        print(f"   í—¬ê°€ì† MLP: {hidden_size} â†’ {self.compressed_dim} â†’ {intermediate_size} â†’ {self.compressed_dim} â†’ {hidden_size}")
        
        # ì••ì¶•ëœ ë ˆì´ì–´ë“¤
        self.compress_in = nn.Linear(hidden_size, self.compressed_dim, bias=False)
        self.gate_expand = nn.Linear(self.compressed_dim, intermediate_size, bias=False)
        self.up_expand = nn.Linear(self.compressed_dim, intermediate_size, bias=False)
        self.compress_mid = nn.Linear(intermediate_size, self.compressed_dim, bias=False)
        self.final_out = nn.Linear(self.compressed_dim, hidden_size, bias=False)
        
        # ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for layer in [self.compress_in, self.gate_expand, self.up_expand, self.compress_mid, self.final_out]:
            nn.init.xavier_uniform_(layer.weight)
            
    def forward(self, x):
        """ìˆœì „íŒŒ"""
        # ì••ì¶•
        compressed = self.compress_in(x)
        
        # í™•ì¥
        gate_out = self.gate_expand(compressed)
        up_out = self.up_expand(compressed)
        
        # ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ (SiLU í™œì„±í™”)
        activated = F.silu(gate_out) * up_out
        
        # ì¬ì••ì¶• ë° ì¶œë ¥
        recompressed = self.compress_mid(activated)
        output = self.final_out(recompressed)
        
        return output
    
    def get_compression_ratio(self):
        """ì‹¤ì œ ì••ì¶•ë¥  ê³„ì‚°"""
        original_params = self.hidden_size * self.intermediate_size * 3  # gate + up + down
        compressed_params = sum(p.numel() for p in self.parameters())
        return compressed_params / original_params


class KoreanGPT(nn.Module):
    """í•œêµ­ì–´ íŠ¹í™” GPT ëª¨ë¸"""
    
    def __init__(self, vocab_size: int, hidden_size: int = 256, num_layers: int = 4, 
                 num_heads: int = 4, max_length: int = 512):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.max_length = max_length
        
        # ì„ë² ë”©
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)
        self.position_embedding = nn.Embedding(max_length, hidden_size)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
        self.layers = nn.ModuleList([
            nn.ModuleDict({
                'attention': nn.MultiheadAttention(hidden_size, num_heads, batch_first=True),
                'attention_norm': nn.LayerNorm(hidden_size),
                'mlp': nn.Sequential(
                    nn.Linear(hidden_size, hidden_size * 4),
                    nn.GELU(),
                    nn.Linear(hidden_size * 4, hidden_size)
                ),
                'mlp_norm': nn.LayerNorm(hidden_size),
            })
            for _ in range(num_layers)
        ])
        
        # ì¶œë ¥ í—¤ë“œ
        self.output_norm = nn.LayerNorm(hidden_size)
        self.lm_head = nn.Linear(hidden_size, vocab_size)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, std=0.02)
    
    def forward(self, input_ids, attention_mask=None):
        """ìˆœì „íŒŒ"""
        batch_size, seq_len = input_ids.shape
        
        # ì„ë² ë”©
        token_emb = self.token_embedding(input_ids)
        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        pos_emb = self.position_embedding(pos_ids)
        
        hidden_states = token_emb + pos_emb
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„± (causal mask)
        if attention_mask is None:
            attention_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
            attention_mask = attention_mask.to(input_ids.device)
        
        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤
        for layer in self.layers:
            # ì…€í”„ ì–´í…ì…˜
            residual = hidden_states
            hidden_states = layer['attention_norm'](hidden_states)
            attn_out, _ = layer['attention'](
                hidden_states, hidden_states, hidden_states,
                attn_mask=attention_mask,
                need_weights=False
            )
            hidden_states = residual + attn_out
            
            # MLP
            residual = hidden_states
            hidden_states = layer['mlp_norm'](hidden_states)
            mlp_out = layer['mlp'](hidden_states)
            hidden_states = residual + mlp_out
        
        # ì¶œë ¥
        hidden_states = self.output_norm(hidden_states)
        logits = self.lm_head(hidden_states)
        
        return logits
    
    def generate(self, input_ids, max_new_tokens=50, temperature=0.8, top_p=0.9):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        self.eval()
        
        generated = input_ids.clone()
        
        with torch.no_grad():
            for _ in range(max_new_tokens):
                # í˜„ì¬ ì‹œí€€ìŠ¤ë¡œ ì˜ˆì¸¡
                logits = self.forward(generated)
                next_token_logits = logits[:, -1, :] / temperature
                
                # Top-p ìƒ˜í”Œë§
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                
                # top_p ì„ê³„ê°’ì„ ë„˜ëŠ” í† í°ë“¤ ì œê±°
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                next_token_logits[indices_to_remove] = float('-inf')
                
                # ìƒ˜í”Œë§
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # í† í° ì¶”ê°€
                generated = torch.cat([generated, next_token], dim=-1)
                
                # EOS í† í°ì´ë©´ ì¤‘ë‹¨
                if next_token.item() == 3:  # <eos>
                    break
        
        return generated


def apply_helgason_compression(model: KoreanGPT, compression_ratio: float = 0.1):
    """ëª¨ë¸ì— í—¬ê°€ì† ì••ì¶• ì ìš©"""
    
    print(f"\nğŸ”§ í•œêµ­ì–´ GPTì— í—¬ê°€ì† ì••ì¶• ì ìš© (ì••ì¶•ë¥ : {compression_ratio:.1%})")
    
    compressed_count = 0
    total_original_params = 0
    total_compressed_params = 0
    
    for layer_idx, layer in enumerate(model.layers):
        print(f"\nğŸ“ Layer {layer_idx} MLP ì••ì¶• ì¤‘...")
        
        try:
            # ì›ë³¸ MLP ì •ë³´
            original_mlp = layer['mlp']
            hidden_size = model.hidden_size
            intermediate_size = hidden_size * 4
            
            # ì›ë³¸ íŒŒë¼ë¯¸í„° ìˆ˜
            original_params = sum(p.numel() for p in original_mlp.parameters())
            
            # í—¬ê°€ì† MLPë¡œ êµì²´
            compressed_mlp = TrueHelgasonMLP(hidden_size, intermediate_size, compression_ratio)
            layer['mlp'] = compressed_mlp
            
            # ì••ì¶•ëœ íŒŒë¼ë¯¸í„° ìˆ˜
            compressed_params = sum(p.numel() for p in compressed_mlp.parameters())
            
            total_original_params += original_params
            total_compressed_params += compressed_params
            compressed_count += 1
            
            actual_ratio = compressed_params / original_params
            print(f"   âœ… Layer {layer_idx}: {original_params:,} â†’ {compressed_params:,} ({actual_ratio:.1%})")
            
        except Exception as e:
            print(f"   âŒ Layer {layer_idx} ì••ì¶• ì‹¤íŒ¨: {e}")
    
    overall_ratio = total_compressed_params / total_original_params if total_original_params > 0 else 1.0
    memory_saved = (total_original_params - total_compressed_params) * 4 / (1024**2)
    
    print(f"\nğŸ¯ ì••ì¶• ì™„ë£Œ:")
    print(f"   ì••ì¶•ëœ ë ˆì´ì–´: {compressed_count}ê°œ")
    print(f"   MLP ì••ì¶•ë¥ : {overall_ratio:.1%}")
    print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.1f}MB")
    
    return model, overall_ratio


def korean_quality_evaluation(original_text: str, compressed_text: str, tokenizer: KoreanTokenizer):
    """í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€"""
    
    # í•œê¸€ ë¬¸ì ë¹„ìœ¨
    def korean_ratio(text):
        korean_chars = sum(1 for c in text if 'ê°€' <= c <= 'í£')
        total_chars = len(text.replace(' ', ''))
        return korean_chars / total_chars if total_chars > 0 else 0
    
    # ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ ë¹„ìœ¨ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    def meaningful_ratio(text):
        # ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì œê±°
        clean_text = re.sub(r'(.)\1{2,}', r'\1', text)  # ê°™ì€ ë¬¸ì 3ë²ˆ ì´ìƒ ë°˜ë³µ ì œê±°
        meaningful_chars = len(clean_text)
        total_chars = len(text)
        return meaningful_chars / total_chars if total_chars > 0 else 0
    
    # ìëª¨ ë ˆë²¨ ìœ ì‚¬ë„
    original_jamos = tokenizer.encode(original_text)
    compressed_jamos = tokenizer.encode(compressed_text)
    
    # ìì¹´ë“œ ìœ ì‚¬ë„
    set1 = set(original_jamos)
    set2 = set(compressed_jamos)
    jaccard = len(set1 & set2) / len(set1 | set2) if len(set1 | set2) > 0 else 0
    
    return {
        'original_korean_ratio': korean_ratio(original_text),
        'compressed_korean_ratio': korean_ratio(compressed_text),
        'original_meaningful_ratio': meaningful_ratio(original_text),
        'compressed_meaningful_ratio': meaningful_ratio(compressed_text),
        'jamo_similarity': jaccard
    }


def korean_compression_experiment():
    """í•œêµ­ì–´ ì••ì¶• ì‹¤í—˜"""
    
    print("ğŸš€ í•œêµ­ì–´ ìµœì í™” ì••ì¶• ì‹¤í—˜ ì‹œì‘")
    print("=" * 80)
    
    # í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
    tokenizer = KoreanTokenizer()
    print(f"ğŸ“š í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ (ì–´íœ˜ í¬ê¸°: {tokenizer.vocab_size})")
    
    # í•œêµ­ì–´ GPT ëª¨ë¸ ìƒì„±
    model = KoreanGPT(
        vocab_size=tokenizer.vocab_size,
        hidden_size=128,  # ì‘ì€ ëª¨ë¸ë¡œ ì‹œì‘
        num_layers=4,
        num_heads=4,
        max_length=256
    )
    
    original_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ§  í•œêµ­ì–´ GPT ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    print(f"   íŒŒë¼ë¯¸í„° ìˆ˜: {original_params:,}")
    print(f"   ëª¨ë¸ í¬ê¸°: {original_params * 4 / (1024**2):.2f}MB")
    
    # í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    korean_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”",
        "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬",
        "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ",
        "ì„œìš¸ ì—¬í–‰"
    ]
    
    print(f"\nğŸ“ ì›ë³¸ ëª¨ë¸ í•œêµ­ì–´ ìƒì„± í…ŒìŠ¤íŠ¸")
    print("-" * 60)
    
    model.eval()
    original_results = []
    
    for i, prompt in enumerate(korean_prompts):
        try:
            # í† í°í™”
            input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long)
            
            # ìƒì„±
            generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0.7)
            generated_text = tokenizer.decode(generated_ids[0].tolist())
            
            original_results.append(generated_text)
            print(f"{i+1}. ì…ë ¥: {prompt}")
            print(f"   ì¶œë ¥: {generated_text}")
            print()
            
        except Exception as e:
            print(f"   ìƒì„± ì‹¤íŒ¨: {e}")
            original_results.append("")
    
    # ì›ë³¸ ëª¨ë¸ ì†ë„ ì¸¡ì •
    print(f"\nâ±ï¸ ì›ë³¸ ëª¨ë¸ ì†ë„ ì¸¡ì •")
    test_input = torch.tensor([tokenizer.encode("í…ŒìŠ¤íŠ¸")], dtype=torch.long)
    
    start_time = time.time()
    with torch.no_grad():
        for _ in range(20):
            _ = model(test_input)
    original_time = (time.time() - start_time) / 20
    print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {original_time*1000:.2f}ms")
    
    # ë‹¤ì–‘í•œ ì••ì¶•ë¥ ë¡œ í…ŒìŠ¤íŠ¸
    compression_ratios = [0.05, 0.1, 0.2, 0.3]
    
    results_summary = []
    
    for compression_ratio in compression_ratios:
        print(f"\nğŸ”§ ì••ì¶•ë¥  {compression_ratio:.1%} í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        try:
            # ëª¨ë¸ ë³µì‚¬
            import copy
            compressed_model = copy.deepcopy(model)
            
            # í—¬ê°€ì† ì••ì¶• ì ìš©
            compressed_model, actual_ratio = apply_helgason_compression(compressed_model, compression_ratio)
            
            # ì••ì¶•ëœ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •
            print(f"\nğŸ“ ì••ì¶•ëœ ëª¨ë¸ í•œêµ­ì–´ ìƒì„± í…ŒìŠ¤íŠ¸")
            print("-" * 50)
            
            compressed_model.eval()
            compressed_results = []
            quality_scores = []
            
            for i, prompt in enumerate(korean_prompts):
                try:
                    # í† í°í™”
                    input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long)
                    
                    # ìƒì„±
                    generated_ids = compressed_model.generate(input_ids, max_new_tokens=20, temperature=0.7)
                    generated_text = tokenizer.decode(generated_ids[0].tolist())
                    
                    compressed_results.append(generated_text)
                    
                    # í’ˆì§ˆ í‰ê°€
                    if i < len(original_results) and original_results[i]:
                        quality = korean_quality_evaluation(original_results[i], generated_text, tokenizer)
                        quality_scores.append(quality)
                        
                        print(f"{i+1}. ì…ë ¥: {prompt}")
                        print(f"   ì›ë³¸: {original_results[i]}")
                        print(f"   ì••ì¶•: {generated_text}")
                        print(f"   í•œê¸€ ë¹„ìœ¨: {quality['compressed_korean_ratio']:.2f}")
                        print(f"   ìëª¨ ìœ ì‚¬ë„: {quality['jamo_similarity']:.3f}")
                        print()
                    
                except Exception as e:
                    print(f"   ìƒì„± ì‹¤íŒ¨: {e}")
                    compressed_results.append("")
            
            # ì†ë„ ì¸¡ì •
            start_time = time.time()
            with torch.no_grad():
                for _ in range(20):
                    _ = compressed_model(test_input)
            compressed_time = (time.time() - start_time) / 20
            
            # ê²°ê³¼ ìš”ì•½
            avg_korean_ratio = np.mean([q['compressed_korean_ratio'] for q in quality_scores]) if quality_scores else 0
            avg_jamo_similarity = np.mean([q['jamo_similarity'] for q in quality_scores]) if quality_scores else 0
            speed_improvement = original_time / compressed_time if compressed_time > 0 else 1.0
            
            compressed_params = sum(p.numel() for p in compressed_model.parameters())
            real_compression_ratio = compressed_params / original_params
            memory_saved = (original_params - compressed_params) * 4 / (1024**2)
            
            result = {
                'compression_ratio': compression_ratio,
                'actual_compression_ratio': real_compression_ratio,
                'korean_ratio': avg_korean_ratio,
                'jamo_similarity': avg_jamo_similarity,
                'speed_improvement': speed_improvement,
                'memory_saved_mb': memory_saved,
                'inference_time_ms': compressed_time * 1000
            }
            
            results_summary.append(result)
            
            print(f"\nğŸ“Š ì••ì¶•ë¥  {compression_ratio:.1%} ê²°ê³¼ ìš”ì•½:")
            print(f"   ì‹¤ì œ ì••ì¶•ë¥ : {real_compression_ratio:.1%}")
            print(f"   í‰ê·  í•œê¸€ ë¹„ìœ¨: {avg_korean_ratio:.3f}")
            print(f"   í‰ê·  ìëª¨ ìœ ì‚¬ë„: {avg_jamo_similarity:.3f}")
            print(f"   ì¶”ë¡  ì‹œê°„: {compressed_time*1000:.2f}ms")
            print(f"   ì†ë„ í–¥ìƒ: {speed_improvement:.2f}x")
            print(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_saved:.2f}MB")
            
        except Exception as e:
            print(f"   âŒ ì••ì¶• ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ‰ í•œêµ­ì–´ ì••ì¶• ì‹¤í—˜ ìµœì¢… ê²°ê³¼")
    print("=" * 80)
    
    if results_summary:
        print(f"{'ì••ì¶•ë¥ ':<8} {'ì‹¤ì œì••ì¶•ë¥ ':<10} {'í•œê¸€ë¹„ìœ¨':<8} {'ìëª¨ìœ ì‚¬ë„':<10} {'ì†ë„í–¥ìƒ':<8} {'ë©”ëª¨ë¦¬ì ˆì•½':<10}")
        print("-" * 70)
        
                        for result in results_summary:            print(f"{result['compression_ratio']:.1%:<8} {result['actual_compression_ratio']:.1%:<10} "                  f"{result['korean_ratio']:.3f:<8} {result['jamo_similarity']:.3f:<10} "                  f"{result['speed_improvement']:.2f}x:<8} {result['memory_saved_mb']:.1f}MB:<10}")
        
        # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
        best_compression = min(results_summary, key=lambda x: x['actual_compression_ratio'])
        best_quality = max(results_summary, key=lambda x: x['jamo_similarity'])
        best_speed = max(results_summary, key=lambda x: x['speed_improvement'])
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥:")
        print(f"   ìµœê³  ì••ì¶•: {best_compression['compression_ratio']:.1%} "
              f"({best_compression['actual_compression_ratio']:.1%} ì‹¤ì œ)")
        print(f"   ìµœê³  í’ˆì§ˆ: {best_quality['compression_ratio']:.1%} "
              f"(ìëª¨ ìœ ì‚¬ë„ {best_quality['jamo_similarity']:.3f})")
        print(f"   ìµœê³  ì†ë„: {best_speed['compression_ratio']:.1%} "
              f"({best_speed['speed_improvement']:.2f}x í–¥ìƒ)")
    
    return results_summary


if __name__ == "__main__":
    try:
        results = korean_compression_experiment()
        print(f"\nâœ… ì‹¤í—˜ ì™„ë£Œ! ê²°ê³¼ ìˆ˜: {len(results)}ê°œ")
    except Exception as e:
        print(f"ì‹¤í—˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc() 