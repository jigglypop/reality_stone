import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import traceback
import time  # ì¶”ë¡  ì†ë„ ì¸¡ì •ìš© ì¶”ê°€
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass


@dataclass
class AccuracyFirstStats:
    """ì •í™•ë„ ìµœìš°ì„  ì••ì¶• í†µê³„"""
    original_size_mb: float
    compressed_size_mb: float
    compression_ratio: float
    accuracy_preserved: float
    fused_layers: List[str]
    energy_preserved: float
    svd_rank: int
    compression_method: str


class AccuracyFirstCompressor:
    def __init__(self, 
                 min_accuracy: float = 0.95,  # ìµœì†Œ 95% ì •í™•ë„
                 energy_threshold: float = 0.99):  # 99% ì—ë„ˆì§€ ë³´ì¡´
        
        self.min_accuracy = min_accuracy
        self.energy_threshold = energy_threshold
        
    def accuracy_first_compress(self, model: nn.Module, layer_names: List[str]) -> Dict:
        """ì •í™•ë„ ìµœìš°ì„  ì••ì¶•"""
        
        print(f"ğŸ¯ ì •í™•ë„ ìµœìš°ì„  ì••ì¶•: {layer_names}")
        print(f"   ìµœì†Œ ì •í™•ë„: {100*self.min_accuracy:.1f}%")
        print(f"   ì—ë„ˆì§€ ë³´ì¡´: {100*self.energy_threshold:.1f}%")
        
        # 1ë‹¨ê³„: ìˆ˜í•™ì ìœ¼ë¡œ ì •í™•í•œ ë“±ê°€ ê°€ì¤‘ì¹˜ ê³„ì‚°
        equivalent_weight, equivalent_bias = self._compute_exact_equivalent(model, layer_names)
        print(f"   ë“±ê°€ ê°€ì¤‘ì¹˜: {equivalent_weight.shape}")
        
        # 2ë‹¨ê³„: ê³ ì •ë°€ë„ SVD ë¶„í•´
        svd_result = self._high_precision_svd(equivalent_weight)
        
        # 3ë‹¨ê³„: ì—ë„ˆì§€ ê¸°ë°˜ ë­í¬ ì„ íƒ (99% ì´ìƒ ë³´ì¡´)
        optimal_rank = self._find_optimal_rank(svd_result, equivalent_weight)
        
        # 4ë‹¨ê³„: ì •í™•ë„ ê²€ì¦ ë° ì¡°ì •
        final_accuracy = self._verify_and_adjust_accuracy(
            equivalent_weight, svd_result, optimal_rank
        )
        
        result = {
            'type': 'accuracy_first_svd',
            'svd_components': {
                'U': svd_result['U'][:, :optimal_rank],
                'S': svd_result['S'][:optimal_rank],
                'V': svd_result['V'][:, :optimal_rank]
            },
            'svd_rank': optimal_rank,
            'original_shape': equivalent_weight.shape,
            'original_bias': equivalent_bias,
            'layer_names': layer_names,
            'accuracy': final_accuracy,
            'energy_preserved': self._calculate_energy_ratio(svd_result, optimal_rank)
        }
        
        return result
    
    def _compute_exact_equivalent(self, model: nn.Module, layer_names: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:
        """ìˆ˜í•™ì ìœ¼ë¡œ ì •í™•í•œ ë“±ê°€ ë ˆì´ì–´ ê³„ì‚°"""
        
        weights = []
        biases = []
        
        for name in layer_names:
            if hasattr(model, name):
                layer = getattr(model, name)
                if isinstance(layer, nn.Linear):
                    weights.append(layer.weight.data.clone())
                    biases.append(layer.bias.data.clone() if layer.bias is not None else None)
        
        if len(weights) == 0:
            raise ValueError("No linear layers found")
        
        # ì •í™•í•œ ì²´ì¸ ê³±ì…ˆ
        equivalent_weight = weights[0]
        for i in range(1, len(weights)):
            equivalent_weight = weights[i] @ equivalent_weight
        
        final_bias = biases[-1] if biases and biases[-1] is not None else None
        
        print(f"   ìˆ˜í•™ì  ë“±ê°€: {equivalent_weight.shape[1]} â†’ {equivalent_weight.shape[0]}")
        
        return equivalent_weight, final_bias
    
    def _high_precision_svd(self, matrix: torch.Tensor) -> Dict:
        """ê³ ì •ë°€ë„ SVD ë¶„í•´"""
        
        print(f"   ê³ ì •ë°€ë„ SVD ë¶„í•´...")
        
        try:
            # double precisionìœ¼ë¡œ SVD
            U, S, V = torch.svd(matrix.double())
            U, S, V = U.float(), S.float(), V.float()
            
            print(f"   SVD ì„±ê³µ: rank {len(S)}")
            
            return {
                'U': U,
                'S': S, 
                'V': V,
                'original_energy': torch.sum(S**2)
            }
            
        except Exception as e:
            print(f"   SVD ì‹¤íŒ¨: {e}")
            # fallback: ë‹¨ìœ„ í–‰ë ¬
            min_dim = min(matrix.shape)
            return {
                'U': torch.eye(matrix.shape[0]),
                'S': torch.ones(min_dim),
                'V': torch.eye(matrix.shape[1]),
                'original_energy': torch.sum(matrix**2)
            }
    
    def _find_optimal_rank(self, svd_result: Dict, original_matrix: torch.Tensor) -> int:
        """ìµœì  ë­í¬ ì°¾ê¸° (ì—ë„ˆì§€ 99% ë³´ì¡´)"""
        
        S = svd_result['S']
        total_energy = svd_result['original_energy']
        
        cumulative_energy = torch.cumsum(S**2, dim=0)
        energy_ratios = cumulative_energy / total_energy
        
        # 99% ì—ë„ˆì§€ ë³´ì¡´í•˜ëŠ” ìµœì†Œ ë­í¬
        optimal_rank = torch.sum(energy_ratios < self.energy_threshold).item() + 1
        
        # ìµœì†Œ 10ê°œëŠ” ë³´ì¡´ (ë„ˆë¬´ ê³µê²©ì  ì••ì¶• ë°©ì§€)
        optimal_rank = max(optimal_rank, 10)
        optimal_rank = min(optimal_rank, len(S))
        
        energy_preserved = energy_ratios[optimal_rank-1].item()
        
        print(f"   ìµœì  ë­í¬: {optimal_rank}/{len(S)} (ì—ë„ˆì§€ {100*energy_preserved:.2f}%)")
        
        return optimal_rank
    
    def _verify_and_adjust_accuracy(self, original_matrix: torch.Tensor, 
                                   svd_result: Dict, initial_rank: int) -> float:
        """ì •í™•ë„ ê²€ì¦ ë° ì¡°ì •"""
        
        print(f"   ì •í™•ë„ ê²€ì¦...")
        
        best_accuracy = 0.0
        best_rank = initial_rank
        
        # ì´ˆê¸° ë­í¬ë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
        for rank in range(initial_rank, min(initial_rank + 20, len(svd_result['S']))):
            
            # í˜„ì¬ ë­í¬ë¡œ ë³µì›
            reconstructed = self._reconstruct_from_svd(svd_result, rank)
            accuracy = self._calculate_precise_accuracy(original_matrix, reconstructed)
            
            print(f"   ë­í¬ {rank}: ì •í™•ë„ {100*accuracy:.2f}%")
            
            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_rank = rank
            
            # ëª©í‘œ ì •í™•ë„ ë‹¬ì„±í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ
            if accuracy >= self.min_accuracy:
                print(f"   ëª©í‘œ ë‹¬ì„±! ë­í¬ {rank}, ì •í™•ë„ {100*accuracy:.2f}%")
                return accuracy
            
            # ì •í™•ë„ê°€ ê°ì†Œí•˜ê¸° ì‹œì‘í•˜ë©´ ì¤‘ë‹¨
            if rank > initial_rank + 5 and accuracy < best_accuracy - 0.01:
                break
        
        print(f"   ìµœê³  ì •í™•ë„: {100*best_accuracy:.2f}% (ë­í¬ {best_rank})")
        
        return best_accuracy
    
    def _reconstruct_from_svd(self, svd_result: Dict, rank: int) -> torch.Tensor:
        """SVDì—ì„œ í–‰ë ¬ ë³µì›"""
        
        U = svd_result['U'][:, :rank]
        S = svd_result['S'][:rank]  
        V = svd_result['V'][:, :rank]
        
        reconstructed = U @ torch.diag(S) @ V.T
        
        return reconstructed
    
    def _calculate_precise_accuracy(self, original: torch.Tensor, reconstructed: torch.Tensor) -> float:
        """ì •ë°€í•œ ì •í™•ë„ ê³„ì‚°"""
        
        try:
            if original.shape != reconstructed.shape:
                return 0.0
            
            orig_flat = original.flatten()
            recon_flat = reconstructed.flatten()
            
            # 1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (50%)
            cos_sim = F.cosine_similarity(orig_flat, recon_flat, dim=0).item()
            
            # 2. í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (30%)
            if torch.std(orig_flat) > 1e-8 and torch.std(recon_flat) > 1e-8:
                corr = torch.corrcoef(torch.stack([orig_flat, recon_flat]))[0, 1].item()
                if torch.isnan(torch.tensor(corr)):
                    corr = cos_sim
            else:
                corr = cos_sim
            
            # 3. ì •ê·œí™”ëœ MSE (20%)
            mse = F.mse_loss(orig_flat, recon_flat)
            var_orig = torch.var(orig_flat)
            if var_orig > 1e-8:
                normalized_mse = mse / var_orig
                mse_accuracy = torch.exp(-normalized_mse).item()
            else:
                mse_accuracy = 1.0 if mse < 1e-8 else 0.0
            
            # ì¢…í•© ì •í™•ë„
            accuracy = 0.5 * cos_sim + 0.3 * corr + 0.2 * mse_accuracy
            
            return max(0.0, min(1.0, accuracy))
            
        except Exception as e:
            print(f"   ì •í™•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_energy_ratio(self, svd_result: Dict, rank: int) -> float:
        """ì—ë„ˆì§€ ë³´ì¡´ë¥  ê³„ì‚°"""
        
        S = svd_result['S']
        total_energy = svd_result['original_energy']
        preserved_energy = torch.sum(S[:rank]**2)
        
        return (preserved_energy / total_energy).item()


class AccuracyFirstLayer(nn.Module):
    """ì •í™•ë„ ìµœìš°ì„  ì••ì¶• ë ˆì´ì–´"""
    
    def __init__(self, compressed_data: Dict):
        super().__init__()
        
        # SVD ì„±ë¶„ì—ì„œ ê°€ì¤‘ì¹˜ ë³µì›
        svd_components = compressed_data['svd_components']
        reconstructed_weight = (svd_components['U'] @ 
                              torch.diag(svd_components['S']) @ 
                              svd_components['V'].T)
        
        self.weight = nn.Parameter(reconstructed_weight)
        
        if compressed_data['original_bias'] is not None:
            self.bias = nn.Parameter(compressed_data['original_bias'])
        else:
            self.register_parameter('bias', None)
        
        print(f"   ì •í™•ë„ ìµœìš°ì„  ë³µì›: {self.weight.shape}")
        print(f"   SVD ë­í¬: {compressed_data['svd_rank']}")
        print(f"   ì •í™•ë„: {100*compressed_data['accuracy']:.2f}%")
        print(f"   ì—ë„ˆì§€ ë³´ì¡´: {100*compressed_data['energy_preserved']:.1f}%")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.linear(x, self.weight, self.bias)


def accuracy_first_compress(model: nn.Module, 
                          layer_names: List[str],
                          min_accuracy: float = 0.95,
                          test_input: Optional[torch.Tensor] = None) -> Tuple[nn.Module, AccuracyFirstStats]:
    """ì •í™•ë„ ìµœìš°ì„  ì••ì¶• (ì •í™•ë„ 95%+ ëª©í‘œ)"""
    
    print("ğŸ¯ ì •í™•ë„ ìµœìš°ì„  ì••ì¶• (95%+ ì •í™•ë„)")
    print("=" * 60)
    
    compressor = AccuracyFirstCompressor(min_accuracy=min_accuracy)
    
    # ì›ë³¸ ì •ë³´
    original_params = sum(p.numel() for p in model.parameters())
    original_size_mb = original_params * 4 / (1024**2)
    
    print(f"ì›ë³¸: {original_params:,} íŒŒë¼ë¯¸í„° ({original_size_mb:.2f}MB)")
    print(f"ëª©í‘œ: ì •í™•ë„ {min_accuracy:.0%}+ (ì •í™•ë„ ìµœìš°ì„ )")
    
    # ì›ë³¸ ì¶œë ¥
    original_output = None
    if test_input is not None:
        with torch.no_grad():
            model.eval()
            original_output = model(test_input)
    
    # ì •í™•ë„ ìµœìš°ì„  ì••ì¶•
    compressed_data = compressor.accuracy_first_compress(model, layer_names)
    
    # ì••ì¶•ëœ ë ˆì´ì–´ ìƒì„±
    accuracy_first_layer = AccuracyFirstLayer(compressed_data)
    
    # ìƒˆ ëª¨ë¸ êµ¬ì„±
    class AccuracyFirstModel(nn.Module):
        def __init__(self, layer):
            super().__init__()
            self.accuracy_layer = layer
        
        def forward(self, x):
            return self.accuracy_layer(x)
    
    compressed_model = AccuracyFirstModel(accuracy_first_layer)
    
    # ì••ì¶• í†µê³„
    compressed_params = accuracy_first_layer.weight.numel()
    if accuracy_first_layer.bias is not None:
        compressed_params += accuracy_first_layer.bias.numel()
    
    # ì›ë˜ ìœµí•© ëŒ€ìƒ íŒŒë¼ë¯¸í„° ìˆ˜
    fusion_params = 0
    for name in layer_names:
        if hasattr(model, name):
            layer = getattr(model, name)
            if isinstance(layer, nn.Linear):
                fusion_params += layer.weight.numel()
                if layer.bias is not None:
                    fusion_params += layer.bias.numel()
    
    compressed_size_mb = compressed_params * 4 / (1024**2)
    compression_ratio = compressed_params / fusion_params
    
    # ì „ì²´ ëª¨ë¸ ì •í™•ë„
    accuracy_preserved = 0.0
    
    if test_input is not None and original_output is not None:
        with torch.no_grad():
            compressed_model.eval()
            try:
                compressed_output = compressed_model(test_input)
                
                if compressed_output.shape == original_output.shape:
                    accuracy_preserved = compressor._calculate_precise_accuracy(
                        original_output, compressed_output
                    )
                    
            except Exception as e:
                print(f"   ëª¨ë¸ ì •í™•ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                accuracy_preserved = 0.0
    
    stats = AccuracyFirstStats(
        original_size_mb=original_size_mb,
        compressed_size_mb=compressed_size_mb,
        compression_ratio=compression_ratio,
        accuracy_preserved=accuracy_preserved,
        fused_layers=layer_names,
        energy_preserved=compressed_data['energy_preserved'],
        svd_rank=compressed_data['svd_rank'],
        compression_method="ì •í™•ë„ ìµœìš°ì„  SVD (99% ì—ë„ˆì§€ ë³´ì¡´)"
    )
    
    print(f"\nâœ… ì •í™•ë„ ìµœìš°ì„  ì••ì¶• ì™„ë£Œ!")
    print(f"ìœµí•© íŒŒë¼ë¯¸í„°: {fusion_params:,} â†’ {compressed_params:,}")
    print(f"í¬ê¸°: {fusion_params * 4 / (1024**2):.2f}MB â†’ {compressed_size_mb:.2f}MB")
    print(f"ì••ì¶•ë¥ : {compression_ratio:.3f} ({100*compression_ratio:.1f}%)")
    print(f"ì •í™•ë„: {100*accuracy_preserved:.2f}%")
    print(f"ì—ë„ˆì§€ ë³´ì¡´: {100*compressed_data['energy_preserved']:.1f}%")
    
    return compressed_model, stats


def test_accuracy_first():
    """ì •í™•ë„ ìµœìš°ì„  ì••ì¶• í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ¯ ì •í™•ë„ ìµœìš°ì„  ì••ì¶• í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ìˆœìˆ˜ ì„ í˜• í…ŒìŠ¤íŠ¸ ëª¨ë¸
    class PureLinearModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(128, 256)
            self.fc2 = nn.Linear(256, 128) 
            self.fc3 = nn.Linear(128, 64)
            self.fc4 = nn.Linear(64, 32)
        
        def forward(self, x):
            x = self.fc1(x)
            x = self.fc2(x)
            x = self.fc3(x)
            return self.fc4(x)
    
    model = PureLinearModel()
    test_input = torch.randn(16, 128)
    
    # ì›ë³¸ í…ŒìŠ¤íŠ¸
    print("ì›ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    with torch.no_grad():
        original_output = model(test_input)
        print(f"ì›ë³¸ ì¶œë ¥: {original_output.shape}")
        print(f"ì›ë³¸ ë²”ìœ„: [{original_output.min():.4f}, {original_output.max():.4f}]")
        print(f"ì›ë³¸ í‰ê· : {original_output.mean():.4f}")
    
    # ì •í™•ë„ ìµœìš°ì„  ì••ì¶•
    layer_names = ['fc1', 'fc2', 'fc3', 'fc4']
    compressed_model, stats = accuracy_first_compress(
        model, 
        layer_names,
        min_accuracy=0.95,  # 95% ì •í™•ë„ ëª©í‘œ
        test_input=test_input
    )
    
    # ì••ì¶•ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    print("\nì••ì¶•ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    with torch.no_grad():
        try:
            compressed_output = compressed_model(test_input)
            print(f"ì••ì¶• ì¶œë ¥: {compressed_output.shape}")
            print(f"ì••ì¶• ë²”ìœ„: [{compressed_output.min():.4f}, {compressed_output.max():.4f}]")
            print(f"ì••ì¶• í‰ê· : {compressed_output.mean():.4f}")
            
            # ì •ë°€ ë¶„ì„
            if compressed_output.shape == original_output.shape:
                diff = torch.abs(original_output - compressed_output)
                rel_error = diff / (torch.abs(original_output) + 1e-8)
                
                print(f"\nğŸ“Š ì •í™•ë„ ìµœìš°ì„  ë¶„ì„:")
                print(f"   ìµœëŒ€ ì°¨ì´: {diff.max():.8f}")
                print(f"   í‰ê·  ì°¨ì´: {diff.mean():.8f}")
                print(f"   ìµœëŒ€ ìƒëŒ€ì˜¤ì°¨: {rel_error.max():.6f}")
                print(f"   í‰ê·  ìƒëŒ€ì˜¤ì°¨: {rel_error.mean():.6f}")
                
                cos_sim = F.cosine_similarity(original_output.flatten(), compressed_output.flatten(), dim=0)
                print(f"   ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {cos_sim:.8f}")
                
                # ìˆ˜í•™ì  ê²€ì¦
                print(f"\nğŸ”¬ ìˆ˜í•™ì  ê²€ì¦:")
                with torch.no_grad():
                    manual_output = test_input
                    for layer_name in layer_names:
                        layer = getattr(model, layer_name)
                        manual_output = layer(manual_output)
                    
                    manual_vs_original = torch.allclose(manual_output, original_output, atol=1e-6)
                    print(f"   ìˆ˜ë™ ê³„ì‚° == ì›ë³¸: {manual_vs_original}")
                    
                    manual_vs_compressed = F.cosine_similarity(manual_output.flatten(), compressed_output.flatten(), dim=0)
                    print(f"   ìˆ˜ë™ vs ì••ì¶• ìœ ì‚¬ë„: {manual_vs_compressed:.8f}")
                
        except Exception as e:
            print(f"ì••ì¶• ëª¨ë¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
    
    # ğŸš€ ì¶”ë¡  ì†ë„ ë¹„êµ í…ŒìŠ¤íŠ¸
    print("\nğŸš€ ì¶”ë¡  ì†ë„ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    def benchmark_model(model, input_data, model_name, warmup_runs=10, test_runs=100):
        """ëª¨ë¸ ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        
        model.eval()
        
        # GPUê°€ ìˆìœ¼ë©´ GPUë¡œ ì´ë™
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        input_data = input_data.to(device)
        
        print(f"ğŸ“Š {model_name} ë²¤ì¹˜ë§ˆí¬ (device: {device})")
        
        # Warmup ì‹¤í–‰ (GPU ìºì‹œ ì¤€ë¹„)
        with torch.no_grad():
            for _ in range(warmup_runs):
                _ = model(input_data)
        
        # ì‹¤ì œ ì¸¡ì •
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        
        start_time = time.time()
        with torch.no_grad():
            for _ in range(test_runs):
                output = model(input_data)
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        end_time = time.time()
        
        total_time = end_time - start_time
        avg_time = total_time / test_runs
        fps = 1.0 / avg_time
        
        print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time*1000:.3f}ms")
        print(f"   ì²˜ë¦¬ëŸ‰: {fps:.1f} FPS")
        print(f"   ì´ ì‹œê°„ ({test_runs}íšŒ): {total_time:.3f}s")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GPUì—ì„œë§Œ)
        if torch.cuda.is_available():
            memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB
            print(f"   GPU ë©”ëª¨ë¦¬: {memory_used:.1f}MB")
            torch.cuda.reset_peak_memory_stats()
        
        return avg_time, fps
    
    # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
    batch_sizes = [1, 8, 16, 32]
    speed_results = {}
    
    for batch_size in batch_sizes:
        print(f"\nğŸ” ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸:")
        test_batch = torch.randn(batch_size, 128)
        
        # ì›ë³¸ ëª¨ë¸ ì†ë„
        original_time, original_fps = benchmark_model(
            model, test_batch, f"ì›ë³¸ ëª¨ë¸ (batch={batch_size})"
        )
        
        # ì••ì¶• ëª¨ë¸ ì†ë„  
        compressed_time, compressed_fps = benchmark_model(
            compressed_model, test_batch, f"ì••ì¶• ëª¨ë¸ (batch={batch_size})"
        )
        
        # ì†ë„ ê°œì„ ë¥  ê³„ì‚°
        speedup = original_time / compressed_time
        throughput_gain = compressed_fps / original_fps
        
        print(f"   âš¡ ì†ë„ ê°œì„ : {speedup:.2f}x ë¹ ë¦„")
        print(f"   ğŸ“ˆ ì²˜ë¦¬ëŸ‰ ì¦ê°€: {throughput_gain:.2f}x")
        
        speed_results[batch_size] = {
            'original_time': original_time,
            'compressed_time': compressed_time,
            'speedup': speedup,
            'throughput_gain': throughput_gain
        }
    
    # ì¢…í•© ì†ë„ ë¶„ì„
    print(f"\nğŸ“Š ì¢…í•© ì†ë„ ë¶„ì„:")
    avg_speedup = np.mean([result['speedup'] for result in speed_results.values()])
    avg_throughput_gain = np.mean([result['throughput_gain'] for result in speed_results.values()])
    
    print(f"   í‰ê·  ì†ë„ ê°œì„ : {avg_speedup:.2f}x")
    print(f"   í‰ê·  ì²˜ë¦¬ëŸ‰ ì¦ê°€: {avg_throughput_gain:.2f}x")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œì™€ ì†ë„ ê°œì„  ë¹„êµ
    param_reduction = (1 - stats.compression_ratio) * 100
    print(f"   íŒŒë¼ë¯¸í„° ê°ì†Œ: {param_reduction:.1f}%")
    print(f"   ì†ë„ ê°œì„ : {(avg_speedup-1)*100:.1f}%")
    
    if avg_speedup > 1.0:
        print("   âœ… ì••ì¶•ìœ¼ë¡œ ì¸í•œ ì†ë„ í–¥ìƒ í™•ì¸!")
    else:
        print("   âš ï¸ ì••ì¶• í›„ ì†ë„ ì €í•˜ ë°œìƒ")
    
    print(f"\nğŸ“Š ìµœì¢… ì •í™•ë„ ìµœìš°ì„  ê²°ê³¼:")
    print(f"   ì••ì¶•ë¥ : {100*stats.compression_ratio:.1f}%")
    print(f"   ì •í™•ë„: {100*stats.accuracy_preserved:.3f}%")
    print(f"   ì—ë„ˆì§€ ë³´ì¡´: {100*stats.energy_preserved:.1f}%")
    print(f"   SVD ë­í¬: {stats.svd_rank}")
    print(f"   ë°©ë²•: {stats.compression_method}")
    
    # ì„±ê³µ ê¸°ì¤€: ì •í™•ë„ 95%+
    success = stats.accuracy_preserved >= 0.95
    
    if success:
        print("âœ… ì •í™•ë„ ìµœìš°ì„  ì••ì¶• ì„±ê³µ!")
        print(f"   ëª©í‘œ ë‹¬ì„±: ì •í™•ë„ {100*stats.accuracy_preserved:.2f}% â‰¥ 95%")
        return True
    else:
        print("âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±")
        print(f"   ì •í™•ë„: {100*stats.accuracy_preserved:.2f}% < 95%")
        return False


if __name__ == "__main__":
    try:
        success = test_accuracy_first()
        if success:
            print("\nğŸ‰ ì •í™•ë„ ìµœìš°ì„  ì••ì¶• ì™„ë£Œ!")
        else:
            print("\nâš ï¸ ì¶”ê°€ ê°œì„  í•„ìš”")
    except Exception as e:
        print(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(traceback.format_exc()) 