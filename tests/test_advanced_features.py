#!/usr/bin/env python3
"""
Reality Stone Advanced Features Test Suite
ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì˜ ì „ì²´ í…ŒìŠ¤íŠ¸
"""

import torch
import reality_stone
import numpy as np
import time
import traceback
import pytest
import sys
import os

def print_test_header(test_name):
    print(f"\n{'='*60}")
    print(f"ğŸ§ª {test_name}")
    print(f"{'='*60}")

class TestAdvancedFeatures:
    """ê³ ê¸‰ ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture(autouse=True)
    def setup_method(self, method):
        """ê° í…ŒìŠ¤íŠ¸ ì „ì— ì‹¤í–‰"""
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”§ Device: {self.device}")
        
    def test_basic_import(self):
        """ê¸°ë³¸ import ë° ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ í™•ì¸"""
        print_test_header("Basic Import & Function List")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤ ì¶œë ¥
        funcs = [attr for attr in dir(reality_stone._C) if not attr.startswith('_')]
        print(f"âœ… Reality Stone loaded successfully")
        print(f"ğŸ“‹ Available functions ({len(funcs)}):")
        for i, func in enumerate(sorted(funcs), 1):
            print(f"  {i:2d}. {func}")
        
        # ê³ ê¸‰ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
        expected_advanced = [
            'fused_linear', 'fused_mobius_chain', 'fused_transform_reg',
            'dynamic_curvature_pred', 'dynamic_mobius_add', 'dynamic_poincare_layer',
            'boundary_penalty', 'curvature_penalty', 'geodesic_penalty', 'combined_reg',
            'einstein_midpoint', 'multi_geodesic', 'geodesic_activation'
        ]
        
        missing = []
        for func in expected_advanced:
            if func not in funcs:
                missing.append(func)
        
        if missing:
            print(f"âš ï¸  Missing advanced functions: {missing}")
        else:
            print(f"âœ… All {len(expected_advanced)} advanced functions available!")
            
        assert len(funcs) >= 42, f"Expected at least 42 functions, got {len(funcs)}"
        
    def test_fused_operations(self):
        """Fused Operations í…ŒìŠ¤íŠ¸"""
        print_test_header("Fused Operations")
        
        batch_size, input_dim, output_dim = 32, 64, 128
        x = torch.randn(batch_size, input_dim, device=self.device) * 0.3
        weight = torch.randn(output_dim, input_dim, device=self.device) * 0.1
        bias = torch.randn(output_dim, device=self.device) * 0.1
        
        # 1. Fused Linear
        print("\n1ï¸âƒ£ Testing fused_linear...")
        if hasattr(reality_stone._C, 'fused_linear'):
            result = reality_stone._C.fused_linear(x, weight, bias, 1.0)
            print(f"   âœ… Input: {x.shape} -> Output: {result.shape}")
            assert result.shape == (batch_size, output_dim)
            assert not torch.isnan(result).any(), "NaN values detected in fused_linear!"
        else:
            print(f"   âŒ fused_linear not available")
            pytest.skip("fused_linear not implemented")
        
        # 2. Fused Mobius Chain
        print("\n2ï¸âƒ£ Testing fused_mobius_chain...")
        if hasattr(reality_stone._C, 'fused_mobius_chain'):
            inputs = [torch.randn(batch_size, input_dim, device=self.device) * 0.2 for _ in range(3)]
            curvatures = [1.0, 0.5, 2.0]
            chain_result = reality_stone._C.fused_mobius_chain(inputs, curvatures)
            print(f"   âœ… Chain of {len(inputs)} tensors -> Output: {chain_result.shape}")
            assert chain_result.shape == (batch_size, input_dim)
            assert not torch.isnan(chain_result).any(), "NaN values in chain result!"
        else:
            print(f"   âŒ fused_mobius_chain not available")
            pytest.skip("fused_mobius_chain not implemented")
        
        # 3. Transform + Regularize
        print("\n3ï¸âƒ£ Testing fused_transform_reg...")
        if hasattr(reality_stone._C, 'fused_transform_reg'):
            transformed, reg_loss = reality_stone._C.fused_transform_reg(x, 1.0, 0.1)
            print(f"   âœ… Transformed: {transformed.shape}, Reg loss: {reg_loss.item():.4f}")
            assert transformed.shape == x.shape
            assert reg_loss.item() >= 0, "Regularization loss should be non-negative!"
            assert not torch.isnan(transformed).any(), "NaN in transformed output!"
        else:
            print(f"   âŒ fused_transform_reg not available")
            pytest.skip("fused_transform_reg not implemented")
        
    def test_dynamic_curvature(self):
        """Dynamic Curvature í…ŒìŠ¤íŠ¸"""
        print_test_header("Dynamic Curvature")
        
        batch_size, dim = 32, 64
        
        # 1. Dynamic Curvature Prediction
        print("\n1ï¸âƒ£ Testing dynamic_curvature_pred...")
        if hasattr(reality_stone._C, 'dynamic_curvature_pred'):
            features = torch.randn(batch_size, 1, device=self.device)
            weight = torch.randn(1, 1, device=self.device)
            bias = torch.randn(1, device=self.device)
            curvatures = reality_stone._C.dynamic_curvature_pred(features, weight, bias, 1.0)
            print(f"   âœ… Features: {features.shape} -> Curvatures: {curvatures.shape}")
            print(f"   ğŸ“Š Curvature range: [{curvatures.min():.3f}, {curvatures.max():.3f}]")
            assert curvatures.shape == (batch_size,)
            assert (curvatures > 0).all(), "Curvatures should be positive!"
        else:
            print(f"   âŒ dynamic_curvature_pred not available")
            pytest.skip("dynamic_curvature_pred not implemented")
        
        # 2. Dynamic Mobius Add
        print("\n2ï¸âƒ£ Testing dynamic_mobius_add...")
        if hasattr(reality_stone._C, 'dynamic_mobius_add'):
            u = torch.randn(batch_size, dim, device=self.device) * 0.3
            v = torch.randn(batch_size, dim, device=self.device) * 0.3
            curvatures = torch.ones(batch_size, device=self.device) * 1.0
            dynamic_result = reality_stone._C.dynamic_mobius_add(u, v, curvatures)
            print(f"   âœ… Dynamic Mobius: {u.shape} + {v.shape} -> {dynamic_result.shape}")
            assert dynamic_result.shape == u.shape
            assert not torch.isnan(dynamic_result).any(), "NaN in dynamic mobius result!"
        else:
            print(f"   âŒ dynamic_mobius_add not available")
            pytest.skip("dynamic_mobius_add not implemented")
        
        # 3. Dynamic Poincare Layer
        print("\n3ï¸âƒ£ Testing dynamic_poincare_layer...")
        if hasattr(reality_stone._C, 'dynamic_poincare_layer'):
            u = torch.randn(batch_size, dim, device=self.device) * 0.3
            v = torch.randn(batch_size, dim, device=self.device) * 0.3
            curvatures = torch.ones(batch_size, device=self.device) * 1.0
            poincare_result = reality_stone._C.dynamic_poincare_layer(u, v, curvatures, 0.5)
            print(f"   âœ… Dynamic Poincare: {u.shape} interpolated with {v.shape} -> {poincare_result.shape}")
            assert poincare_result.shape == u.shape
            assert not torch.isnan(poincare_result).any(), "NaN in poincare result!"
        else:
            print(f"   âŒ dynamic_poincare_layer not available")
            pytest.skip("dynamic_poincare_layer not implemented")
        
    def test_regularization(self):
        """Regularization í…ŒìŠ¤íŠ¸"""
        print_test_header("Hyperbolic Regularization")
        
        batch_size, dim = 32, 64
        x = torch.randn(batch_size, dim, device=self.device) * 0.5  # ê²½ê³„ ë‚´ë¶€ì—
        weights = torch.randn(10, dim, device=self.device) * 0.3
        
        # 1. Boundary Penalty
        print("\n1ï¸âƒ£ Testing boundary_penalty...")
        if hasattr(reality_stone._C, 'boundary_penalty'):
            boundary_loss = reality_stone._C.boundary_penalty(x, 1.0, 0.01)
            print(f"   âœ… Boundary penalty: {boundary_loss.shape}, mean: {boundary_loss.mean().item():.4f}")
            assert boundary_loss.shape == (batch_size,)
            assert (boundary_loss >= 0).all(), "Boundary penalty should be non-negative!"
        else:
            print(f"   âŒ boundary_penalty not available")
            pytest.skip("boundary_penalty not implemented")
        
        # 2. Curvature Adaptive Penalty
        print("\n2ï¸âƒ£ Testing curvature_penalty...")
        if hasattr(reality_stone._C, 'curvature_penalty'):
            curvature_loss = reality_stone._C.curvature_penalty(x, 1.0)
            print(f"   âœ… Curvature penalty: {curvature_loss.shape}, mean: {curvature_loss.mean().item():.4f}")
            assert curvature_loss.shape == (batch_size,)
            assert (curvature_loss >= 0).all(), "Curvature penalty should be non-negative!"
        else:
            print(f"   âŒ curvature_penalty not available")
            pytest.skip("curvature_penalty not implemented")
        
        # 3. Geodesic Variance Penalty
        print("\n3ï¸âƒ£ Testing geodesic_penalty...")
        if hasattr(reality_stone._C, 'geodesic_penalty'):
            geodesic_loss = reality_stone._C.geodesic_penalty(weights, 1.0)
            print(f"   âœ… Geodesic penalty: {geodesic_loss.shape}, value: {geodesic_loss.item():.4f}")
            assert geodesic_loss.numel() == 1, "Geodesic penalty should be scalar!"
            assert geodesic_loss.item() >= 0, "Geodesic penalty should be non-negative!"
        else:
            print(f"   âŒ geodesic_penalty not available")
            pytest.skip("geodesic_penalty not implemented")
        
        # 4. Combined Regularization
        print("\n4ï¸âƒ£ Testing combined_reg...")
        if hasattr(reality_stone._C, 'combined_reg'):
            combined_loss = reality_stone._C.combined_reg(x, weights, 1.0, 0.1, 0.1, 0.1, 0.01)
            print(f"   âœ… Combined regularization: {combined_loss.shape}, value: {combined_loss.item():.4f}")
            assert combined_loss.numel() == 1, "Combined reg should be scalar!"
            assert combined_loss.item() >= 0, "Combined reg should be non-negative!"
        else:
            print(f"   âŒ combined_reg not available")
            pytest.skip("combined_reg not implemented")
        
    def test_geodesic_activation(self):
        """Geodesic Activation í…ŒìŠ¤íŠ¸"""
        print_test_header("Geodesic Activation")
        
        batch_size, dim, num_anchors = 32, 64, 8
        
        input_tensor = torch.randn(batch_size, dim, device=self.device) * 0.3
        anchors = torch.randn(num_anchors, dim, device=self.device) * 0.4
        t_values = torch.randn(num_anchors, device=self.device)
        weights = torch.softmax(torch.randn(num_anchors, device=self.device), dim=0)
        
        # 1. Einstein Midpoint
        print("\n1ï¸âƒ£ Testing einstein_midpoint...")
        if hasattr(reality_stone._C, 'einstein_midpoint'):
            points = torch.randn(batch_size, num_anchors, dim, device=self.device) * 0.2
            midpoint_weights = torch.softmax(torch.randn(num_anchors, device=self.device), dim=0)
            midpoint = reality_stone._C.einstein_midpoint(points, midpoint_weights, 1.0)
            print(f"   âœ… Einstein midpoint: {points.shape} -> {midpoint.shape}")
            assert midpoint.shape == (batch_size, dim)
            assert not torch.isnan(midpoint).any(), "NaN in einstein midpoint!"
        else:
            print(f"   âŒ einstein_midpoint not available")
            pytest.skip("einstein_midpoint not implemented")
        
        # 2. Multi-Geodesic Mixing
        print("\n2ï¸âƒ£ Testing multi_geodesic...")
        if hasattr(reality_stone._C, 'multi_geodesic'):
            multi_result = reality_stone._C.multi_geodesic(input_tensor, anchors, t_values, weights, 1.0)
            print(f"   âœ… Multi-geodesic: {input_tensor.shape} -> {multi_result.shape}")
            assert multi_result.shape == input_tensor.shape
            assert not torch.isnan(multi_result).any(), "NaN in multi geodesic!"
        else:
            print(f"   âŒ multi_geodesic not available")
            pytest.skip("multi_geodesic not implemented")
        
        # 3. Geodesic Activation
        print("\n3ï¸âƒ£ Testing geodesic_activation...")
        if hasattr(reality_stone._C, 'geodesic_activation'):
            activation_result = reality_stone._C.geodesic_activation(input_tensor, anchors, t_values, weights, 1.0)
            print(f"   âœ… Geodesic activation: {input_tensor.shape} -> {activation_result.shape}")
            assert activation_result.shape == input_tensor.shape
            assert not torch.isnan(activation_result).any(), "NaN in geodesic activation!"
        else:
            print(f"   âŒ geodesic_activation not available")
            pytest.skip("geodesic_activation not implemented")
        
    @pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
    def test_performance(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print_test_header("Performance Test")
        
        batch_size, dim = 1024, 256
        num_iterations = 100
        
        x = torch.randn(batch_size, dim, device='cuda')
        y = torch.randn(batch_size, dim, device='cuda')
        
        # Warmup
        for _ in range(10):
            _ = reality_stone._C.mobius_add_cuda(x, y, 1.0)
        
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(num_iterations):
            result = reality_stone._C.mobius_add_cuda(x, y, 1.0)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_iterations * 1000  # ms
        throughput = (batch_size * num_iterations) / (end_time - start_time)
        
        print(f"   âœ… Mobius Add CUDA Performance:")
        print(f"      ğŸš€ Average time: {avg_time:.3f} ms")
        print(f"      ğŸ“ˆ Throughput: {throughput:.0f} samples/sec")
        
        assert avg_time < 1.0, f"Performance regression: {avg_time:.3f}ms > 1.0ms"
        
    def test_numerical_stability(self):
        """ìˆ˜ì¹˜ì  ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (NaN ë¬¸ì œ í•´ê²° í™•ì¸)"""
        print_test_header("Numerical Stability Test")
        
        # ê²½ê³„ì— ê°€ê¹Œìš´ ê°’ë“¤ë¡œ í…ŒìŠ¤íŠ¸
        edge_values = torch.tensor([
            [0.99, 0.0],   # ê±°ì˜ ê²½ê³„
            [0.0, 0.99],   
            [-0.99, 0.0],  
            [0.0, -0.99],  
            [0.98, 0.02],  # ê²½ê³„ ê·¼ì²˜ ì¡°í•©
        ], device=self.device)
        
        print(f"\nğŸ” Testing edge cases near boundary...")
        
        # ê¸°ë³¸ Mobius ì—°ì‚° í…ŒìŠ¤íŠ¸
        for i, val in enumerate(edge_values):
            for j in range(i+1, len(edge_values)):
                result = reality_stone._C.mobius_add_cuda(val.unsqueeze(0), edge_values[j].unsqueeze(0), 1.0)
                
                # NaN ì²´í¬
                if torch.isnan(result).any():
                    print(f"   âŒ NaN detected: {val} + {edge_values[j]} = {result}")
                    assert False, f"NaN values at edge case {i},{j}"
                
                # ê²½ê³„ ì²´í¬
                norm = torch.norm(result, dim=-1)
                if (norm >= 1.0).any():
                    print(f"   âš ï¸  Boundary violation: ||{result}|| = {norm}")
        
        print(f"   âœ… All edge cases passed numerical stability test!")


if __name__ == "__main__":
    # pytest ì—†ì´ ì§ì ‘ ì‹¤í–‰í•  ë•Œ
    print("ğŸŒŸ Reality Stone Advanced Features Test Suite")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    print(f"ğŸ’» CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    
    tester = TestAdvancedFeatures()
    tester.setup_method(None)
    
    test_methods = [
        tester.test_basic_import,
        tester.test_fused_operations,
        tester.test_dynamic_curvature,
        tester.test_regularization,
        tester.test_geodesic_activation,
        tester.test_numerical_stability,
    ]
    
    if torch.cuda.is_available():
        test_methods.append(tester.test_performance)
    
    passed = 0
    total = len(test_methods)
    
    for test_method in test_methods:
        try:
            test_method()
            passed += 1
            print(f"âœ… {test_method.__name__} PASSED")
        except Exception as e:
            print(f"âŒ {test_method.__name__} FAILED: {e}")
            traceback.print_exc()
    
    print(f"\nğŸ¯ Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ All tests passed! Reality Stone advanced features are working!")
        sys.exit(0)
    else:
        print("âš ï¸  Some tests failed. Check implementation.")
        sys.exit(1) 