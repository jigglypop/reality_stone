#!/usr/bin/env python3
"""
MNIST Integration Test with Dynamic Curvature
ë™ì  ê³¡ë¥ ì„ ì‚¬ìš©í•œ MNIST ë¶„ë¥˜ í…ŒìŠ¤íŠ¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
import reality_stone
import pytest
import time
import numpy as np
import sys


class DynamicCurvatureNet(nn.Module):
    """ë™ì  ê³¡ë¥ ì„ ì‚¬ìš©í•˜ëŠ” í•˜ì´í¼ë³¼ë¦­ MNIST ë¶„ë¥˜ê¸°"""
    
    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        
        # ì¼ë°˜ ì„ í˜• ë ˆì´ì–´ë“¤
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, num_classes)
        
        # ì •ê·œí™”ìš© íŒŒë¼ë¯¸í„°
        self.base_curvature = 1.0
        self.reg_lambda = 0.01
        
    def forward(self, x):
        batch_size = x.size(0)
        x = x.view(batch_size, -1)  # Flatten
        
        # Layer 1: ê¸°ë³¸ ì„ í˜•
        h1 = torch.relu(self.fc1(x))
        
        # Layer 2: í•˜ì´í¼ë³¼ë¦­ ì •ê·œí™” ì ìš©
        h2 = torch.relu(self.fc2(h1))
        
        # í•˜ì´í¼ë³¼ë¦­ ì •ê·œí™” (ê²½ê³„ í˜ë„í‹°) ì ìš©
        if hasattr(reality_stone._C, 'boundary_penalty'):
            # í¬ì¸ì¹´ë ˆ ë””ìŠ¤í¬ ë‚´ë¶€ë¡œ ì •ê·œí™”
            h2_norm = torch.norm(h2, dim=-1, keepdim=True)
            max_norm = 1.0 / np.sqrt(self.base_curvature) - 0.01
            h2_clamped = torch.where(
                h2_norm > max_norm,
                h2 * (max_norm / (h2_norm + 1e-7)),
                h2
            )
            h2 = h2_clamped
        
        # ì¶œë ¥ ë ˆì´ì–´
        logits = self.fc3(h2)
        
        return logits
    
    def get_regularization_loss(self):
        """ì •ê·œí™” ì†ì‹¤ ê³„ì‚°"""
        total_reg = 0.0
        
        # ëª¨ë“  ê°€ì¤‘ì¹˜ì— ëŒ€í•´ ì •ê·œí™” ì ìš©
        for name, param in self.named_parameters():
            if 'weight' in name and param.requires_grad:
                if hasattr(reality_stone._C, 'boundary_penalty'):
                    reg_loss = reality_stone._C.boundary_penalty(
                        param.view(param.size(0), -1), 
                        self.base_curvature, 
                        0.01
                    )
                    total_reg += torch.mean(reg_loss)
        
        return self.reg_lambda * total_reg


class AdvancedDynamicNet(nn.Module):
    """ì§„ì§œ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_classes = num_classes
        
        # ë™ì  ê³¡ë¥  ì˜ˆì¸¡ìš© íŒŒë¼ë¯¸í„°
        self.curvature_weight = nn.Parameter(torch.randn(1, 1) * 0.1)
        self.curvature_bias = nn.Parameter(torch.zeros(1))
        
        # ë©”ì¸ ë„¤íŠ¸ì›Œí¬
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, num_classes)
        
        # ì¸¡ì§€ì„  í™œì„±í™”ìš© ì•µì»¤ë“¤
        if hasattr(reality_stone._C, 'geodesic_activation'):
            self.num_anchors = 4
            self.anchors = nn.Parameter(torch.randn(self.num_anchors, hidden_dim) * 0.1)
            self.anchor_weights = nn.Parameter(torch.ones(self.num_anchors) / self.num_anchors)
            self.t_values = nn.Parameter(torch.randn(self.num_anchors) * 0.1)
        
    def forward(self, x):
        batch_size = x.size(0)
        x = x.view(batch_size, -1)
        
        # ë™ì  ê³¡ë¥  ì˜ˆì¸¡
        if hasattr(reality_stone._C, 'dynamic_curvature_pred'):
            # ì…ë ¥ì˜ L2 normì„ íŠ¹ì§•ìœ¼ë¡œ ì‚¬ìš©
            x_norm = torch.norm(x, dim=-1, keepdim=True)
            curvatures = reality_stone._C.dynamic_curvature_pred(
                x_norm, self.curvature_weight, self.curvature_bias, 1.0
            )
        else:
            curvatures = torch.ones(batch_size, device=x.device)
        
        # Layer 1
        h1 = torch.relu(self.fc1(x))
        
        # Layer 2 with ìœµí•© ì—°ì‚°
        if hasattr(reality_stone._C, 'fused_linear'):
            h2_raw, reg_loss = reality_stone._C.fused_transform_reg(h1, 1.0, 0.01)
            h2 = torch.relu(h2_raw)
        else:
            h2 = torch.relu(self.fc2(h1))
            reg_loss = torch.tensor(0.0, device=x.device)
        
        # ì¸¡ì§€ì„  í™œì„±í™” ì ìš©
        if hasattr(reality_stone._C, 'geodesic_activation'):
            h2_geo = reality_stone._C.geodesic_activation(
                h2, self.anchors, self.t_values, 
                torch.softmax(self.anchor_weights, dim=0), 1.0
            )
            h2 = h2_geo
        
        # ì¶œë ¥
        logits = self.fc3(h2)
        
        return logits, curvatures, reg_loss


def create_mnist_dataloaders(batch_size=128, num_workers=0):
    """MNIST ë°ì´í„°ë¡œë” ìƒì„±"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ìˆìœ¼ë©´ skip)
    train_dataset = torchvision.datasets.MNIST(
        root='./MNIST', train=True, download=True, transform=transform
    )
    test_dataset = torchvision.datasets.MNIST(
        root='./MNIST', train=False, download=True, transform=transform
    )
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë°ì´í„° ê°œìˆ˜ ì œí•œ
    train_subset = torch.utils.data.Subset(train_dataset, range(0, 1000))
    test_subset = torch.utils.data.Subset(test_dataset, range(0, 200))
    
    train_loader = DataLoader(
        train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers
    )
    test_loader = DataLoader(
        test_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers
    )
    
    return train_loader, test_loader


def test_basic_mnist():
    """ê¸°ë³¸ MNIST ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ§ª Basic MNIST Classification Test")
    print("="*60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ Device: {device}")
    
    # ë°ì´í„° ë¡œë”
    train_loader, test_loader = create_mnist_dataloaders(batch_size=64)
    
    # ëª¨ë¸ ìƒì„±
    model = DynamicCurvatureNet().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    print(f"ğŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # í›ˆë ¨ (ì§§ê²Œ)
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        if batch_idx >= 5:  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 5ë°°ì¹˜ë§Œ
            break
            
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        
        # ë¶„ë¥˜ ì†ì‹¤
        loss = criterion(output, target)
        
        # ì •ê·œí™” ì†ì‹¤ ì¶”ê°€
        reg_loss = model.get_regularization_loss()
        total_loss_val = loss + reg_loss
        
        total_loss_val.backward()
        optimizer.step()
        
        total_loss += total_loss_val.item()
        num_batches += 1
        
        if batch_idx % 2 == 0:
            print(f"   Batch {batch_idx}: Loss {total_loss_val.item():.4f} "
                  f"(CE: {loss.item():.4f}, Reg: {reg_loss.item():.4f})")
    
    avg_loss = total_loss / num_batches
    print(f"âœ… Training completed. Average loss: {avg_loss:.4f}")
    
    # í…ŒìŠ¤íŠ¸
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            if batch_idx >= 3:  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
                break
                
            data, target = data.to(device), target.to(device)
            output = model(data)
            
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    
    accuracy = 100.0 * correct / total
    print(f"âœ… Test accuracy: {accuracy:.2f}% ({correct}/{total})")
    
    assert accuracy > 10.0, f"ë„ˆë¬´ ë‚®ì€ ì •í™•ë„: {accuracy:.2f}%"
    print("ğŸ‰ Basic MNIST test passed!")


def test_advanced_mnist():
    """ê³ ê¸‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ëŠ” MNIST í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ§ª Advanced MNIST with Dynamic Curvature")
    print("="*60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ Device: {device}")
    
    # ê³ ê¸‰ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
    advanced_funcs = ['dynamic_curvature_pred', 'fused_transform_reg', 'geodesic_activation']
    available_funcs = []
    
    for func in advanced_funcs:
        if hasattr(reality_stone._C, func):
            available_funcs.append(func)
            print(f"   âœ… {func} available")
        else:
            print(f"   âŒ {func} not available")
    
    if not available_funcs:
        print("âš ï¸  No advanced functions available. Skipping advanced test.")
        pytest.skip("Advanced functions not implemented")
        return
    
    # ë°ì´í„° ë¡œë”
    train_loader, test_loader = create_mnist_dataloaders(batch_size=32)
    
    # ê³ ê¸‰ ëª¨ë¸ ìƒì„±
    model = AdvancedDynamicNet().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    print(f"ğŸ“Š Advanced model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # í›ˆë ¨
    model.train()
    total_loss = 0.0
    total_reg = 0.0
    num_batches = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        if batch_idx >= 3:  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
            break
            
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        
        # ì „ë°© íŒ¨ìŠ¤
        output, curvatures, reg_loss = model(data)
        
        # ì†ì‹¤ ê³„ì‚°
        ce_loss = criterion(output, target)
        total_loss_val = ce_loss + reg_loss
        
        total_loss_val.backward()
        optimizer.step()
        
        total_loss += total_loss_val.item()
        total_reg += reg_loss.item()
        num_batches += 1
        
        print(f"   Batch {batch_idx}: Loss {total_loss_val.item():.4f} "
              f"(CE: {ce_loss.item():.4f}, Reg: {reg_loss.item():.4f})")
        print(f"      Curvature range: [{curvatures.min():.3f}, {curvatures.max():.3f}]")
    
    avg_loss = total_loss / num_batches
    avg_reg = total_reg / num_batches
    print(f"âœ… Advanced training completed. Loss: {avg_loss:.4f}, Reg: {avg_reg:.4f}")
    
    # í…ŒìŠ¤íŠ¸
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            if batch_idx >= 2:
                break
                
            data, target = data.to(device), target.to(device)
            output, curvatures, _ = model(data)
            
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    
    accuracy = 100.0 * correct / total
    print(f"âœ… Advanced test accuracy: {accuracy:.2f}% ({correct}/{total})")
    
    assert accuracy > 5.0, f"ê³ ê¸‰ ëª¨ë¸ ì •í™•ë„ê°€ ë„ˆë¬´ ë‚®ìŒ: {accuracy:.2f}%"
    print(f"ğŸ‰ Advanced MNIST test passed with {len(available_funcs)} advanced features!")


def test_nan_detection():
    """NaN ë¬¸ì œ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ§ª NaN Detection Test")
    print("="*60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # ê·¹ë‹¨ì ì¸ ê°’ë“¤ë¡œ í…ŒìŠ¤íŠ¸
    extreme_input = torch.tensor([
        [10.0] * 784,    # ë§¤ìš° í° ê°’
        [-10.0] * 784,   # ë§¤ìš° ì‘ì€ ê°’
        [0.0] * 784,     # ì˜ë²¡í„°
    ], device=device)
    
    model = DynamicCurvatureNet().to(device)
    
    print("ğŸ” Testing with extreme inputs...")
    
    with torch.no_grad():
        output = model(extreme_input)
        
        # NaN ì²´í¬
        if torch.isnan(output).any():
            print("âŒ NaN detected in output!")
            print(f"   Output: {output}")
            assert False, "NaN values detected with extreme inputs"
        else:
            print("âœ… No NaN detected with extreme inputs")
        
        # ë¬´í•œëŒ€ ì²´í¬
        if torch.isinf(output).any():
            print("âŒ Infinite values detected!")
            assert False, "Infinite values detected"
        else:
            print("âœ… No infinite values detected")
    
    print("ğŸ‰ NaN detection test passed!")


def test_performance_comparison():
    """ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ§ª Performance Comparison Test")
    print("="*60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    batch_size = 128
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_input = torch.randn(batch_size, 784, device=device)
    
    # ê¸°ë³¸ ëª¨ë¸
    basic_model = DynamicCurvatureNet().to(device)
    
    # ê³ ê¸‰ ëª¨ë¸ (ê°€ëŠ¥í•œ ê¸°ëŠ¥ë“¤ë§Œ ì‚¬ìš©)
    advanced_model = AdvancedDynamicNet().to(device)
    
    # ì„±ëŠ¥ ì¸¡ì •
    def measure_time(model, input_data, num_runs=50):
        model.eval()
        torch.cuda.synchronize() if device == 'cuda' else None
        
        start_time = time.time()
        with torch.no_grad():
            for _ in range(num_runs):
                if hasattr(model, 'forward'):
                    if isinstance(model, AdvancedDynamicNet):
                        output, _, _ = model(input_data)
                    else:
                        output = model(input_data)
        
        torch.cuda.synchronize() if device == 'cuda' else None
        end_time = time.time()
        
        return (end_time - start_time) / num_runs * 1000  # ms per run
    
    # ì¸¡ì •
    basic_time = measure_time(basic_model, test_input)
    advanced_time = measure_time(advanced_model, test_input)
    
    print(f"ğŸ“Š Performance Results:")
    print(f"   Basic model: {basic_time:.3f} ms/batch")
    print(f"   Advanced model: {advanced_time:.3f} ms/batch")
    print(f"   Overhead: {((advanced_time - basic_time) / basic_time * 100):.1f}%")
    
    # ì˜¤ë²„í—¤ë“œê°€ ë„ˆë¬´ í¬ì§€ ì•Šì€ì§€ í™•ì¸
    overhead_ratio = advanced_time / basic_time
    assert overhead_ratio < 3.0, f"ê³ ê¸‰ ê¸°ëŠ¥ ì˜¤ë²„í—¤ë“œê°€ ë„ˆë¬´ í¼: {overhead_ratio:.1f}x"
    
    print("âœ… Performance test passed!")


if __name__ == "__main__":
    print("ğŸŒŸ Reality Stone MNIST Integration Test Suite")
    print(f"ğŸ”¥ PyTorch: {torch.__version__}")
    print(f"ğŸ’» CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    
    tests = [
        test_basic_mnist,
        test_advanced_mnist,
        test_nan_detection,
        test_performance_comparison,
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            test_func()
            passed += 1
            print(f"âœ… {test_func.__name__} PASSED")
        except Exception as e:
            print(f"âŒ {test_func.__name__} FAILED: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\nğŸ¯ MNIST Tests: {passed}/{total} passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ All MNIST integration tests passed!")
        sys.exit(0)
    else:
        print("âš ï¸  Some MNIST tests failed.")
        sys.exit(1) 